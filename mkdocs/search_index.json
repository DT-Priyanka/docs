{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to DataTorrent RTS!\n\n\nDataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.  \n\n\n\n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n\n\n\n\n\n  \n\n    \n\n    \nRun Demos\n\n    \nExperience the power of DataTorrent RTS quickly. Import, launch, manage and visualize applications in minutes.\n\n  \n\n\n  \n\n\n  \n\n    \n\n    \nDiscover RTS\n\n    \nLearn about the architecture and the rich feature set of the DataTorrent RTS platform and applications.\n\n  \n  \n\n  \n\n\n  \n\n    \n\n    \nCreate Apps\n\n    \nTutorials and code samples to rapidly create DataTorrent applications using Java or dtAssemble.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-datatorrent-rts", 
            "text": "DataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.    \n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }", 
            "title": "Welcome to DataTorrent RTS!"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nDataTorrent RTS Elastic Scalability Demo\n\n\n\n\n\nDataTorrent RTS Dimensional Computing Demo\n\n\n\n\n\nDataTorrent RTS Application Builder Demo\n\n\n\n\n\nDataTorrent RTS Fault Tolerance Demo", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "DataTorrent RTS Elastic Scalability Demo   DataTorrent RTS Dimensional Computing Demo   DataTorrent RTS Application Builder Demo   DataTorrent RTS Fault Tolerance Demo", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demos/", 
            "text": "DataTorrent RTS Demo Applications\n\n\nDataTorrent RTS includes a number of demo applications, which help demonstrate the features of the RTS platform.  These demos are available for import from the \nDevelopment \n App Packages\n section of the DataTorrent management console.\n\n\nImporting Demo Applications\n\n\n\n\nNavigate to \nDevelop \n App Packages \n Import\n section of the DataTorrent console.\n\n\nSelect one of the available packages, such as \nDataTorrent Pi Demo\n and click \nImport\n button.\n\n\nImported packages and included applications will be listed under \nDevelop \n App Packages\n page.\n\n\n\n\nBasic Demo Applications\n\n\nThese applications require minimal resources and configuration changes and can be launched with a single click.  \nNote\n: Ensure Hadoop YARN and HDFS services are active and ready by making sure there are no errors in the DataTorrent console before launching demo applications.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available Applications, locate PiDemo and press the launch button.\n\n\n\n\n\n\n\n\nProceed with default options on launch confirmation screen by pressing the Launch button.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent console is available in \ndtManage Guide\n\n\nAdvanced Demo Applications\n\n\nThese applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the \ndocs\n.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console.  In this example we will use \nDataTorrent Twitter Demo\n application package.  Import this package from \nDevelop \n App Packages \n Import\n if it is not available.\n\n\n\n\n\n\nFrom the list of Applications, select TwitterDemo and press the corresponding launch button.\n\n\n\n\n\n\nRetrieve Twitter API access information by registering for \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:\n\n\ndt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\nSelect \nSpecify custom properties\n on the launch confirmation screen, click \nadd required properties\n button, and provide Twitter API access values.  Choose to save this configuration as \ntwitter.xml\n file and proceed to Launch the application.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboards\n.", 
            "title": "Applications"
        }, 
        {
            "location": "/demos/#datatorrent-rts-demo-applications", 
            "text": "DataTorrent RTS includes a number of demo applications, which help demonstrate the features of the RTS platform.  These demos are available for import from the  Development   App Packages  section of the DataTorrent management console.", 
            "title": "DataTorrent RTS Demo Applications"
        }, 
        {
            "location": "/demos/#importing-demo-applications", 
            "text": "Navigate to  Develop   App Packages   Import  section of the DataTorrent console.  Select one of the available packages, such as  DataTorrent Pi Demo  and click  Import  button.  Imported packages and included applications will be listed under  Develop   App Packages  page.", 
            "title": "Importing Demo Applications"
        }, 
        {
            "location": "/demos/#basic-demo-applications", 
            "text": "These applications require minimal resources and configuration changes and can be launched with a single click.   Note : Ensure Hadoop YARN and HDFS services are active and ready by making sure there are no errors in the DataTorrent console before launching demo applications.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use  PiDemo  application package.    From the list of available Applications, locate PiDemo and press the launch button.     Proceed with default options on launch confirmation screen by pressing the Launch button.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.     More information about using DataTorrent console is available in  dtManage Guide", 
            "title": "Basic Demo Applications"
        }, 
        {
            "location": "/demos/#advanced-demo-applications", 
            "text": "These applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the  docs .    Navigate to  App Packages  under  Develop  tab of the DataTorrent console.  In this example we will use  DataTorrent Twitter Demo  application package.  Import this package from  Develop   App Packages   Import  if it is not available.    From the list of Applications, select TwitterDemo and press the corresponding launch button.    Retrieve Twitter API access information by registering for  Twitter Developer  account, creating a new  Twitter Application , and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:  dt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret    Select  Specify custom properties  on the launch confirmation screen, click  add required properties  button, and provide Twitter API access values.  Choose to save this configuration as  twitter.xml  file and proceed to Launch the application.     Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboards .", 
            "title": "Advanced Demo Applications"
        }, 
        {
            "location": "/sandbox/", 
            "text": "DataTorrent RTS Sandbox\n\n\nWelcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  For additional information about DataTorrent products, please visit https://www.datatorrent.com/\n\n\nSandbox Overview\n\n\nThe DataTorrent Sandbox automatically launches Hadoop HDFS and YARN services on startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching DataTorrent applications.\n\n\nNote\n: By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and DataTorrent applications startup.\n\n\nWhen accessing DataTorrent \nconsole\n in the sandbox for the first time, you will be required to log in.  Use username \ndtadmin\n and password \ndtadmin\n.  Same credentials are also valid for sandbox system access.\n\n\n\n\nOnce authenticated, you can continue to \nDataTorrent Docs\n to explore demo applications, discover features of the RTS platform, or learn how to write your own DataTorrent applications.", 
            "title": "Welcome to the Sandbox"
        }, 
        {
            "location": "/sandbox/#datatorrent-rts-sandbox", 
            "text": "Welcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  For additional information about DataTorrent products, please visit https://www.datatorrent.com/", 
            "title": "DataTorrent RTS Sandbox"
        }, 
        {
            "location": "/sandbox/#sandbox-overview", 
            "text": "The DataTorrent Sandbox automatically launches Hadoop HDFS and YARN services on startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching DataTorrent applications.  Note : By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and DataTorrent applications startup.  When accessing DataTorrent  console  in the sandbox for the first time, you will be required to log in.  Use username  dtadmin  and password  dtadmin .  Same credentials are also valid for sandbox system access.   Once authenticated, you can continue to  DataTorrent Docs  to explore demo applications, discover features of the RTS platform, or learn how to write your own DataTorrent applications.", 
            "title": "Sandbox Overview"
        }, 
        {
            "location": "/sandbox_demos/", 
            "text": "DataTorrent RTS Sandbox Demo Applications\n\n\nDataTorrent RTS includes a number of pre-loaded demo applications, which help demonstrate the features of the RTS platform.\n\n\nBasic Demo Applications\n\n\nThese applications require minimal resources and configuration changes and can be launched with a single click.  Ensure Hadoop \nYARN\n and \nHDFS\n are active and ready before launching DataTorrent demo applications.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under Develop tab of the DataTorrent console, and select one of the demo packages.  In this example we will use \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available Applications, locate PiDemo and press the launch button.\n\n\n\n\n\n\n\n\nProceed with default options on launch confirmation screen by pressing the Launch button.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the Console and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent Console is available in \nConsole Guide\n\n\nAdvanced Demo Applications\n\n\nThese applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the \ndocs\n.\n\n\n\n\n\n\nNavigate to \nApp Packages\n tab of the DataTorrent console, and select one of the demo packages.  In this example we will use \nTwitter Demo\n application package.\n\n\n\n\n\n\nFrom the list of Applications, select TwitterDemo and press the corresponding launch button.\n\n\n\n\n\n\nRetrieve Twitter API access information by registering for \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:\n\n\ndt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\nSelect \nSpecify custom properties\n on the launch confirmation screen, click \nadd required properties\n button, and provide Twitter API access values.  Choose to save this configuration as \ntwitter.xml\n file and proceed to Launch the application.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to DataTorrent \nConsole\n and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboards\n.\n\n\n\n\n\n\nStopping Applications\n\n\nApplications can be shut down or killed from the \nDataTorrent Console\n by selecting application from the list and clicking \nshutdown\n or \nkill\n buttons.", 
            "title": "Sandbox Demos"
        }, 
        {
            "location": "/sandbox_demos/#datatorrent-rts-sandbox-demo-applications", 
            "text": "DataTorrent RTS includes a number of pre-loaded demo applications, which help demonstrate the features of the RTS platform.", 
            "title": "DataTorrent RTS Sandbox Demo Applications"
        }, 
        {
            "location": "/sandbox_demos/#basic-demo-applications", 
            "text": "These applications require minimal resources and configuration changes and can be launched with a single click.  Ensure Hadoop  YARN  and  HDFS  are active and ready before launching DataTorrent demo applications.    Navigate to  App Packages  under Develop tab of the DataTorrent console, and select one of the demo packages.  In this example we will use  PiDemo  application package.    From the list of available Applications, locate PiDemo and press the launch button.     Proceed with default options on launch confirmation screen by pressing the Launch button.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the Console and selecting the launched application.     More information about using DataTorrent Console is available in  Console Guide", 
            "title": "Basic Demo Applications"
        }, 
        {
            "location": "/sandbox_demos/#advanced-demo-applications", 
            "text": "These applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the  docs .    Navigate to  App Packages  tab of the DataTorrent console, and select one of the demo packages.  In this example we will use  Twitter Demo  application package.    From the list of Applications, select TwitterDemo and press the corresponding launch button.    Retrieve Twitter API access information by registering for  Twitter Developer  account, creating a new  Twitter Application , and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:  dt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret    Select  Specify custom properties  on the launch confirmation screen, click  add required properties  button, and provide Twitter API access values.  Choose to save this configuration as  twitter.xml  file and proceed to Launch the application.     Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to DataTorrent  Console  and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboards .", 
            "title": "Advanced Demo Applications"
        }, 
        {
            "location": "/sandbox_demos/#stopping-applications", 
            "text": "Applications can be shut down or killed from the  DataTorrent Console  by selecting application from the list and clicking  shutdown  or  kill  buttons.", 
            "title": "Stopping Applications"
        }, 
        {
            "location": "/sandbox_services/", 
            "text": "Managing DataTorrent RTS Sandbox Services\n\n\nDataTorrent Sandbox automatically launches following services on startup.\n\n\n\n\nHadoop HDFS NameNode\n\n\nHadoop HDFS DataNode\n\n\nHadoop YARN ResourceManager\n\n\nHadoop YARN NodeManager\n\n\nDataTorrent Gateway\n\n\n\n\nAll the services can be managed by right-clicking on \nDataTorrent Services\n desktop launcher.\n\n\n\n\n\n\nStart Hadoop and DataTorrent services by right-clicking on \nDataTorrent Services\n desktop launcher and selecting \nStart Hadoop and DataTorrent\n\n\nShut down Hadoop and DataTorrent services by right-clicking on \nDataTorrent Services\n icon and selecting \nShut down Hadoop and DataTorrent\n\n\nDelete hdfs, tmp, and log files and restart all services by right-clicking on DataTorrent Services icon and selecting \nRebuild HDFS and Restart Services\n\n\n\n\nSupport\n\n\nIf you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on \ndatatorrent.com/contact\n page.", 
            "title": "Sandbox Services"
        }, 
        {
            "location": "/sandbox_services/#managing-datatorrent-rts-sandbox-services", 
            "text": "DataTorrent Sandbox automatically launches following services on startup.   Hadoop HDFS NameNode  Hadoop HDFS DataNode  Hadoop YARN ResourceManager  Hadoop YARN NodeManager  DataTorrent Gateway   All the services can be managed by right-clicking on  DataTorrent Services  desktop launcher.    Start Hadoop and DataTorrent services by right-clicking on  DataTorrent Services  desktop launcher and selecting  Start Hadoop and DataTorrent  Shut down Hadoop and DataTorrent services by right-clicking on  DataTorrent Services  icon and selecting  Shut down Hadoop and DataTorrent  Delete hdfs, tmp, and log files and restart all services by right-clicking on DataTorrent Services icon and selecting  Rebuild HDFS and Restart Services", 
            "title": "Managing DataTorrent RTS Sandbox Services"
        }, 
        {
            "location": "/sandbox_services/#support", 
            "text": "If you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on  datatorrent.com/contact  page.", 
            "title": "Support"
        }, 
        {
            "location": "/rts/", 
            "text": "DataTorrent RTS Overview\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using \ndtAssemble\n, a graphical application assembly tool.  Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "RTS"
        }, 
        {
            "location": "/rts/#datatorrent-rts-overview", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using  dtAssemble , a graphical application assembly tool.  Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "DataTorrent RTS Overview"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "dtManage"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtassemble/", 
            "text": "dtAssemble - Graphical Application Builder\n\n\nThe dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application. \n\n\n\n\nAccessing the Builder\n\n\nTo get to the App Builder, you must perform the following steps (illustrated by GIF):\n\n\n\n\n\n\nCreate a DataTorrent Application Package\n  To do this, read through the \nApplication Packages Guide\n, which provides step-by-step instructions on how to do this.\n\n\nUpload it to HDFS using the Console\n  Click on the \nDevelop\n link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.\n\n\nAdd a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.\n\n\nDrag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d\n\n\nConfigure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.\n\n\nConnect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.\n\n\n\n\nUsing Application Builder\n\n\nThe Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:\n\n\n\n\nOperator Library Navigator\n\n\n\n\nUse this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder\nsection). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.\n\n\nOnce you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.\n\n\n\n\nCanvas\n\n\n\n\nThe Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder\nsection.\n\n\n\n\nInspector\n\n\nThe Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.\n\n\nOperator Inspector\n\n\n\n\nWhen an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties. \n\n\nPort Inspector\n\n\nWhen a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port). \n\n\nStream Inspector\n\n\n\n\nThe stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.\n\n\nPrepping Operators for the Application Builder\n\n\nThe way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:\n\n\n\n\nOperators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.\n\n\n\n\nUse javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.\n\n\n/**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }\n\n\n\n\n\n\n\nEvery property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended\n\n\n/**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\n\n\nUtilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.  \n\n\n\n\nWhen a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.\n\n\nMark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization.\n/**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\nMake the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.\n\n\n\n\n\n\n\n\nApp Builder Usage Examples\n\n\nPi Demo\n\n\nAs an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application. \n\n\n\n\n\n\nFirst you will need to clone the malhar repository:\n\n\ngit@github.com:DataTorrent/Malhar.git \n cd Malhar\n\n\n\n\n\n\n\nThen, run maven install, which will create the App Package jar need to upload to your gateway.\n\n\n\n\nFollow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.\n\n\nAdd the Console Output\noperators.\n\n\nConnect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.\n\n\nClick the \u201claunch\u201d button in the top left once it turns purple.\n\n\nIn the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.\n\n\nTo see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "dtAssemble"
        }, 
        {
            "location": "/dtassemble/#dtassemble-graphical-application-builder", 
            "text": "The dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application.", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#accessing-the-builder", 
            "text": "To get to the App Builder, you must perform the following steps (illustrated by GIF):    Create a DataTorrent Application Package\n  To do this, read through the  Application Packages Guide , which provides step-by-step instructions on how to do this.  Upload it to HDFS using the Console\n  Click on the  Develop  link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.  Add a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.  Drag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d  Configure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.  Connect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.", 
            "title": "Accessing the Builder"
        }, 
        {
            "location": "/dtassemble/#using-application-builder", 
            "text": "The Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:   Operator Library Navigator   Use this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder section). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.  Once you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.   Canvas   The Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder section.   Inspector  The Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.  Operator Inspector   When an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties.   Port Inspector  When a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).   Stream Inspector   The stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.", 
            "title": "Using Application Builder"
        }, 
        {
            "location": "/dtassemble/#prepping-operators-for-the-application-builder", 
            "text": "The way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:   Operators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.   Use javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.  /**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }    Every property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended  /**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }    Utilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.     When a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.  Mark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization. /**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }   Make the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.", 
            "title": "Prepping Operators for the Application Builder"
        }, 
        {
            "location": "/dtassemble/#app-builder-usage-examples", 
            "text": "Pi Demo  As an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application.     First you will need to clone the malhar repository:  git@github.com:DataTorrent/Malhar.git   cd Malhar    Then, run maven install, which will create the App Package jar need to upload to your gateway.   Follow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.  Add the Console Output operators.  Connect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.  Click the \u201claunch\u201d button in the top left once it turns purple.  In the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.  To see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "App Builder Usage Examples"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.\n\n\nAnd below is the DAG for the Ads Dimension Demo:\n\n\n\n\nIn this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.\n\n\nAll these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.  \nApp Data Framework Programming Guide\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new button on the Dashboards screen\n\n\ngenerate new dashboard option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nSelect compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.\n\n\n\n\n\n\nChoose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nModifying Dashboards\n\n\nDashboards controls are presented as a row of buttons just below the dashboard title and description.\n\n\n\n\nNew widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget \n\n\nAlternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.\n\n\n\n\nWhether using auto generate buttons, results are not persisted until save changes is applied.\n\n\nEach data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "Twitter Example  The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:   Ads Dimension Example  The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.  And below is the DAG for the Ads Dimension Demo:   In this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.  All these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.   App Data Framework Programming Guide", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.   Below is an example of accessing the data visualization dashboard from a running application.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new button on the Dashboards screen  generate new dashboard option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)    Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Select compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.    Choose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#modifying-dashboards", 
            "text": "Dashboards controls are presented as a row of buttons just below the dashboard title and description.   New widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.", 
            "title": "Modifying Dashboards"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget   Alternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.   Whether using auto generate buttons, results are not persisted until save changes is applied.  Each data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "dtGateway"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/apex/", 
            "text": "Apache Apex\n\n\nApache Apex (incubating) is the industry\u2019s only Apache 2.0 licensed open source enterprise grade unified stream and batch processing engine.  Project Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex/#apache-apex", 
            "text": "Apache Apex (incubating) is the industry\u2019s only Apache 2.0 licensed open source enterprise grade unified stream and batch processing engine.  Project Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Apache Apex Deep Dive"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex-Malhar is an open source operator and codec library that can be used with the DataTorrent platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of DataTorrent RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 DataTorrent Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 DataTorrent Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code. \nSome operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application. \n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted. \n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like DataTorrent RTS is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis. \n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows. \n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into a DataTorrent streaming application. E.g. In the Telco space, a Java based CDR parser can be directly plugged into DataTorrent RTS. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files \n Syslog.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nDataTorrent supports an operator to connect to the popular Twitter stream fire hose", 
            "title": "Apache Apex-Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex-Malhar is an open source operator and codec library that can be used with the DataTorrent platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of DataTorrent RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 DataTorrent Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 DataTorrent Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.   Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.    Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like DataTorrent RTS is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.   Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.   Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into a DataTorrent streaming application. E.g. In the Telco space, a Java based CDR parser can be directly plugged into DataTorrent RTS. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files   Syslog.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "DataTorrent supports an operator to connect to the popular Twitter stream fire hose", 
            "title": "Social Media"
        }, 
        {
            "location": "/dtingest/", 
            "text": "dtIngest Tutorial\n\n\n\"dtIngest\" is a datatorrent application that can read data from various\nsources and store the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the \ndtIngest\nblog\n.\n\n\nThis tutorial refers to dtIngest version 1.0.0\n\n\nPre-requisites\n\n\n\n\n\n\nDatatorrent RTS should be installed on your hadoop cluster. Please\n    refer \nInstallation\n    guide\n\u00a0for\n    installation steps.\n\n\n\n\n\n\nSource and destination file systems should be accessible from all\n    nodes of the cluster running DataTorrent RTS. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.\n\n\n\n\n\n\nIn case your source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.\n\n\n\n\n\n\nLaunching ingestion application\n\n\ndtIngest application can be configured and launched from \nDatatorrent\nManagement\nConsole\n.\n\n\n\n\n\n\nNavigate to 'Develop' tab.\n    \n\n\n\n\n\n\nThe dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0\n\n\n\n\n\n\nSelect 'Ingestion Application' from list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n    \n\n\n\n\n\n\nConfiguration page for dtingest is displayed after the 'launch'.\n    Just provide desired values for the configuration and click\n    'Launch' to ingest your data.\n\n\n\n\n\n\nConfiguring dtingest instance properties\n\n\n\n\n\n\nIn the 'Name this application' textbox; enter suitable name of\n    your choice for this ingestion instance. For example, 'Ingestion\n    test'\n    \n\n\n\n\n\n\nUnder 'Specify a queue' option; keep the 'Specify a queue'\n    checkbox unselected if you want to use default queue.\n\n\nIf you want to specify some specific queue to launch this application; then\nselect the 'Specify a queue' checkbox and select desired queue from the\ndropdown. To know more about this read \nHadoop Capacity Scheduler Docs\n\n\n\n\n\n\n\n\nUnder 'Use config a file' option; select the checkbox if you wish\n    to use some configuration which is already saved. In the drop down\n    select the desired configuration.\n    After selection, all the input options will load the values saved in the configuration.\n    \n\n\nAfter values are loaded you can modify the values of any option to the\ndesired value. You can also save this new combination of values as a\nconfiguration under the same name or different name.\n\n\nIf you want to specify all the options from scratch without using any pre-saved configuration; then unselect a checkbox 'Use a config file'\n\n\n\n\n\n\nConfigure input source, refer to \nConfiguring input\n    source\n\u00a0section for details.\n\n\n\n\n\n\nConfigure output destination, refer to \nConfiguring output\n    destination\n\u00a0section for details.\n\n\n\n\n\n\nConfigure processing steps, refer to \nConfiguring processing\n    steps\n\u00a0section for details.\n\n\n\n\n\n\nUnder 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.\u00a0\n\n\n\n\n\n\nConfiguring input source\n\n\nConfiguring HDFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n    \nhdfs://namenode1.cluster.company.org:8020/user/john/data\n.\n\n    Where,\n\n\n\n\nhdfs://\n indicates HDFS protocol\n\n\nnamenode1.cluster.company.org\n indicates fully qualified domain\n   name for the namenode of source HDFS.\n\n\n8020\n indicates port number for HDFS namenode service\n\n\n/user/john/data\n indicates full path for destination directory  \n\n\n\n\n\n\nIf there are more than one directories/file to be ingested; click on\n'Add directory' button. Then specify complete URL for the file path to be\ningested as specified above\n\n\n\n\n\n\nIn the 'Filtering criteria' field; specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n    \nJava regular expression  documentation\n.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.  \n\n\n\n\nWhere,\n-   \n.*\n indicates any character zero or more times\n-   \n\\.\n indicates dot escaped with backslash\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only '.log' files from the source\ndirectories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you expect\n    application to shutdown after ingesting files currently present in\n    the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based on\n'modification timestamp'. Entire file will be ingested again in case of\nany change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.  \n\n\n\n\n\n\n\n\nConfiguring\u00a0NFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'File/NFS' option from the\n    drop-down. \n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n\nFor example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk5/nfsmount/path/to/data/directory\n.\nWhere,\n-   \nfile://\n indicates that it is some file system mounted on the node.\n-   \n/disk5/nfsmount/\n indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-   \npath/to/data/directory\n is the directory to be ingested\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nIf there are more than one directories to be ingested; click on\n    'Add directory' button. Then specify complete URL for the file\n    path to be ingested as specified in point 3.\n\n\n\n\n\n\nIf there are specific files to be ingested; then specify complete\n    URL for the file path to be ingested.\n\n\nFor example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt\n.\n\n\n\n\n\n\n\n\nIn the 'Filtering criteria' field; specify regular expression for\n    files to be copied.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.\n\n\n\n\nWhere,\n-   \n.\\*\n indicates any character zero or more times\n-   \n\\\\.\n indicates dot escaped with backslash '\\'\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only \n.log\n files from the source directories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you expect application\n    to shutdown after ingesting files currently present in the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based\non 'modification timestamp'. Entire file will be ingested again in case\nof any change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.\n\n\n\n\n\n\n\n\nConfiguring FTP input\n\n\nFTP is most widely used transfer protocol, used for transfer files from\none host to another. Use dtIngest, to copy files/directory from ftp\nsource location to some destination. This section gives details about\nhow to ingest files/directories from FTP using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0FTP as input type\n    \n\n\n\n\n\n\nAfter selecting the FTP as input type, snapshot of UI as below:     \n\n\n\n\n\n\nConfigure FTP input url\n    Input url for FTP needs to be provided in following format,  \nftp://username:password@host:port/path\n\n    where,\n\n\n\n\nftp\n : \u00a0protocol name\n\n\nusername\n : \u00a0username for ftp server\n\n\npassword\n : password\n\n\nhost\n : FTP host\n\n\nport\n : port number\n\n\npath\n : path to either file / directory\n\n\n\n\n\n\nIf you want to copy multiple files/directories, click on (+) button. If\nyou are copying multiple files, then UI would be as follows:\n\n\n\nIf you are copying multiple directories, then UI would be as follows:\n\n\n\n\n\n\n\nConfiguring Amazon S3 input\n\n\nS3 is clustered storage service of Amazon. Amazon s3 provides a web\nservices interface that can be used to store and retrieve any amount of\ndata, at any time, from anywhere on web. For more details about Amazon\nS3, please refer to \nAmazon S3\nDocumentation\n.\n\n\nUse dtIngest, you can copy files/directory efficiently from S3 source\nlocation to some destination. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0S3 as input type\n\n\n\n\n\n\n\nAfter selecting the S3 as source type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure S3 input url.\n\n\nInput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\nwhere,\n- \ns3n\n: \u00a0protocol name\n- \nukey\n: access key\n- \nupass\n: secret access key\n- \nbucketName\n : bucketName\n- \npath\n : path to either file / directory\n\n\n\n\n\n\n\n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n  \n\n\nConfiguring Kafka input\n\n\nKafka is a distributed publish subscribe messaging system. Since kafka\nis distributed system, topics are partitioned and replicated across\nnodes. Messages in kafka are expired after some time. For more details\nabout Kafka, please refer to \nApache Kafka\nDocumentation\n.\n\n\nWe need to copy messages from multiple partitions to one destination\nbefore expiry. This section gives details about how to ingest messages\nfrom kafka using dtIngest.\n\n\n\n\n\n\nSelect kafka as input type\n    \n\n\n\n\n\n\nAfter selecting kafka as input type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure topic name and zookeeper quorum.\n    zookeeper quorum \u00a0is a string in the form of\n    \nhostname1:port1,hostname2:port2,hostname3:port3\n\n\nwhere,\n- \nhostname1,hostname2,hostname3\n are hosts\n- \nport1,port2,port3\n are ports of zookeeper server\n\n\ne.g. localhost:2181,localhost:2182\n\n\n\n\n\n\n\nSelect the offset type. By default, \u201cLatest\u201d offset is enabled. If\n    you want to consume messages from beginning of kafka queue, then\n    select \u201cEarliest\u201d offset option.\n\n\n\n\n\n\nIf the topic name is same across the kafka clusters and want to\n    ingest data from these clusters, then configure the zookeeper quorum\n    as follows:\n\n\nc1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere,\n- \nc1,c2,c3\n indicates the cluster names,\n- \nhs1,hs2,hs3,hs4,hs5,hs6\n are zookeeper host names\n- \np1,p2,p3,p4,p5,p6\n are corresponding ports.\n\n\nFor\u00a0example, ClusterA and ClusterB are 2 kafka clusters as below, then\nzookeeper quorum would be as \nClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181\n\n\n\n\n\n\n\n\nConfiguring JMS input\n\n\nJMS provides the facility to create, send and read messages. You can\ncopy messages from JMS source location to some destination using\ndtIngest. This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0\n\n\n\n\n\n\nSelect JMS as input type.\n    \n\n\n\n\n\n\nAfter selecting the JMS as source type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure Broker URL and topic name.\n    Broker url would be in the form of tcp://hostName:port\n    \n\n\n\n\n\n\nConfiguring\u00a0output destination\n\n\nConfiguring HDFS output\n\n\n\n\n\n\nFor 'Output Location' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Target directory' specify complete URL for the HDFS path\n    for the destination directory. For example, URL should be like\n\nhdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory\n\n\n\n\n\n\n\n\nWhere,\n    - \nhdfs://\n indicates HDFS protocol\n    - \nnamenode1.cluster.company.org\n indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    - \n:8020\n indicates port number for HDFS namenode service\n    - \n/user/username/path/to/destination/directory\n indicates full path\n    for destination directory.\n\n\n\n\n\n\nUnder 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.\n\n\n\n\n\n\nUnder 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.\n\n\n\n\n\n\nCompact files\n\n\nIf you want to copy the data to the HDFS and partition the data into\npartitions of fixed size; use 'Compact files' feature to do this. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Additionally, you may also use this to breakdown very\nlarge file into partitions of manageable size.\n\n\n\n\n\n\n\n\nSelect 'yes' for radio button under 'Compact files' option. This\n    will display other additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.\n\n\n\n\n\n\nSelect delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with \n\\\n. For example, tab character\n    \n\\t\n should be specified as \n\\\\t\n.\n\n\n\n\n\n\nSpecify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.\n\n\n\n\n\n\nNote that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.\n\n\nConfiguring NFS output\n\n\n\n\n\n\nFor \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down  \n\n\n\n\n\n\n\n\nUnder \u2018Target directory\u2019 specify complete URL for NFS path for\n    destination directory.\n\n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n    \nfile:///disk5/nfsmount/path/to/data/directory\n\n\nWhere,\n\n\n\n\nfile://\n indicates that it is some file system mounted on the node.\n\n\n/disk5/nfsmount/\n indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.\n\n\npath/to/data/directory\n is the directory to be ingested\n\n\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nConfiguring FTP output\n\n\n\n\n\n\nSelect FTP as output type.\n    \n\n\n\n\n\n\nAfter selecting FTP as output type then UI looks like as below:   \n\n\n\n\n\n\nSpecify the destination url below the \u201cOutput directory\u201d label.\n    Output url for FTP needs to be provided in following format,  \nftp://username:password@host:port/path\n\n\nWhere,\n- \nftp\n : \u00a0protocol name\n- \nusername\n : username for ftp server\n- \npassword\n : password\n- \nhost\n : FTP host\n- \nport\n : port number\n- \npath\n : Directory path to ingested\n\n\n\n\n\n\n\n\nConfiguring Amazon S3 output\n\n\n\n\n\n\nSelect S3 as output type.\n    \n\n\n\n\n\n\nAfter selecting S3 as output then UI looks like as below:\n    \n\n\n\n\n\n\nSpecify the destination url below the 'Output directory' label.\noutput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\n    Where,\n\n\n\n\ns3n\n\u00a0: protocol name\n\n\nukey\n : access key\n\n\nupass\n :\u00a0secret access key\n\n\nbucketName\n : \u00a0bucketName\n\n\npath\n : Directory path\n\n\n\n\n\n\n\n\nConfiguring Kafka output\n\n\n\n\n\n\nSelect kafka as output type.\n    \n\n\n\n\n\n\nAfter Selecting kafka as output then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure broker list and topic name.\n\n\n\n\n\n\nConfiguring JMS output\n\n\n\n\n\n\nSelect JMS as output type.\n\n\n\n\n\n\n\nAfter selecting JMS as output type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure Broker URL and topic name. Broker URL would be in the form of tcp://host:port\n\n\n\n\n\n\n\nConfiguring processing steps\n\n\nConfiguring compression\n\n\nSelect compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\nSelect LZO radio button to apply LZO compression\n\n\n\n\n\n\nLzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/\n\n\n\n\nSelect GZIP radio button to apply GZIP compression\n\n\n\n\n\nConfiguring encryption\n\n\nSelect type of encryption on configuration page.\n\n\n\n\n\nApply AES encryption:\n\n\n\n\nSelect AES radio button to apply AES encryption\n\n\n\n\n\n\nProvide AES symmetric encryption key in \u201cAES key\u201d text box\n\n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.\n\n\n\n\n\n\n\nApply PKI encryption:  \n\n\n\n\nSelect PKI encryption button to apply PKI encryption  \n\n\nProvide Asymmetric public key to be used for PKI encryption", 
            "title": "dtIngest"
        }, 
        {
            "location": "/dtingest/#dtingest-tutorial", 
            "text": "\"dtIngest\" is a datatorrent application that can read data from various\nsources and store the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the  dtIngest\nblog .  This tutorial refers to dtIngest version 1.0.0", 
            "title": "dtIngest Tutorial"
        }, 
        {
            "location": "/dtingest/#pre-requisites", 
            "text": "Datatorrent RTS should be installed on your hadoop cluster. Please\n    refer  Installation\n    guide \u00a0for\n    installation steps.    Source and destination file systems should be accessible from all\n    nodes of the cluster running DataTorrent RTS. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.    In case your source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/dtingest/#launching-ingestion-application", 
            "text": "dtIngest application can be configured and launched from  Datatorrent\nManagement\nConsole .    Navigate to 'Develop' tab.\n        The dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0    Select 'Ingestion Application' from list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n        Configuration page for dtingest is displayed after the 'launch'.\n    Just provide desired values for the configuration and click\n    'Launch' to ingest your data.", 
            "title": "Launching ingestion application"
        }, 
        {
            "location": "/dtingest/#configuring-dtingest-instance-properties", 
            "text": "In the 'Name this application' textbox; enter suitable name of\n    your choice for this ingestion instance. For example, 'Ingestion\n    test'\n        Under 'Specify a queue' option; keep the 'Specify a queue'\n    checkbox unselected if you want to use default queue.  If you want to specify some specific queue to launch this application; then\nselect the 'Specify a queue' checkbox and select desired queue from the\ndropdown. To know more about this read  Hadoop Capacity Scheduler Docs     Under 'Use config a file' option; select the checkbox if you wish\n    to use some configuration which is already saved. In the drop down\n    select the desired configuration.\n    After selection, all the input options will load the values saved in the configuration.\n      After values are loaded you can modify the values of any option to the\ndesired value. You can also save this new combination of values as a\nconfiguration under the same name or different name.  If you want to specify all the options from scratch without using any pre-saved configuration; then unselect a checkbox 'Use a config file'    Configure input source, refer to  Configuring input\n    source \u00a0section for details.    Configure output destination, refer to  Configuring output\n    destination \u00a0section for details.    Configure processing steps, refer to  Configuring processing\n    steps \u00a0section for details.    Under 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.", 
            "title": "Configuring dtingest instance properties"
        }, 
        {
            "location": "/dtingest/#configuring-input-source", 
            "text": "Configuring HDFS input    For 'Input data source' field; select 'HDFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested. \n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n     hdfs://namenode1.cluster.company.org:8020/user/john/data . \n    Where,   hdfs://  indicates HDFS protocol  namenode1.cluster.company.org  indicates fully qualified domain\n   name for the namenode of source HDFS.  8020  indicates port number for HDFS namenode service  /user/john/data  indicates full path for destination directory      If there are more than one directories/file to be ingested; click on\n'Add directory' button. Then specify complete URL for the file path to be\ningested as specified above    In the 'Filtering criteria' field; specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n     Java regular expression  documentation .\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.     Where,\n-    .*  indicates any character zero or more times\n-    \\.  indicates dot escaped with backslash\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only '.log' files from the source\ndirectories.    Under 'Runs' field, select 'Single run' if you expect\n    application to shutdown after ingesting files currently present in\n    the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based on\n'modification timestamp'. Entire file will be ingested again in case of\nany change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.       Configuring\u00a0NFS input    For 'Input data source' field; select 'File/NFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested.  For example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk5/nfsmount/path/to/data/directory .\nWhere,\n-    file://  indicates that it is some file system mounted on the node.\n-    /disk5/nfsmount/  indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-    path/to/data/directory  is the directory to be ingested   Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    If there are more than one directories to be ingested; click on\n    'Add directory' button. Then specify complete URL for the file\n    path to be ingested as specified in point 3.    If there are specific files to be ingested; then specify complete\n    URL for the file path to be ingested.  For example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt .     In the 'Filtering criteria' field; specify regular expression for\n    files to be copied.\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.   Where,\n-    .\\*  indicates any character zero or more times\n-    \\\\.  indicates dot escaped with backslash '\\'\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only  .log  files from the source directories.    Under 'Runs' field, select 'Single run' if you expect application\n    to shutdown after ingesting files currently present in the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based\non 'modification timestamp'. Entire file will be ingested again in case\nof any change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.     Configuring FTP input  FTP is most widely used transfer protocol, used for transfer files from\none host to another. Use dtIngest, to copy files/directory from ftp\nsource location to some destination. This section gives details about\nhow to ingest files/directories from FTP using dtIngest. \u00a0    Select \u00a0FTP as input type\n        After selecting the FTP as input type, snapshot of UI as below:         Configure FTP input url\n    Input url for FTP needs to be provided in following format,   ftp://username:password@host:port/path \n    where,   ftp  : \u00a0protocol name  username  : \u00a0username for ftp server  password  : password  host  : FTP host  port  : port number  path  : path to either file / directory    If you want to copy multiple files/directories, click on (+) button. If\nyou are copying multiple files, then UI would be as follows:  If you are copying multiple directories, then UI would be as follows:    Configuring Amazon S3 input  S3 is clustered storage service of Amazon. Amazon s3 provides a web\nservices interface that can be used to store and retrieve any amount of\ndata, at any time, from anywhere on web. For more details about Amazon\nS3, please refer to  Amazon S3\nDocumentation .  Use dtIngest, you can copy files/directory efficiently from S3 source\nlocation to some destination. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0    Select \u00a0S3 as input type    After selecting the S3 as source type then UI looks like as below:    Configure S3 input url.  Input url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path \nwhere,\n-  s3n : \u00a0protocol name\n-  ukey : access key\n-  upass : secret access key\n-  bucketName  : bucketName\n-  path  : path to either file / directory    \n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n    Configuring Kafka input  Kafka is a distributed publish subscribe messaging system. Since kafka\nis distributed system, topics are partitioned and replicated across\nnodes. Messages in kafka are expired after some time. For more details\nabout Kafka, please refer to  Apache Kafka\nDocumentation .  We need to copy messages from multiple partitions to one destination\nbefore expiry. This section gives details about how to ingest messages\nfrom kafka using dtIngest.    Select kafka as input type\n        After selecting kafka as input type then UI looks like as below:\n        Configure topic name and zookeeper quorum.\n    zookeeper quorum \u00a0is a string in the form of\n     hostname1:port1,hostname2:port2,hostname3:port3  where,\n-  hostname1,hostname2,hostname3  are hosts\n-  port1,port2,port3  are ports of zookeeper server  e.g. localhost:2181,localhost:2182    Select the offset type. By default, \u201cLatest\u201d offset is enabled. If\n    you want to consume messages from beginning of kafka queue, then\n    select \u201cEarliest\u201d offset option.    If the topic name is same across the kafka clusters and want to\n    ingest data from these clusters, then configure the zookeeper quorum\n    as follows:  c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where,\n-  c1,c2,c3  indicates the cluster names,\n-  hs1,hs2,hs3,hs4,hs5,hs6  are zookeeper host names\n-  p1,p2,p3,p4,p5,p6  are corresponding ports.  For\u00a0example, ClusterA and ClusterB are 2 kafka clusters as below, then\nzookeeper quorum would be as  ClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181     Configuring JMS input  JMS provides the facility to create, send and read messages. You can\ncopy messages from JMS source location to some destination using\ndtIngest. This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0    Select JMS as input type.\n        After selecting the JMS as source type then UI looks like as below:\n        Configure Broker URL and topic name.\n    Broker url would be in the form of tcp://hostName:port", 
            "title": "Configuring input source"
        }, 
        {
            "location": "/dtingest/#configuring-output-destination", 
            "text": "Configuring HDFS output    For 'Output Location' field; select 'HDFS' option from the\n    drop-down.     Under 'Target directory' specify complete URL for the HDFS path\n    for the destination directory. For example, URL should be like hdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory     Where,\n    -  hdfs://  indicates HDFS protocol\n    -  namenode1.cluster.company.org  indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    -  :8020  indicates port number for HDFS namenode service\n    -  /user/username/path/to/destination/directory  indicates full path\n    for destination directory.    Under 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.    Under 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.    Compact files  If you want to copy the data to the HDFS and partition the data into\npartitions of fixed size; use 'Compact files' feature to do this. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Additionally, you may also use this to breakdown very\nlarge file into partitions of manageable size.     Select 'yes' for radio button under 'Compact files' option. This\n    will display other additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.    Select delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with  \\ . For example, tab character\n     \\t  should be specified as  \\\\t .    Specify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.    Note that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.  Configuring NFS output    For \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down       Under \u2018Target directory\u2019 specify complete URL for NFS path for\n    destination directory. \n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n     file:///disk5/nfsmount/path/to/data/directory  Where,   file://  indicates that it is some file system mounted on the node.  /disk5/nfsmount/  indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.  path/to/data/directory  is the directory to be ingested    Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    Configuring FTP output    Select FTP as output type.\n        After selecting FTP as output type then UI looks like as below:       Specify the destination url below the \u201cOutput directory\u201d label.\n    Output url for FTP needs to be provided in following format,   ftp://username:password@host:port/path  Where,\n-  ftp  : \u00a0protocol name\n-  username  : username for ftp server\n-  password  : password\n-  host  : FTP host\n-  port  : port number\n-  path  : Directory path to ingested     Configuring Amazon S3 output    Select S3 as output type.\n        After selecting S3 as output then UI looks like as below:\n        Specify the destination url below the 'Output directory' label.\noutput url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path \n    Where,   s3n \u00a0: protocol name  ukey  : access key  upass  :\u00a0secret access key  bucketName  : \u00a0bucketName  path  : Directory path     Configuring Kafka output    Select kafka as output type.\n        After Selecting kafka as output then UI looks like as below:\n        Configure broker list and topic name.    Configuring JMS output    Select JMS as output type.    After selecting JMS as output type then UI looks like as below:    Configure Broker URL and topic name. Broker URL would be in the form of tcp://host:port", 
            "title": "Configuring\u00a0output destination"
        }, 
        {
            "location": "/dtingest/#configuring-processing-steps", 
            "text": "Configuring compression  Select compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    Select LZO radio button to apply LZO compression    Lzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/   Select GZIP radio button to apply GZIP compression   Configuring encryption  Select type of encryption on configuration page.   Apply AES encryption:   Select AES radio button to apply AES encryption    Provide AES symmetric encryption key in \u201cAES key\u201d text box \n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.    Apply PKI encryption:     Select PKI encryption button to apply PKI encryption    Provide Asymmetric public key to be used for PKI encryption", 
            "title": "Configuring processing steps"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Quick Start"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Installation"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Security"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "dtCli"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/create_wordcount_app/", 
            "text": "Create a Word Count Application in Java\n\n\nComing Soon!", 
            "title": "Top N Words"
        }, 
        {
            "location": "/create_wordcount_app/#create-a-word-count-application-in-java", 
            "text": "Coming Soon!", 
            "title": "Create a Word Count Application in Java"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Twitter Top Hashtags"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/create/", 
            "text": "Sales Dimensions - Transform, Analyze and Alert\n\n\nSales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.\n\n\nA large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.\n\n\nIn order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.\n\n\nApplication setup will require following steps:\n\n\n\n\nInput\n - receive individual sales transactions\n\n\nTransform\n - convert incoming records into consumable form\n\n\nEnrich\n - provide additional information for each record by performing additional lookups\n\n\nCompute\n - perform aggregate computations on all possible key field combinations\n\n\nStore\n - store computed results for further analysis and visualizations\n\n\nAnalyze\n, Alert \n Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.\n\n\n\n\nCreate New Application\n\n\nDataTorrent platforms supports building new applications with \nGraphical Application Builder\n, which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.\n\n\nGo to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.\n\n\nThis will bring up the Application Builder interface\n\n\n\n\nAdd and Connect Operators\n\n\nFrom the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.\n\n\n\n\nJSON Sales Event Generator (Input)\n - generates synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.\n\n\nEnrichment (Enrich)\n - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n - stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n - dashboard connector for visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n - dashboard connector for visualization data results.\n\n\n\n\nConnect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.\n\n\n\n\nCustomize Application and Operator Settings\n\n\nBy clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:\n\n\n\n\n\n\nCopy the Sales schema below and paste the contents into the \nEvent Schema JSON\n field of \nInput\n operator, and \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n operators.\n\n\n{\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.  Set the \nBase Path\n to \nSalesDimensionsDemoStore\n value.  This sets the HDHT storage path to write dimensions computation results to the \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n\n\n\n\n\n\n\n\nClick on the stream and set the \nStream Locality\n to \nCONTAINER_LOCAL\n for all the streams between \nInput\n and \nCompute\n operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.\n\n\n\n\n\n\nLaunch Application\n\n\nOnce application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it. \n\n\n\n\nAfter launching, go to \nSales Dimensions\n application operations page in the \nMonitor\n section.\n\n\n\n\nConfirm that the application is launched successfully by looking for \nRunning\n state in the \nApplication Overview\n section, and all confirming all the operators are successfully started under \nStram Events\n section.  By navigating to \nPhysical\n view tab, and looking at \nInput\n, \nParse\n, \nEnrich\n, or \nCompute\n operators, notice that they are all deployed to the single container, thanks to the stream locality setting of \nCONTAINER_LOCAL\n we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.\n\n\n\n\nVisualize Data\n\n\nDataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see \nData Visualization\n tutorial.\n\n\nAfter application is started, a \nvisualize\n button, available in the \nApplication Overview\n section, can be used to quickly generate a new dashboard for the Sales Dimensions application.\n\n\n\n\nOnce dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Sales Dimenions"
        }, 
        {
            "location": "/create/#sales-dimensions-transform-analyze-and-alert", 
            "text": "Sales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.  A large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.  In order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.  Application setup will require following steps:   Input  - receive individual sales transactions  Transform  - convert incoming records into consumable form  Enrich  - provide additional information for each record by performing additional lookups  Compute  - perform aggregate computations on all possible key field combinations  Store  - store computed results for further analysis and visualizations  Analyze , Alert   Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.", 
            "title": "Sales Dimensions - Transform, Analyze and Alert"
        }, 
        {
            "location": "/create/#create-new-application", 
            "text": "DataTorrent platforms supports building new applications with  Graphical Application Builder , which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.  Go to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.  This will bring up the Application Builder interface", 
            "title": "Create New Application"
        }, 
        {
            "location": "/create/#add-and-connect-operators", 
            "text": "From the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.   JSON Sales Event Generator (Input)  - generates synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.  Enrichment (Enrich)  - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.  Simple App Data Dimensions Store (Store)  - stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)  - dashboard connector for visualization queries.  App Data Pub Sub Result (Result)  - dashboard connector for visualization data results.   Connect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.", 
            "title": "Add and Connect Operators"
        }, 
        {
            "location": "/create/#customize-application-and-operator-settings", 
            "text": "By clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:    Copy the Sales schema below and paste the contents into the  Event Schema JSON  field of  Input  operator, and  Configuration Schema JSON  of the  Compute  and  Store  operators.  {\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to  SalesDimensionsQuery  and  SalesDimensionsResult  respectively.    Select the  Store  operator, and edit the  File Store  property.  Set the  Base Path  to  SalesDimensionsDemoStore  value.  This sets the HDHT storage path to write dimensions computation results to the  /user/ username /SalesDimensionsDemoStore  on HDFS.     Click on the stream and set the  Stream Locality  to  CONTAINER_LOCAL  for all the streams between  Input  and  Compute  operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.", 
            "title": "Customize Application and Operator Settings"
        }, 
        {
            "location": "/create/#launch-application", 
            "text": "Once application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it.    After launching, go to  Sales Dimensions  application operations page in the  Monitor  section.   Confirm that the application is launched successfully by looking for  Running  state in the  Application Overview  section, and all confirming all the operators are successfully started under  Stram Events  section.  By navigating to  Physical  view tab, and looking at  Input ,  Parse ,  Enrich , or  Compute  operators, notice that they are all deployed to the single container, thanks to the stream locality setting of  CONTAINER_LOCAL  we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.", 
            "title": "Launch Application"
        }, 
        {
            "location": "/create/#visualize-data", 
            "text": "DataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see  Data Visualization  tutorial.  After application is started, a  visualize  button, available in the  Application Overview  section, can be used to quickly generate a new dashboard for the Sales Dimensions application.   Once dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Visualize Data"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Applications"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/ApexPackage/", 
            "text": "Apex Application Packages\n\n\n1. Introduction\n\n\nAn Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Project Apex. It is the\nstandard way for assembling and sharing an Apex application. \n\n\n2. Requirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the App Package)\n\n\nApex 3.0.0 or later (for launching the App Package in your cluster)\n\n\n\n\n3. Creating Your First Apex App Package\n\n\nYou can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):\n\n\n $ mvn archetype:generate                                                \n -DarchetypeRepository=https://www.datatorrent.com/maven/content/reposito \n ries/releases                                                            \n -DarchetypeGroupId=com.datatorrent                                       \n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.0.0        \n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \n -Dversion=1.0-SNAPSHOT                                                   \n\n\n\n\nThis creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:\n\n\n $cd mydtapp; mvn package\n\n\n\n\nThis builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:\n\n\n -------------------------------------------------------                  \n\n  TESTS                                                               \n\n -------------------------------------------------------                  \n\n Running com.example.mydtapp.ApplicationTest                              \n\n hello world: 0.8015370953286478                                        \n\n hello world: 0.9785359225545481                                          \n\n hello world: 0.6322611586644047                                          \n\n hello world: 0.8460953663451775                                          \n\n hello world: 0.5719372906929072                                          \n\n hello world: 0.6361174312337172                                          \n\n hello world: 0.14873007534816318                                         \n\n hello world: 0.8866986277418261                                          \n\n hello world: 0.6346526809866057                                          \n                                                                              hello world: 0.48587295703904465                                         \n\n hello world: 0.6436832429676687                                          \n\n ...                                                                      \n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863   \n sec                                                                      \n\n Results :                                                                \n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0                         \n\n\n\n\nThe \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as shown below.\n\n\n\n\nGroup ID: com.datatorrent\nArtifact ID: apex-app-archetype\nVersion: 3.0.0 (or any later version)\n\n\nRepository:\n\nhttps://www.datatorrent.com/maven/content/repositories/releases\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own DataTorrent App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.\n\n\n4. Writing Your Own App Package\n\n\nPlease refer to the \nApplication Developer Guide\n on the basics on how to write a DataTorrent application.  In your AppPackage project, you can add custom operators (refer to \nOperator Developer Guide\n), project dependencies, default and required configuration properties, pre-set configurations and other metadata.\n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\ncom.datatorrent\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${datatorrent.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--    \n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\ncom.datatorrent\n/groupId\n\n      \nartifactId\ndt-engine\n/artifactId\n\n      \nversion\n${datatorrent.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n                                                   \n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for     \n com.datatorrent:malhar-library:jar with value '*' does not match a      \n valid id pattern.                                                        \n\n [WARNING]                                                              \n [WARNING] It is highly recommended to fix these problems because they    \n threaten the stability of your build.                                  \n [WARNING]                                                               \n [WARNING] For this reason, future Maven versions might no longer support \n building such malformed projects.                                       \n [WARNING]                                                                \n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n \n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\ndt.attr.\nattribute\n. The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.attr.\nattribute\n. The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10\n\n\nproperty\n\n  \nname\ndt.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\ndt.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.\n\n\n  \nproperty\n\n    \nname\ndt.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and\n\ndt.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.\n\n\nproperty\n\n\nname\ndt.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\ndt.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\ndt.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\ndt.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n \n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\ndt.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    dt.attr.APPLICATION_NAME\n\n\n\nThere are also other properties that can be set.  For details on\nproperties, refer to the \nOperation and Installation Guide\n.\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\nin your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n                                                                 \ndatatorrent.version\n3.0.0\n/datatorrent.version\n                             \ndatatorrent.apppackage.classpath\\\nlib*.jar\n/datatorrent.apppackage.classpath\n\n\n/properties\n\n\n\n\n\ndatatorrent.version is the DataTorrent RTS version that are to be used\nwith this Application Package.\n\n\ndatatorrent.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE                                           \n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender                 \n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout            \n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p    \n %c{2} %M - %m%n                                                          \n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\n5. Zip Structure of Application Package\n\n\nDataTorrent Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.\n\n\n\n\n6. Managing Application Packages Through DT Gateway\n\n\nThe DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.\n\n\nStoring an Application Package\n\n\nYou can store your Application Packages through DT Gateway using this\nREST call:\n\n\n POST /ws/v2/appPackages \n\n\n\n\nThe payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:\n\n\n$ curl -XPOST -T \napp-package-file\n http://localhost:9090/ws/v2/appPackages \n\n\n\n\nGetting Meta Information on Application Packages\n\n\nYou can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent} \n\n\n\n\nThe parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this call.\n\n\nGET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}\n\n\n\n\nGetting List of Pre-Set Configurations in Application Package\n\n\nYou can get a list of pre-set configurations within the Application\nPackage using this call.  \n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs\n\n\n\n\nYou can also get the content of a specific pre-set configuration within\nthe Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  \n\n\n\n\nChanging Pre-Set Configurations in Application Package\n\n\nYou can create or replace pre-set configurations within the Application\nPackage\n\n\n PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nThe payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.\n\n\n DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}   \n\n\n\n\nRetrieving an Application Package\n\n\nYou can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download\n\n\n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\nPOST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}                                              \n\n\n\n\nThe config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:\n\n\n {\nproperty-name\n:\nproperty-value\n, ... }\n\n\n\n\nHere is an example of launching an application through curl:\n\n\n $ curl -XPOST -d'{\ndt.operator.console.prop.stringFormat\n:\nxyz %s\n}'    \n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app \n lications/MyFirstApplication/launch                      \n\n\n\n\nPlease refer to the \nGateway API reference\n for the complete specification of the REST API.\n\n\n7. Examining and Launching Application Packages Through DT CLI\n\n\nIf you are working with Application Packages in the local filesystem and\ndo not want to deal with the DT Gateway, you can use the DataTorrent\nCommand Line Interface (DT CLI).  Please refer to the \nGateway API reference\n \nto see samples for these commands.\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis DT CLI command.  \n\n\n dt\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n dt\n get-app-package-operators \napp-package-file\n \npackage-prefix\n \n [parent-class] \n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\ndt\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\ndt\n launch [-D property-name=property-value, ...] [-conf config-name]   \n [-apconf config-file-within-app-package] \napp-package-file\n            \n [matching-app-name]                                                      \n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.\n\n\n\u00a9 2014-2015 DataTorrent Inc.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/ApexPackage/#apex-application-packages", 
            "text": "", 
            "title": "Apex Application Packages"
        }, 
        {
            "location": "/ApexPackage/#1-introduction", 
            "text": "An Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Project Apex. It is the\nstandard way for assembling and sharing an Apex application.", 
            "title": "1. Introduction"
        }, 
        {
            "location": "/ApexPackage/#2-requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the App Package)  Apex 3.0.0 or later (for launching the App Package in your cluster)", 
            "title": "2. Requirements"
        }, 
        {
            "location": "/ApexPackage/#3-creating-your-first-apex-app-package", 
            "text": "You can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.", 
            "title": "3. Creating Your First Apex App Package"
        }, 
        {
            "location": "/ApexPackage/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):   $ mvn archetype:generate                                                \n -DarchetypeRepository=https://www.datatorrent.com/maven/content/reposito \n ries/releases                                                            \n -DarchetypeGroupId=com.datatorrent                                       \n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.0.0        \n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \n -Dversion=1.0-SNAPSHOT                                                     This creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:   $cd mydtapp; mvn package  This builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:   -------------------------------------------------------                  \n\n  TESTS                                                               \n\n -------------------------------------------------------                  \n\n Running com.example.mydtapp.ApplicationTest                              \n\n hello world: 0.8015370953286478                                        \n\n hello world: 0.9785359225545481                                          \n\n hello world: 0.6322611586644047                                          \n\n hello world: 0.8460953663451775                                          \n\n hello world: 0.5719372906929072                                          \n\n hello world: 0.6361174312337172                                          \n\n hello world: 0.14873007534816318                                         \n\n hello world: 0.8866986277418261                                          \n\n hello world: 0.6346526809866057                                          \n                                                                              hello world: 0.48587295703904465                                         \n\n hello world: 0.6436832429676687                                          \n\n ...                                                                      \n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863   \n sec                                                                      \n\n Results :                                                                \n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0                           The \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/ApexPackage/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as shown below.   Group ID: com.datatorrent\nArtifact ID: apex-app-archetype\nVersion: 3.0.0 (or any later version)  Repository: https://www.datatorrent.com/maven/content/repositories/releases  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own DataTorrent App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/ApexPackage/#4-writing-your-own-app-package", 
            "text": "Please refer to the  Application Developer Guide  on the basics on how to write a DataTorrent application.  In your AppPackage project, you can add custom operators (refer to  Operator Developer Guide ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.", 
            "title": "4. Writing Your Own App Package"
        }, 
        {
            "location": "/ApexPackage/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId com.datatorrent /groupId \n       artifactId malhar-library /artifactId \n       version ${datatorrent.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--    \n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId com.datatorrent /groupId \n       artifactId dt-engine /artifactId \n       version ${datatorrent.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies                                                      By default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for     \n com.datatorrent:malhar-library:jar with value '*' does not match a      \n valid id pattern.                                                        \n\n [WARNING]                                                              \n [WARNING] It is highly recommended to fix these problems because they    \n threaten the stability of your build.                                  \n [WARNING]                                                               \n [WARNING] For this reason, future Maven versions might no longer support \n building such malformed projects.                                       \n [WARNING]                                                                  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/ApexPackage/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/ApexPackage/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter dt.attr. attribute . The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/ApexPackage/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter dt.operator. operator-name .attr. attribute . The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10  property \n   name dt.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/ApexPackage/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter dt.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.     property \n     name dt.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.", 
            "title": "Operator properties"
        }, 
        {
            "location": "/ApexPackage/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter dt.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and dt.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.  property  name dt.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/ApexPackage/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter dt.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name dt.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/ApexPackage/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name dt.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/ApexPackage/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration    The name of an application-specific property takes the form of:  dt.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      dt.attr.APPLICATION_NAME  There are also other properties that can be set.  For details on\nproperties, refer to the  Operation and Installation Guide .  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/ApexPackage/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/ApexPackage/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/ApexPackage/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/ApexPackage/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties                                                                   datatorrent.version 3.0.0 /datatorrent.version                               datatorrent.apppackage.classpath\\ lib*.jar /datatorrent.apppackage.classpath  /properties   datatorrent.version is the DataTorrent RTS version that are to be used\nwith this Application Package.  datatorrent.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/ApexPackage/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE                                           \n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender                 \n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout            \n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p    \n %c{2} %M - %m%n                                                            The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/ApexPackage/#5-zip-structure-of-application-package", 
            "text": "DataTorrent Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.", 
            "title": "5. Zip Structure of Application Package"
        }, 
        {
            "location": "/ApexPackage/#6-managing-application-packages-through-dt-gateway", 
            "text": "The DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.", 
            "title": "6. Managing Application Packages Through DT Gateway"
        }, 
        {
            "location": "/ApexPackage/#storing-an-application-package", 
            "text": "You can store your Application Packages through DT Gateway using this\nREST call:   POST /ws/v2/appPackages   The payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:  $ curl -XPOST -T  app-package-file  http://localhost:9090/ws/v2/appPackages", 
            "title": "Storing an Application Package"
        }, 
        {
            "location": "/ApexPackage/#getting-meta-information-on-application-packages", 
            "text": "You can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}", 
            "title": "Getting Meta Information on Application Packages"
        }, 
        {
            "location": "/ApexPackage/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}   The parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/ApexPackage/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this call.  GET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/ApexPackage/#getting-list-of-pre-set-configurations-in-application-package", 
            "text": "You can get a list of pre-set configurations within the Application\nPackage using this call.    GET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs  You can also get the content of a specific pre-set configuration within\nthe Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Getting List of Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/ApexPackage/#changing-pre-set-configurations-in-application-package", 
            "text": "You can create or replace pre-set configurations within the Application\nPackage   PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  The payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.   DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Changing Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/ApexPackage/#retrieving-an-application-package", 
            "text": "You can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download", 
            "title": "Retrieving an Application Package"
        }, 
        {
            "location": "/ApexPackage/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  POST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}                                                The config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:   { property-name : property-value , ... }  Here is an example of launching an application through curl:   $ curl -XPOST -d'{ dt.operator.console.prop.stringFormat : xyz %s }'    \n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app \n lications/MyFirstApplication/launch                        Please refer to the  Gateway API reference  for the complete specification of the REST API.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/ApexPackage/#7-examining-and-launching-application-packages-through-dt-cli", 
            "text": "If you are working with Application Packages in the local filesystem and\ndo not want to deal with the DT Gateway, you can use the DataTorrent\nCommand Line Interface (DT CLI).  Please refer to the  Gateway API reference  \nto see samples for these commands.", 
            "title": "7. Examining and Launching Application Packages Through DT CLI"
        }, 
        {
            "location": "/ApexPackage/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis DT CLI command.     dt  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/ApexPackage/#getting-available-operators-in-application-package_1", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   dt  get-app-package-operators  app-package-file   package-prefix  \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/ApexPackage/#getting-properties-of-operators-in-application-package_1", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  dt  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/ApexPackage/#launching-an-application-package_1", 
            "text": "You can launch an application within an Application Package.  dt  launch [-D property-name=property-value, ...] [-conf config-name]   \n [-apconf config-file-within-app-package]  app-package-file             \n [matching-app-name]                                                        Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.  \u00a9 2014-2015 DataTorrent Inc.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/AppConfPackage/", 
            "text": "Application Configuration Packages\n\n\nIntroduction\n\n\nAn Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an\n\nApplication Package\n using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the Config Package)\n\n\nApex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)\n\n\n\n\nCreating Your First Configuration Package\n\n\nYou can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:\n\n\n $ mvn archetype:generate                                                \n -DarchetypeRepository=https://www.datatorrent.com/maven/content/reposito \n ries/releases                                                            \n -DarchetypeGroupId=com.datatorrent                                       \n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.0.0       \n -DgroupId=com.example -Dpackage=com.example.mydtconfig                   \n -DartifactId=mydtconfig -Dversion=1.0-SNAPSHOT                           \n\n\n\n\n\nThis creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:\n\n\n$ mvn package                                                         \n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an application in your actual\nDataTorrent RTS installation.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.\n\n\n\n\nGroup ID: com.datatorrent\nArtifact ID: apex-conf-archetype\nVersion: 3.0.0 (or any later version)\n\n\nRepository:\n\nhttps://www.datatorrent.com/maven/content/repositories/releases\n\n\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n    ./pom.xml\n    ./src/main/resources/classpath\n    ./src/main/resources/files\n\n\n\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmydtconf\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy DataTorrent Application Configuration\n/name\n\n  \ndescription\nMy DataTorrent Application Configuration Description\n/description\n\n  \nproperties\n\n    \ndatatorrent.apppackage.name\nmydtapp\n/datatorrent.apppackage.name\n\n    \ndatatorrent.apppackage.minversion\n1.0.0\n/datatorrent.apppackage.minversion\n\n   \ndatatorrent.apppackage.maxversion\n1.9999.9999\n/datatorrent.apppackage.maxversion\n\n    \ndatatorrent.appconf.classpath\nclasspath/*\n/datatorrent.appconf.classpath\n\n    \ndatatorrent.appconf.files\nfiles/*\n/datatorrent.appconf.files\n\n  \n/properties\n \n\n\n\n\n\nIn pom.xml, you can change the values of  \ngroupId\n, \nversion\n,\n\nartifactId\n, \nname\nand \ndescription\n to your desired values.  \n\n\nYou can also change the values of \ndatatorrent.apppackage.name\n, \ndatatorrent.apppackage.minversion\n \ndatatorrent.apppackage.maxversion\n\nto reflect what app packages should be used with this configuration\npackage.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when\nyou issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  \n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files} \n\n\n\n\nLaunching with CLI\n\n\n-conf of the launch command in CLI supports specifying configuration\npackage in the local filesystem.  Example:\n\n\ndt> launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar\n\n\nThis command expects both the application package and the configuration\npackage to be in the local file system.\n\n\nRelated REST API\n\n\nPOST /ws/v2/configPackages\n\n\nPayload: Raw content of configuration package zip\nFunction: Creates or replace a configuration package zip file in HDFS\n\n\nCurl example:\n$ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages\n\n\nGET /ws/v2/configPackages?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages that the user is authorized\nto use and that are compatible with the specified appPackageName,\nappPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages under the specified user\nand that are compatible with the specified appPackageName,\nappPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n\n\nFunction: Returns the information of the specified configuration package\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n/download\n\n\nFunction: Returns the raw config package file\n\n\nCurl example:\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download > DTConfig-xyz.jar\n\n\n\\$ unzip -t DTConfig-xyz.jar\n\n\nPOST /ws/v2/appPackages/\nuser\n/\napp-pkg-name\n/\napp-pkg-version\n/applications/{app-name}/launch?configPackage=\nuser\n/\nconfpkgname\n\n\nFunction: Launches the app package with the specified configuration\npackage stored in HDFS.\n\n\nCurl example:\n\n\n$ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}\n\n\n\u00a9 2014-2015 DataTorrent Inc.", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/AppConfPackage/#application-configuration-packages", 
            "text": "", 
            "title": "Application Configuration Packages"
        }, 
        {
            "location": "/AppConfPackage/#introduction", 
            "text": "An Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an Application Package  using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.", 
            "title": "Introduction"
        }, 
        {
            "location": "/AppConfPackage/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the Config Package)  Apex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/AppConfPackage/#creating-your-first-configuration-package", 
            "text": "You can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.", 
            "title": "Creating Your First Configuration Package"
        }, 
        {
            "location": "/AppConfPackage/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:   $ mvn archetype:generate                                                \n -DarchetypeRepository=https://www.datatorrent.com/maven/content/reposito \n ries/releases                                                            \n -DarchetypeGroupId=com.datatorrent                                       \n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.0.0       \n -DgroupId=com.example -Dpackage=com.example.mydtconfig                   \n -DartifactId=mydtconfig -Dversion=1.0-SNAPSHOT                             This creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:  $ mvn package                                                           The \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an application in your actual\nDataTorrent RTS installation.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/AppConfPackage/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.   Group ID: com.datatorrent\nArtifact ID: apex-conf-archetype\nVersion: 3.0.0 (or any later version)  Repository: https://www.datatorrent.com/maven/content/repositories/releases   Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/AppConfPackage/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:      ./pom.xml\n    ./src/main/resources/classpath\n    ./src/main/resources/files  ./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/AppConfPackage/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId mydtconf /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My DataTorrent Application Configuration /name \n   description My DataTorrent Application Configuration Description /description \n   properties \n     datatorrent.apppackage.name mydtapp /datatorrent.apppackage.name \n     datatorrent.apppackage.minversion 1.0.0 /datatorrent.apppackage.minversion \n    datatorrent.apppackage.maxversion 1.9999.9999 /datatorrent.apppackage.maxversion \n     datatorrent.appconf.classpath classpath/* /datatorrent.appconf.classpath \n     datatorrent.appconf.files files/* /datatorrent.appconf.files \n   /properties    In pom.xml, you can change the values of   groupId ,  version , artifactId ,  name and  description  to your desired values.    You can also change the values of  datatorrent.apppackage.name ,  datatorrent.apppackage.minversion   datatorrent.apppackage.maxversion \nto reflect what app packages should be used with this configuration\npackage.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when\nyou issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/AppConfPackage/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/AppConfPackage/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/AppConfPackage/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.    Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:  properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.  After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/AppConfPackage/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/AppConfPackage/#launching-with-cli", 
            "text": "-conf of the launch command in CLI supports specifying configuration\npackage in the local filesystem.  Example:  dt> launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar  This command expects both the application package and the configuration\npackage to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/AppConfPackage/#related-rest-api", 
            "text": "", 
            "title": "Related REST API"
        }, 
        {
            "location": "/AppConfPackage/#post-wsv2configpackages", 
            "text": "Payload: Raw content of configuration package zip\nFunction: Creates or replace a configuration package zip file in HDFS  Curl example:\n$ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages", 
            "title": "POST /ws/v2/configPackages"
        }, 
        {
            "location": "/AppConfPackage/#get-wsv2configpackagesapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages that the user is authorized\nto use and that are compatible with the specified appPackageName,\nappPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/AppConfPackage/#get-wsv2configpackagesuserapppackagenameapppackageversion", 
            "text": "All query parameters are optional  Function: Returns the configuration packages under the specified user\nand that are compatible with the specified appPackageName,\nappPackageVersion and appName.", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;?appPackageName=...&amp;appPackageVersion=..."
        }, 
        {
            "location": "/AppConfPackage/#get-wsv2configpackagesusername", 
            "text": "Function: Returns the information of the specified configuration package", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;"
        }, 
        {
            "location": "/AppConfPackage/#get-wsv2configpackagesusernamedownload", 
            "text": "Function: Returns the raw config package file  Curl example:\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download > DTConfig-xyz.jar  \\$ unzip -t DTConfig-xyz.jar", 
            "title": "GET /ws/v2/configPackages/&lt;user&gt;/&lt;name&gt;/download"
        }, 
        {
            "location": "/AppConfPackage/#post-wsv2apppackagesuserapp-pkg-nameapp-pkg-versionapplicationsapp-namelaunchconfigpackageuserconfpkgname", 
            "text": "Function: Launches the app package with the specified configuration\npackage stored in HDFS.  Curl example:  $ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}  \u00a9 2014-2015 DataTorrent Inc.", 
            "title": "POST /ws/v2/appPackages/&lt;user&gt;/&lt;app-pkg-name&gt;/&lt;app-pkg-version&gt;/applications/{app-name}/launch?configPackage=&lt;user&gt;/&lt;confpkgname&gt;"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Operators"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "File splitter"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Block reader"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "File Input, S3, NFS"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "File Output"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Deduper"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "HDHT"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "DimensionsStore"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "DimenstionComputation"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "KafkaInput"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "KafkaOutput"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Solace"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "JMS"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "HBase"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "REST API"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Scalable Design"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Additional Documentation\n\n\nDataTorrent RTS\n\n\nFor more DataTorrent documentation visit\n\n\n\n\nFeatured Resources\n\n\nProduct Features\n\n\nArchitecture Overview\n\n\nDocumentation and Guides\n\n\n\n\nFor webinars and videos check out\n\n\n\n\nWebinars\n\n\nSolution Demos\n\n\n\n\nApache Apex\n\n\nTo find out more about Apache Apex visit\n\n\n\n\nApache Apex (incubating): \nhttp://apex.incubator.apache.org/\n\n\nApex Mailing List: \ndev@apex.incubator.apache.org\n\n\nApex Overview and Comparison", 
            "title": "Glossary"
        }, 
        {
            "location": "/additional_docs/#additional-documentation", 
            "text": "DataTorrent RTS  For more DataTorrent documentation visit   Featured Resources  Product Features  Architecture Overview  Documentation and Guides   For webinars and videos check out   Webinars  Solution Demos   Apache Apex  To find out more about Apache Apex visit   Apache Apex (incubating):  http://apex.incubator.apache.org/  Apex Mailing List:  dev@apex.incubator.apache.org  Apex Overview and Comparison", 
            "title": "Additional Documentation"
        }
    ]
}