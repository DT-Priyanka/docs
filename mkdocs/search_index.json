{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to DataTorrent RTS!\n\n\nDataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.  \n\n\n\n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n\n\n\n\n\n  \n\n    \n\n    \nRun Demos\n\n    \nExperience the power of DataTorrent RTS quickly. Import, launch, manage and visualize applications in minutes.\n\n  \n\n\n  \n\n\n  \n\n    \n\n    \nDiscover RTS\n\n    \nLearn about the architecture and the rich feature set of the DataTorrent RTS platform and applications.\n\n  \n  \n\n  \n\n\n  \n\n    \n\n    \nCreate Apps\n\n    \nTutorials and code samples to rapidly create DataTorrent applications using Java or dtAssemble.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-datatorrent-rts", 
            "text": "DataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.    \n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }", 
            "title": "Welcome to DataTorrent RTS!"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nThe following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance. \n\n\nApplication Builder\n\n\n\n\n\nDimensional Computing\n\n\n\n\n\nElastic Scalability\n\n\n\n\n\nFault Tolerance", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "The following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can develop an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance.   Application Builder   Dimensional Computing   Elastic Scalability   Fault Tolerance", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demos/", 
            "text": "Running Demo Applications\n\n\nDataTorrent RTS includes a number of demo applications and they are available for import from the \nDevelopment \n App Packages\n section of the DataTorrent management console.\n\n\nImporting Demo Applications\n\n\n\n\nNavigate to \nDevelop \n App Packages \n Import\n section of the DataTorrent console.\n\n\nSelect one of the available packages, such as \nApache Apex Malhar Pi Demo\n and click \nImport\n button.\n\n\nImported packages and included applications will be listed under \nDevelop \n App Packages\n page.\n\n\n\n\nLaunching Demo Applications\n\n\nOnce imported, applications can be launched with a single click.  \nNote\n: Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available Applications, locate PiDemo and click the launch button.\n\n\n\n\n\n\n\n\nProceed with default options on launch confirmation screen by clicking the Launch button.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent console is available in \ndtManage Guide\n\n\nConfiguring Launch Parameters\n\n\nSome applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the \nCreating Applications\n guide.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console.  In this example we will use \nApache Apex Malhar Twitter Demo\n application package.  Import this package from \nDevelop \n App Packages \n Import\n if it is not available.\n\n\n\n\n\n\nFrom the list of Applications, select TwitterDemo and press the corresponding launch button.\n\n\n\n\n\n\nRetrieve Twitter API access information by registering for \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:\n\n\ndt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\nSelect \nSpecify custom properties\n on the launch confirmation screen, click \nadd required properties\n button, and provide Twitter API access values.  Choose to save this configuration as \ntwitter.xml\n file and proceed to Launch the application.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboard\n.\n\n\n\n\n\n\nStopping Applications\n\n\nApplications can be shut down or killed from the Monitor section of \ndtManage\n by selecting application from the list and clicking \nshutdown\n or \nkill\n buttons.", 
            "title": "Running Apps"
        }, 
        {
            "location": "/demos/#running-demo-applications", 
            "text": "DataTorrent RTS includes a number of demo applications and they are available for import from the  Development   App Packages  section of the DataTorrent management console.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/demos/#importing-demo-applications", 
            "text": "Navigate to  Develop   App Packages   Import  section of the DataTorrent console.  Select one of the available packages, such as  Apache Apex Malhar Pi Demo  and click  Import  button.  Imported packages and included applications will be listed under  Develop   App Packages  page.", 
            "title": "Importing Demo Applications"
        }, 
        {
            "location": "/demos/#launching-demo-applications", 
            "text": "Once imported, applications can be launched with a single click.   Note : Ensure Hadoop YARN and HDFS services are active and ready by checking for errors in the DataTorrent console before launching demo applications.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use  PiDemo  application package.    From the list of available Applications, locate PiDemo and click the launch button.     Proceed with default options on launch confirmation screen by clicking the Launch button.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.     More information about using DataTorrent console is available in  dtManage Guide", 
            "title": "Launching Demo Applications"
        }, 
        {
            "location": "/demos/#configuring-launch-parameters", 
            "text": "Some applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the  Creating Applications  guide.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console.  In this example we will use  Apache Apex Malhar Twitter Demo  application package.  Import this package from  Develop   App Packages   Import  if it is not available.    From the list of Applications, select TwitterDemo and press the corresponding launch button.    Retrieve Twitter API access information by registering for  Twitter Developer  account, creating a new  Twitter Application , and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:  dt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret    Select  Specify custom properties  on the launch confirmation screen, click  add required properties  button, and provide Twitter API access values.  Choose to save this configuration as  twitter.xml  file and proceed to Launch the application.     Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboard .", 
            "title": "Configuring Launch Parameters"
        }, 
        {
            "location": "/demos/#stopping-applications", 
            "text": "Applications can be shut down or killed from the Monitor section of  dtManage  by selecting application from the list and clicking  shutdown  or  kill  buttons.", 
            "title": "Stopping Applications"
        }, 
        {
            "location": "/sandbox/", 
            "text": "DataTorrent RTS Sandbox\n\n\nWelcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster.  The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.\n\n\nInstallation\n\n\nIf you have not already, sandbox can be downloaded by visiting \ndatatorrent.com/download\n.  To run the DataTorrent Sandbox, ensure you have downloaded and installed \nVirtualBox\n 4.3 or greater.\n\n\nAccessing Console\n\n\nWhen accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username \ndtadmin\n and password \ndtadmin\n.  Same credentials are also valid for sandbox system access.\n\n\n\n\n\n\n\n\nInside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting \nhttp://localhost:9090/\n\n\nRunning Demo Applications\n\n\nOnce authenticated, you can continue to \nDemo Applications\n section to learn how to import, launch, and run demo applications.\n\n\nService Management \n\n\nThe DataTorrent Sandbox automatically launches Hadoop HDFS and YARN, dtGateway, and other required services during startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching applications.  \n\n\n\n\nNote\n: By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and applications startup.\n\n\n\n\nDataTorrent Sandbox automatically launches following services on startup.\n\n\n\n\nHadoop HDFS NameNode\n\n\nHadoop HDFS DataNode\n\n\nHadoop YARN ResourceManager\n\n\nHadoop YARN NodeManager\n\n\nDataTorrent Gateway\n\n\n\n\nFollowing service management actions are available:\n\n\n\n\nStart all Hadoop and DataTorrent services\n\n\nShut down all Hadoop and DataTorrent services\n\n\nRebuild HDFS (deletes all data!) and restart all services.\n\n\n\n\nTo manage the services following desktop launchers have been set up\n\n\n\n\n\n\nUbuntu Sandbox Edition\n - right-click on \nDataTorrent Services\n desktop launcher\n\n\n\n\n\n\n\n\nLubuntu Sandbox Edition\n - hover the mouse over Lubuntu launcher hidden in the bottom left corner of the sandbox screen, and navigate to DataTorrent menu\n\n\n\n\n\n\n\n\nSupport\n\n\nIf you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on \ndatatorrent.com/contact\n page.", 
            "title": "Sandbox"
        }, 
        {
            "location": "/sandbox/#datatorrent-rts-sandbox", 
            "text": "Welcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster.  The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.", 
            "title": "DataTorrent RTS Sandbox"
        }, 
        {
            "location": "/sandbox/#installation", 
            "text": "If you have not already, sandbox can be downloaded by visiting  datatorrent.com/download .  To run the DataTorrent Sandbox, ensure you have downloaded and installed  VirtualBox  4.3 or greater.", 
            "title": "Installation"
        }, 
        {
            "location": "/sandbox/#accessing-console", 
            "text": "When accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username  dtadmin  and password  dtadmin .  Same credentials are also valid for sandbox system access.     Inside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting  http://localhost:9090/", 
            "title": "Accessing Console"
        }, 
        {
            "location": "/sandbox/#running-demo-applications", 
            "text": "Once authenticated, you can continue to  Demo Applications  section to learn how to import, launch, and run demo applications.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/sandbox/#service-management", 
            "text": "The DataTorrent Sandbox automatically launches Hadoop HDFS and YARN, dtGateway, and other required services during startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching applications.     Note : By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and applications startup.   DataTorrent Sandbox automatically launches following services on startup.   Hadoop HDFS NameNode  Hadoop HDFS DataNode  Hadoop YARN ResourceManager  Hadoop YARN NodeManager  DataTorrent Gateway   Following service management actions are available:   Start all Hadoop and DataTorrent services  Shut down all Hadoop and DataTorrent services  Rebuild HDFS (deletes all data!) and restart all services.   To manage the services following desktop launchers have been set up    Ubuntu Sandbox Edition  - right-click on  DataTorrent Services  desktop launcher     Lubuntu Sandbox Edition  - hover the mouse over Lubuntu launcher hidden in the bottom left corner of the sandbox screen, and navigate to DataTorrent menu", 
            "title": "Service Management "
        }, 
        {
            "location": "/sandbox/#support", 
            "text": "If you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on  datatorrent.com/contact  page.", 
            "title": "Support"
        }, 
        {
            "location": "/create/", 
            "text": "Getting Started\n\n\nGet started quickly by following one of these tutorials, and create your first Apache Apex application today!\n\n\nTop N Words\n\n\nTop N Words\n is a complete guide to writing your first Apache Apex application using \nJava\n or \ndtAssemble\n\n\n\n\nSales Dimensions\n\n\nSales Dimensions\n is an introduction to assembling and visualizing sales analytics applicaiton with \ndtAssemble\n \n\n\n\n\n\n\nAdvanced Topics\n\n\n\n\nApplication Development\n - comprehensive guide to developing Apache Apex applications\n\n\nApplication Packaging\n - creating application packages, changing settings, and launching application packages \n\n\nOperator Development\n - creating new operators for Apache Apex applications\n\n\ndtGateway REST API\n - complete listing of all services offered by dtGateway", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#getting-started", 
            "text": "Get started quickly by following one of these tutorials, and create your first Apache Apex application today!", 
            "title": "Getting Started"
        }, 
        {
            "location": "/create/#top-n-words", 
            "text": "Top N Words  is a complete guide to writing your first Apache Apex application using  Java  or  dtAssemble", 
            "title": "Top N Words"
        }, 
        {
            "location": "/create/#sales-dimensions", 
            "text": "Sales Dimensions  is an introduction to assembling and visualizing sales analytics applicaiton with  dtAssemble", 
            "title": "Sales Dimensions"
        }, 
        {
            "location": "/create/#advanced-topics", 
            "text": "Application Development  - comprehensive guide to developing Apache Apex applications  Application Packaging  - creating application packages, changing settings, and launching application packages   Operator Development  - creating new operators for Apache Apex applications  dtGateway REST API  - complete listing of all services offered by dtGateway", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/tutorials/topnwords/", 
            "text": "Top N Words Application\n\n\nThe Top N words application is a tutorial on building a word counting application using:\n\n\n\n\nApache Apex platform\n\n\nApache Apex Malhar, an associated library of operators\n\n\nOther related tools\n\n\n\n\nNote: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.\n\n\nThe Top N words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words, and computes the word-frequency for that specific file,\nas well as across all files that are processed. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on \ndtDashboard\n, the browser-based dashboard of DataTorrent RTS.\n\n\nA simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:\n\n\n\n\nThe Apex platform\n\n\nThe operator library\n\n\nThe tools required for developing and deploying\n    applications on a cluster\n\n\ndtcli\n \n the command-line tool for managing\n    application packages and the constituent applications\n\n\ndtManage\n \n for monitoring the applications\n\n\ndtDashboard\n \n for visualizing the output\n\n\ndtAssemble\n \n for  visual application assembly\n\n\n\n\nIn the context of such an application, a number of questions arise:\n\n\n\n\nWhat operators do we need ?\n\n\nHow many are present in the Malhar library ?\n\n\nHow many need to be written from scratch ?\n\n\nHow are operators wired together ?\n\n\nHow do we monitor the running application ?\n\n\nHow do we display the output data in an aesthetically pleasing way ?\n\n\n\n\nThe answers to these and other questions are explored in the sections below.\n\n\nFor this tutorial, use the Data Torrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and DataTorrent RTS 3.1.1 configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/topnwords/#top-n-words-application", 
            "text": "The Top N words application is a tutorial on building a word counting application using:   Apache Apex platform  Apache Apex Malhar, an associated library of operators  Other related tools   Note: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.  The Top N words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words, and computes the word-frequency for that specific file,\nas well as across all files that are processed. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on  dtDashboard , the browser-based dashboard of DataTorrent RTS.  A simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:   The Apex platform  The operator library  The tools required for developing and deploying\n    applications on a cluster  dtcli    the command-line tool for managing\n    application packages and the constituent applications  dtManage    for monitoring the applications  dtDashboard    for visualizing the output  dtAssemble    for  visual application assembly   In the context of such an application, a number of questions arise:   What operators do we need ?  How many are present in the Malhar library ?  How many need to be written from scratch ?  How are operators wired together ?  How do we monitor the running application ?  How do we display the output data in an aesthetically pleasing way ?   The answers to these and other questions are explored in the sections below.  For this tutorial, use the Data Torrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and DataTorrent RTS 3.1.1 configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Top N Words Application"
        }, 
        {
            "location": "/tutorials/topnwords-c1/", 
            "text": "Setting up your development environment\n\n\nThis section describes how you can set up your development environment\nincluding starting the sandbox and downloading some sample input files for\ntesting the application.\n\n\nSample input files\n\n\nFor this tutorial, you need some sample text files to use as input application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:\n\n\nOpen a terminal and run the following commands to create a directory named\n\ndata\n under your home directory and download 3 files there:\n\n\ncd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt\n\n\n\nValidation for third-party applications\n\n\nIf you are using your own installation of DataTorrent RTS instead\nof Sandbox, make sure that you have Java JDK (version 1.7.0_79 or\nlater), Maven (version 3.0.5 or later), and Git (version 1.7.1 or later)\nby running the following commands:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nExpected output\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_79\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_79-b15) \n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.0.5 \n\n\nMaven home: /usr/share/maven \n\n\nJava version: 1.7.0_79, vendor: Oracle Corporation \n\n\nJava home: /home/\nuser\n/Software/java/jdk1.7.0_79/jre \n\n\nDefault locale: en_US, platform encoding: UTF-8 \n\n\nOS name: \nlinux\n, version: \n3.16.0-44-generic\n, arch: \namd64\n, family: \nunix\n \n\n\n\n\n\n\ngit --version\n\n\ngit version 1.7.1\n\n\n\n\n\n\n\n\n\n\n\nSet up the sandbox\n\n\nAt the time of writing, the sandbox corresponds to version 3.1.1 of DataTorrent\nRTS, which, as noted above, includes a complete, stand-alone, instance of the\nEnterprise Edition configured as a single-node Hadoop cluster.\n\n\nBefore you begin, ensure that you have Oracle VM VirtualBox version 4.3 or\nlater on your development machine. The sandbox is a virtual appliance bundled\nalong with Ubuntu 12.0.4 (or later) that needs to be imported into VirtualBox.\n\n\nThese steps describe how to download, import, and start the sandbox.\n\n\n\n\n\n\nDownload Sandbox:\n\n\n\n\nOpen \nhttps://www.datatorrent.com/download/\n in a web browser.\n\n\nUnder \nDataTorrent RTS Sandbox\n, click \nDOWNLOAD NOW\n button.\n\n\nOn the contact details form that appears, provide your name, email, and\n   your organization s name, and click \nSubmit\n.\n\n\nClick the link named \nclick here\n (scroll the page down if necessary to\n   see the link).\n\n\nOn the Sandbox downloads page that appears, click \nDownload\n under\n    \nRequirements\n.\n\n\n\n\n\n\n\n\nImport the sandBox into Oracle VirtualBox:\n\n\n\n\nOpen the VirtualBox Manager.\n\n\nIn the \nFile\n menu, choose \nImport Appliance\n.  \n\n\nOn the \nAppliance to import\n dialog box, type or select the full path to\n    the OVA template file that you downloaded and click \nNext\n.\n\n\nClick \nImport\n.\n\n\n\n\n\n\n\n\nAfter the import completes, click the \nStart\n button on the VirtualBox\n    Manager. If this is a first-time start, select the default operating\n    system and wait till the virtual machine initializes.\n\n\nAfter the virtual machine initializes, wait for a few minutes to allow all\nHadoop and Sandbox processes to start.\n\n\n\n\n\n\nLog on to Sandbox.\n\n\n\n\n\n\nIn the browser that appears displaying the Readme, click DataTorrent\n    Console. You can also click the \nDataTorrent Console\n button on the toolbar of your virtual machine.\n\n\n\n\n\n\nType the username and password (dtadmin/dtadmin), and click \nLogin\n.\n\n\n\n\n\n\n\n\n\n\n\n\nYou should see a welcome page with a diagram illustrating the components in the\nplatform stack; each block of the diagram is a clickable link for exploring\nthat component.\n\n\nValidate the Sandbox setup\n\n\nAfter setting up the sandbox, validate the installation:\n\n\n\n\n\n\nOn the top navigation bar, look for links named \nConfigure\n, \nDevelop\n,\n   \nMonitor\n, \nVisualize\n and \nLearn\n.\n\n\n\n\n\n\nIf Visualize is not present, click \nConfigure\n, and then \nLicense\n    Information\n.\n\n\n\n\n\n\nA page showing license details including the text: \nLicense Edition:\n  enterprise\n appears.\n\n\n\n\nIf the license details display   \ncommunity\n instead of \nenterprise\n, wait\n  for a few minutes, refresh the page, and check again.\n\n\nIf that does not work, navigate to the other tabs, and repeat these steps\n  again. The license edition should be \nenterprise\n and you should see the\n  \nVisualize\n link on the top navigation bar.\n\n\n\n\nNote: The enterprise license is present on the Hadoop HDFS file system. To\nretrieve details of this license, the HDFS servers need to be up and running,\nwhich can take a few minutes.", 
            "title": "Development Environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#setting-up-your-development-environment", 
            "text": "This section describes how you can set up your development environment\nincluding starting the sandbox and downloading some sample input files for\ntesting the application.", 
            "title": "Setting up your development environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#sample-input-files", 
            "text": "For this tutorial, you need some sample text files to use as input application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:  Open a terminal and run the following commands to create a directory named data  under your home directory and download 3 files there:  cd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt", 
            "title": "Sample input files"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#validation-for-third-party-applications", 
            "text": "If you are using your own installation of DataTorrent RTS instead\nof Sandbox, make sure that you have Java JDK (version 1.7.0_79 or\nlater), Maven (version 3.0.5 or later), and Git (version 1.7.1 or later)\nby running the following commands:         Command  Expected output    java -version  java version  1.7.0_79  Java(TM) SE Runtime Environment (build 1.7.0_79-b15)   Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)    mvn --version  Apache Maven 3.0.5   Maven home: /usr/share/maven   Java version: 1.7.0_79, vendor: Oracle Corporation   Java home: /home/ user /Software/java/jdk1.7.0_79/jre   Default locale: en_US, platform encoding: UTF-8   OS name:  linux , version:  3.16.0-44-generic , arch:  amd64 , family:  unix      git --version  git version 1.7.1", 
            "title": "Validation for third-party applications"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#set-up-the-sandbox", 
            "text": "At the time of writing, the sandbox corresponds to version 3.1.1 of DataTorrent\nRTS, which, as noted above, includes a complete, stand-alone, instance of the\nEnterprise Edition configured as a single-node Hadoop cluster.  Before you begin, ensure that you have Oracle VM VirtualBox version 4.3 or\nlater on your development machine. The sandbox is a virtual appliance bundled\nalong with Ubuntu 12.0.4 (or later) that needs to be imported into VirtualBox.  These steps describe how to download, import, and start the sandbox.    Download Sandbox:   Open  https://www.datatorrent.com/download/  in a web browser.  Under  DataTorrent RTS Sandbox , click  DOWNLOAD NOW  button.  On the contact details form that appears, provide your name, email, and\n   your organization s name, and click  Submit .  Click the link named  click here  (scroll the page down if necessary to\n   see the link).  On the Sandbox downloads page that appears, click  Download  under\n     Requirements .     Import the sandBox into Oracle VirtualBox:   Open the VirtualBox Manager.  In the  File  menu, choose  Import Appliance .    On the  Appliance to import  dialog box, type or select the full path to\n    the OVA template file that you downloaded and click  Next .  Click  Import .     After the import completes, click the  Start  button on the VirtualBox\n    Manager. If this is a first-time start, select the default operating\n    system and wait till the virtual machine initializes.  After the virtual machine initializes, wait for a few minutes to allow all\nHadoop and Sandbox processes to start.    Log on to Sandbox.    In the browser that appears displaying the Readme, click DataTorrent\n    Console. You can also click the  DataTorrent Console  button on the toolbar of your virtual machine.    Type the username and password (dtadmin/dtadmin), and click  Login .       You should see a welcome page with a diagram illustrating the components in the\nplatform stack; each block of the diagram is a clickable link for exploring\nthat component.", 
            "title": "Set up the sandbox"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#validate-the-sandbox-setup", 
            "text": "After setting up the sandbox, validate the installation:    On the top navigation bar, look for links named  Configure ,  Develop ,\n    Monitor ,  Visualize  and  Learn .    If Visualize is not present, click  Configure , and then  License\n    Information .    A page showing license details including the text:  License Edition:\n  enterprise  appears.   If the license details display    community  instead of  enterprise , wait\n  for a few minutes, refresh the page, and check again.  If that does not work, navigate to the other tabs, and repeat these steps\n  again. The license edition should be  enterprise  and you should see the\n   Visualize  link on the top navigation bar.   Note: The enterprise license is present on the Hadoop HDFS file system. To\nretrieve details of this license, the HDFS servers need to be up and running,\nwhich can take a few minutes.", 
            "title": "Validate the Sandbox setup"
        }, 
        {
            "location": "/tutorials/topnwords-c2/", 
            "text": "Building top N words using JAVA\n\n\nThis chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on the sandbox. We will use the \ndtManage\n GUI tool to launch the\napplication.\n\n\nNote\n: You can build the top N words application using an IDE such as NetBeans\nor maven on the command line. The following sections describe both ways.\n\n\nStep I: Clone the Apex Malhar repository\n\n\nClone the Malhar repository (we will use some of these source files in a later\nsection):\n\n\n\n\n\n\nOpen a terminal window and create a new directory where you want the code\n    to reside, for example: \ncd ~/src; mkdir dt; cd dt\n\n\n\n\n\n\nDownload the code for Malhar:\n\n\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\nYou should now see a directory named \nincubator-apex-malhar\n.\n\n\n\n\n\n\nNavigate to the \nincubator-apex-malhar\n directory and switch to the\n    \ndevel-3\n branch:\n\n\ncd incubator-apex-malhar\ngit checkout devel-3\n\n\n\n\n\n\n\nStep II: Create the application project using NetBeans IDE\n\n\nThis section describes how to generate a new Apache Apex maven project using\nthe NetBeans IDE (downloadable from \nhttps://netbeans.org/downloads/\n). You can\nalso use other IDEs such as Eclipse if you wish. The next section describes an\nalternative method of doing the same thing from the command-line without using\nan IDE.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen NetBeans.\n\n\nClick File \n New Project.\n\n\n\n\nFrom the projects list, select \nProject from Archetype\n, and click \nNext\n.\n    \n\n\n\n\n\n\nOn the Maven Archetype window, type \napex\n in the \nSearch\n box, and\n     from the list of \nKnown Archetypes\n, select \napex-app-archetype\n.\n     \n\n\n\n\nMake sure that the values for the fields match the values shown in this\n     table:\n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \ncom.datatorrent\n\n  \n\n  \n\n  \nArtifact ID\n\n  \napex-app-archetype\n\n  \n\n  \n\n  \nVersion\n\n  \n3.1.1\n\n  \n\n  \n\n  \nRepository\n\n  \nhttps://www.datatorrent.com/maven/content/repositories/releases/\n\n  \n\n  \n\n  \n\n\n\n\nClick Next.\n\n\n\n\nOn the \nName and Location\n window, do the following:\n\n\n\n\nEnter a name for this project in the \nProject Name\n box, for example,\n    \nTopNWordCount\n.\n\n\nEnter a location for this project in the \nProject Location\n box, for\n     example, \n/home/dtadmin/NetBeansProjects\n.\n\n\nEnter an ID in the \nGroup Id\n box, for example, \ncom.example\n.\n\n\nEnter a version for this project in the \nVersion\n box, for example,\n     \n1.0-SNAPSHOT\n.\n\n\nEnter the package name in the \nPackage\n box, for example,\n      \ncom.example.topnwordcount\n.\n\n\n\n\n\n\n\n\n\n\nClick Finish.\n\n\n\n\n\n\nThe project is generated at the specified location and should be visible in\nthe left panel with the name \nMy Apex Application\n. You can right-click the\nproject and choose \nRename\n to provide a more descriptive name such as\n\nTopNWordCount\n.\n\n\n\n\nStep II (Optional): Create the application project using the command line\n\n\nThe new maven project can be created using the command line (instead of an IDE)\nas follows:\n\n\n\n\n\n\nCopy this script to a simple text file named, for example, \nnewdt.sh\n.\n\n\n#!/bin/bash\n# script to create a new project\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n  -DarchetypeGroupId=com.datatorrent \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=3.1.1 \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.topnwordcount \\\n  -DartifactId=topNwordcount \\\n  -Dversion=1.0-SNAPSHOT\n\n\n\n\n\n\n\nRun the file: \nbash newdt.sh\n\n\nNote\n: The command parameters might require minor adjustments as newer\nversions of Apache Apex are released. The parameters are displayed when you run this command.\n\n\n\n\n\n\nPress \nEnter\n when prompted with \nY : :\n. A new project directory named\n    \ntopNwordcount\n containing source files for a simple application should\n    appear.\n\n\n\n\n\n\nStep III: Copy application files to the new project\n\n\nWe assume you now have a new project created via one of the two methods\noutlined above. We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.\n\n\n\n\nDelete files \nApplication.java\n and \nRandomNumberGenerator.java\n\n   under \nsrc/main/java/com/example/topnwordcount\n.\n\n\nDelete file \nApplicationTest.java\n file under\n   \nsrc/test/java/com/example/topnwordcount\n.\n\n\n\n\nCopy the following files from:\n\n\nincubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/\n\n\n\nto\n\n\nsrc/main/java/com/example/topnwordcount\n\n\n\n\n\nApplicationWithQuerySupport.java\n\n\nFileWordCount.java\n\n\nLineReader.java\n\n\nWCPair.java\n\n\nWindowWordCount.java\n\n\nWordCountWriter.java\n\n\nWordReader.java\n\n\n\n\n\n\n\n\nCopy the file \nWordDataSchema.json\n from\n\n\nincubator-apex-malhar/demos/wordcount/src/main/resources/\n\n\n\nto\n\n\nsrc/main/resources/\n\n\n\nin the new project.\n\n\nNote\n: This file defines the format of data sent to the visualization widgets within \ndtDashboard\n.\n\n\n\n\n\n\nStep IV: Customize the application and operators\n\n\nWe will now customize the application so that we can build and run it in our\nsandbox environment.\n\n\nThe first customization involves the package name within the files which\ncurrently reflects the package from which they were copied. The relevant line\nwithin each file might look like this:\n\n\npackage com.datatorrent.demos.wordcount;\n\n\n\n\nChange this line to reflect the current location, for example:\n\n\npackage com.example.topnwordcount;\n\n\n\n\nNote: This change is easily accomplished in NetBeans by opening each file,\nclicking the red error icon in the left margin, and selecting the\n\nChange package declaration to com.example.topnwordcount\n entry from the\ndrop-down list. Do this for each Java file that you copied.\n\n\nCustomize the operators\n\n\nThe next customization involves the query operators: The new code has two\nquery operators that are embedded within other operators. Support for such\nembedding is absent in the 3.1.1 code, so we must:\n\n\n\n\nRemove the embedding calls\n\n\nAdd the operators directly to the DAG\n\n\nConnect these query operators to the rest of the DAG via suitable streams\n\n\n\n\nNote\n: All the operators used in this application are described in detail in the\nAppendix.\n\n\nTo make these changes, edit the file \nApplicationWithQuerySupport.java\n:\n\n\n\n\n\n\nRemove the lines containing calls to \nsetEmbeddableQueryInfoProvider()\n\n    and add these two lines in their place:\n\n\ndag.addOperator(\"QueryFile\",   wsQueryFile);  \ndag.addOperator(\"QueryGlobal\", wsQueryGlobal);\n\n\n\n\n\n\n\nAdd streams to connect the two query operators to the DAG by adding these\n    lines before the four existing \naddstream()\n calls:\n\n\ndag.addStream(\"QueryFileStream\", wsQueryFile.outputPort, snapshotServerFile.query);\ndag.addStream(\"QueryGlobalStream\", wsQueryGlobal.outputPort, snapshotServerGlobal.query);\n\n\n\n\n\n\n\nSave the file.\n\n\n\n\n\n\nNote\n: After you complete this procedure, the newly copied Java files\nshould not show the red error icon in the IDE.\n\n\nCustomize the application configuration\n\n\nLastly, you must add configure some properties of the application. These\nproperties accomplish the following aims:\n\n\n\n\nLimit the amount of memory used by most operators so that more memory can\n  be allocated for \nfileWordCount\n which maintains the frequency counts.\n\n\nSet the locality of a couple of streams to \nCONTAINER_LOCAL\n to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).\n\n\nDefine the regular expression for matching the non-word string that\n  delimits words.\n\n\nDefine number of top (word, frequency) pairs we want output.\n\n\nDefine the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.\n\n\nDefine the topics for sending queries and retrieving data for visualization.\n\n\n\n\nTo do this:\n\n\nOpen the src/main/resources/META-INF/properties.xml file, and replace its\ncontent with the following:\n\n\nconfiguration\n\n \nproperty\n\n   \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n   \nvalue\n500\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.*.operator.*.attr.MEMORY_MB\n/name\n\n   \nvalue\n200\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB\n/name\n\n   \nvalue\n512\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.lineReader.directory\n/name\n\n   \nvalue\n/tmp/test/input-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n/name\n\n   \nvalue\n[\\p{Punct}\\s]+\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wcWriter.filePath\n/name\n\n   \nvalue\n/tmp/test/output-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n/name\n\n   \nvalue\n10\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryFileStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFile\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFileResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobal\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobalResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TwitterDemo.operator.wsResult.numRetries\n/name\n\n   \nvalue\n2147483647\n/value\n\n \n/property\n\n\n/configuration\n\n\n\n\n\nStep V: Build the top N words count application\n\n\nTo build the application from NetBeans\n\n\n\n\nOpen NetBeans IDE.\n\n\nRight-click the project, and click \nBuild\n.\n\n\n\n\nBuilding the application from the command line is equally simple:\n\n\ncd topNwordcount; mvn clean package -DskipTests\n\n\n\nIn either case, if the build is successful, it should have created the\napplication package file\n\ntopNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa\n.\n\n\nStep VI: Upload the top N words application package\n\n\nTo upload the top N words application package\n\n\n\n\nLog on to the DataTorrent Console using the default username and password\n   (both are \ndtadmin\n).\n\n\nOn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, click \nupload a package\n.\n\n\n\n\nNavigate to the location of the \ntopNwordcount-1.0-SNAPSHOT.apa\n\n   application package file is stored.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep VII: Launch the top N words application\n\n\nNote\n: Before launching the top N words application, shut down the IDE. If your\nIDE is running at the time of a launch, the sandbox might hang due to resource\nexhaustion.\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nIn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, locate the top N word count application, and click\n   \nLaunch Application\n.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n. To specify individual properties, select \nSpecify\n    custom properties\n.\n\n\nClick Launch.\n\n\n\n\nA message indicating success of the launch operation should appear along with\nthe application ID.\n\n\nNote\n: After a successful launch, monitor the top N words application following\ninstructions in the chapter \nMonitoring with dtManage\n.", 
            "title": "Building in Java"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#building-top-n-words-using-java", 
            "text": "This chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on the sandbox. We will use the  dtManage  GUI tool to launch the\napplication.  Note : You can build the top N words application using an IDE such as NetBeans\nor maven on the command line. The following sections describe both ways.", 
            "title": "Building top N words using JAVA"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-i-clone-the-apex-malhar-repository", 
            "text": "Clone the Malhar repository (we will use some of these source files in a later\nsection):    Open a terminal window and create a new directory where you want the code\n    to reside, for example:  cd ~/src; mkdir dt; cd dt    Download the code for Malhar:  git clone https://github.com/apache/incubator-apex-malhar  You should now see a directory named  incubator-apex-malhar .    Navigate to the  incubator-apex-malhar  directory and switch to the\n     devel-3  branch:  cd incubator-apex-malhar\ngit checkout devel-3", 
            "title": "Step I: Clone the Apex Malhar repository"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-create-the-application-project-using-netbeans-ide", 
            "text": "This section describes how to generate a new Apache Apex maven project using\nthe NetBeans IDE (downloadable from  https://netbeans.org/downloads/ ). You can\nalso use other IDEs such as Eclipse if you wish. The next section describes an\nalternative method of doing the same thing from the command-line without using\nan IDE.  Generate a new Maven archetype project as follows:   Open NetBeans.  Click File   New Project.   From the projects list, select  Project from Archetype , and click  Next .\n        On the Maven Archetype window, type  apex  in the  Search  box, and\n     from the list of  Known Archetypes , select  apex-app-archetype .\n        Make sure that the values for the fields match the values shown in this\n     table:   \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   com.datatorrent \n   \n   \n   Artifact ID \n   apex-app-archetype \n   \n   \n   Version \n   3.1.1 \n   \n   \n   Repository \n   https://www.datatorrent.com/maven/content/repositories/releases/ \n   \n   \n     Click Next.   On the  Name and Location  window, do the following:   Enter a name for this project in the  Project Name  box, for example,\n     TopNWordCount .  Enter a location for this project in the  Project Location  box, for\n     example,  /home/dtadmin/NetBeansProjects .  Enter an ID in the  Group Id  box, for example,  com.example .  Enter a version for this project in the  Version  box, for example,\n      1.0-SNAPSHOT .  Enter the package name in the  Package  box, for example,\n       com.example.topnwordcount .      Click Finish.    The project is generated at the specified location and should be visible in\nthe left panel with the name  My Apex Application . You can right-click the\nproject and choose  Rename  to provide a more descriptive name such as TopNWordCount .", 
            "title": "Step II: Create the application project using NetBeans IDE"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-optional-create-the-application-project-using-the-command-line", 
            "text": "The new maven project can be created using the command line (instead of an IDE)\nas follows:    Copy this script to a simple text file named, for example,  newdt.sh .  #!/bin/bash\n# script to create a new project\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n  -DarchetypeGroupId=com.datatorrent \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=3.1.1 \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.topnwordcount \\\n  -DartifactId=topNwordcount \\\n  -Dversion=1.0-SNAPSHOT    Run the file:  bash newdt.sh  Note : The command parameters might require minor adjustments as newer\nversions of Apache Apex are released. The parameters are displayed when you run this command.    Press  Enter  when prompted with  Y : : . A new project directory named\n     topNwordcount  containing source files for a simple application should\n    appear.", 
            "title": "Step II (Optional): Create the application project using the command line"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iii-copy-application-files-to-the-new-project", 
            "text": "We assume you now have a new project created via one of the two methods\noutlined above. We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.   Delete files  Application.java  and  RandomNumberGenerator.java \n   under  src/main/java/com/example/topnwordcount .  Delete file  ApplicationTest.java  file under\n    src/test/java/com/example/topnwordcount .   Copy the following files from:  incubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/  to  src/main/java/com/example/topnwordcount   ApplicationWithQuerySupport.java  FileWordCount.java  LineReader.java  WCPair.java  WindowWordCount.java  WordCountWriter.java  WordReader.java     Copy the file  WordDataSchema.json  from  incubator-apex-malhar/demos/wordcount/src/main/resources/  to  src/main/resources/  in the new project.  Note : This file defines the format of data sent to the visualization widgets within  dtDashboard .", 
            "title": "Step III: Copy application files to the new project"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iv-customize-the-application-and-operators", 
            "text": "We will now customize the application so that we can build and run it in our\nsandbox environment.  The first customization involves the package name within the files which\ncurrently reflects the package from which they were copied. The relevant line\nwithin each file might look like this:  package com.datatorrent.demos.wordcount;  Change this line to reflect the current location, for example:  package com.example.topnwordcount;  Note: This change is easily accomplished in NetBeans by opening each file,\nclicking the red error icon in the left margin, and selecting the Change package declaration to com.example.topnwordcount  entry from the\ndrop-down list. Do this for each Java file that you copied.  Customize the operators  The next customization involves the query operators: The new code has two\nquery operators that are embedded within other operators. Support for such\nembedding is absent in the 3.1.1 code, so we must:   Remove the embedding calls  Add the operators directly to the DAG  Connect these query operators to the rest of the DAG via suitable streams   Note : All the operators used in this application are described in detail in the\nAppendix.  To make these changes, edit the file  ApplicationWithQuerySupport.java :    Remove the lines containing calls to  setEmbeddableQueryInfoProvider() \n    and add these two lines in their place:  dag.addOperator(\"QueryFile\",   wsQueryFile);  \ndag.addOperator(\"QueryGlobal\", wsQueryGlobal);    Add streams to connect the two query operators to the DAG by adding these\n    lines before the four existing  addstream()  calls:  dag.addStream(\"QueryFileStream\", wsQueryFile.outputPort, snapshotServerFile.query);\ndag.addStream(\"QueryGlobalStream\", wsQueryGlobal.outputPort, snapshotServerGlobal.query);    Save the file.    Note : After you complete this procedure, the newly copied Java files\nshould not show the red error icon in the IDE.  Customize the application configuration  Lastly, you must add configure some properties of the application. These\nproperties accomplish the following aims:   Limit the amount of memory used by most operators so that more memory can\n  be allocated for  fileWordCount  which maintains the frequency counts.  Set the locality of a couple of streams to  CONTAINER_LOCAL  to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).  Define the regular expression for matching the non-word string that\n  delimits words.  Define number of top (word, frequency) pairs we want output.  Define the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.  Define the topics for sending queries and retrieving data for visualization.   To do this:  Open the src/main/resources/META-INF/properties.xml file, and replace its\ncontent with the following:  configuration \n  property \n    name dt.attr.MASTER_MEMORY_MB /name \n    value 500 /value \n  /property   property \n    name dt.application.*.operator.*.attr.MEMORY_MB /name \n    value 200 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB /name \n    value 512 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.lineReader.directory /name \n    value /tmp/test/input-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr /name \n    value [\\p{Punct}\\s]+ /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wcWriter.filePath /name \n    value /tmp/test/output-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.topN /name \n    value 10 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryFileStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryFile.topic /name \n    value TopNWordsQueryFile /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultFile.topic /name \n    value TopNWordsQueryFileResult /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryGlobal.topic /name \n    value TopNWordsQueryGlobal /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic /name \n    value TopNWordsQueryGlobalResult /value \n  /property   property \n    name dt.application.TwitterDemo.operator.wsResult.numRetries /name \n    value 2147483647 /value \n  /property  /configuration", 
            "title": "Step IV: Customize the application and operators"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-v-build-the-top-n-words-count-application", 
            "text": "To build the application from NetBeans   Open NetBeans IDE.  Right-click the project, and click  Build .   Building the application from the command line is equally simple:  cd topNwordcount; mvn clean package -DskipTests  In either case, if the build is successful, it should have created the\napplication package file topNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa .", 
            "title": "Step V: Build the top N words count application"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vi-upload-the-top-n-words-application-package", 
            "text": "To upload the top N words application package   Log on to the DataTorrent Console using the default username and password\n   (both are  dtadmin ).  On the top navigation bar, click  Develop .  Under  App Packages , click  upload a package .   Navigate to the location of the  topNwordcount-1.0-SNAPSHOT.apa \n   application package file is stored.  Wait till the package is successfully uploaded.", 
            "title": "Step VI: Upload the top N words application package"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vii-launch-the-top-n-words-application", 
            "text": "Note : Before launching the top N words application, shut down the IDE. If your\nIDE is running at the time of a launch, the sandbox might hang due to resource\nexhaustion.   Log on to the DataTorrent Console (the default username and password are\n   both  dtadmin ).  In the top navigation bar, click  Develop .  Under  App Packages , locate the top N word count application, and click\n    Launch Application .  (Optional) To configure the application using a configuration file, select\n     Use a config file . To specify individual properties, select  Specify\n    custom properties .  Click Launch.   A message indicating success of the launch operation should appear along with\nthe application ID.  Note : After a successful launch, monitor the top N words application following\ninstructions in the chapter  Monitoring with dtManage .", 
            "title": "Step VII: Launch the top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c3/", 
            "text": "Building top N words using dtAssemble\n\n\nYou can build top N words using dtAssemble \n the graphical drag-and-drop\napplication builder.\n\n\nNote\n: This tool is not available with the Community edition license.\n\n\nUsing the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.\n\n\nNote\n: You cannot undo the auto-layout or zoom-to-fit operations.\n\n\nPrerequisites\n\n\nTo use \ndtAssemble\n, you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.\n\n\nTo ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the \nMonitor\n tab, and under \nDataTorrent Applications\n, kill any\nrunning applications.\n\n\nStep I: Create top N words using dtAssemble\n\n\n\n\nLog on to the DataTorrent RTS console (default username and password are\n   both \ndtadmin\n).\n\n\nOn the DataTorrent RTS console, click \nDevelop\n \n \nApp Packages\n.\n\n\nMake sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.\n\n\nClick \nTopNWordCount\n in the name column to see the application details.\n  \n\n\nClick \ncreate new application\n button.\n  \n\n\nType a name for your application, for example, \nTop N words\n, and click\n   \nCreate\n. The \nApplication Canvas\n should open.\n\n\n\n\nThe existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For each JSON application, there are\nthree additional buttons that allow editing, deleting, and cloning operations.\nThese operations are not supported for JAVA applications.\n\n\n\n\nStep II: Drag operators to the application canvas\n\n\n\n\nWait till the Application Canvas opens.\n  \n\n\nFrom the Operator Library list on the left, locate the desired\n   operators by either:\n\n\n\n\nExploring the categories.\n\n\n-or-\n\n\n\n\n\n\nUsing the \nquick find\n feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type \nfile\n  into the search box to see a list of\n   matching operators. For example, the first operator is \nLineReader\n.\n    \n\n\n\n\nDrag the operator onto the canvas.\n\n\nRepeat this process for all the operators described in Appendix entitled\n    \nOperators in Top N words application\n, and arrange them on the canvas.\n\n\nTo magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.\n\n\n\n\nStep III: Connect the operators\n\n\nObserve that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.\n\n\nConnect the operators as shown in the diagram below. Note the following points about\nthe \nFileWordCount\n operator which has the largest number of connections:\n\n\n\n\nThe control port is connected to the control port of \nLineReader\n.\n\n\nThe input port is connected to the output port of \nWindowWordCount\n.\n\n\nThe \nfileOutput\n port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of \nWordCountWriter\n.\n\n\nThe \noutputGlobal\n port emits the global top N pairs and so is connected to\n  the input port of \nAppDataSnapshotMapServer\n.\n\n\nThe \noutputPerFile\n port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to \nConsoleOutput\n as\n  well as to \nAppDataSnapshotMapServer\n.\n\n\n\n\nNote\n: As you make changes, the top left corner displays \nAll\nchanges saved to HDFS\n. No explicit save step is needed.\n\n\nAfter you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.\n\n\n\n\nStep IV: Configure the operator properties\n\n\nThe last step before running this application is to configure\nproperties of the operators.\n\n\n\n\nClick the first operator (\nLineReader\n) in the canvas to see the list of\n   configurable properties in the right panel.\n\n\n\n\nLocate the property labelled \nDirectory\n and enter the path to the input\n   directory: \n/tmp/test/input-dir\n:\n\n\n\n\n\n\n\n\nConfigure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nProperty Name\n\n\nValue\n\n\n\n\n\n\n5\n\n\nFile Path\n\n\n/tmp/test/output-dir\n\n\n\n\n\n\n2\n\n\nNon Word Str\n\n\n[\\p{Punct}\\s]+\n\n\n\n\n\n\n9\n\n\nTopic\n\n\nTopNWordsQueryFile\n\n\n\n\n\n\n10\n\n\nTopic\n\n\nTopNWordsQueryGlobal\n\n\n\n\n\n\n11\n\n\nTopic\n\n\nTopNWordsQueryFileResult\n\n\n\n\n\n\n12\n\n\nTopic\n\n\nTopNWordsQueryGlobalResult\n\n\n\n\n\n\n7, 8\n\n\nSnapshot Schema JSON\n\n\n{ \nvalues\n: [{\nname\n: \nword\n, \ntype\n: \nstring\n},\n\n\n{\nname\n: \ncount\n, \ntype\n: \ninteger\n}] }\n\n\n\n\n\n\n4\n\n\nTop N\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\nClick Stream 8 and Stream 9, and change \nStream Locality\n from\n  \nAUTOMATIC\n to \nCONTAINER_LOCAL\n to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.\n\n\n\n\n\n\nClick the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to \nMaster Memory Mb\n, and\n   change its value to 500.\n\n\n\n\n\n\nClick each operator, navigate to the \nMemory Mb\n attribute in the\n   \nAttributes\n section, and change the value to 200 except for \nOperator\n    4\n for which the value is 512.\n\n\n\n\n\n\nClick \nlaunch\n in the top-left corner. \nNote\n: Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n  \n\n\n\n\n\n\nOn the launch application dialog window, type a name for your application.\n  \n\n\n\n\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n checkbox. To specify individual properties, select\n    \nSpecify custom properties\n checkbox.\n\n\n\n\nClick \nLaunch\n.\n\n\n\n\nA transient pop-up at the top-right indicating that the launch was successful\nshould appear.\n\n\n\nAfter a successful launch, monitor the application following\ninstructions in the Chapter entitled \nMonitoring with dtManage\n.", 
            "title": "Building with dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#building-top-n-words-using-dtassemble", 
            "text": "You can build top N words using dtAssemble   the graphical drag-and-drop\napplication builder.  Note : This tool is not available with the Community edition license.  Using the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.  Note : You cannot undo the auto-layout or zoom-to-fit operations.  Prerequisites  To use  dtAssemble , you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.  To ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the  Monitor  tab, and under  DataTorrent Applications , kill any\nrunning applications.", 
            "title": "Building top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-i-create-top-n-words-using-dtassemble", 
            "text": "Log on to the DataTorrent RTS console (default username and password are\n   both  dtadmin ).  On the DataTorrent RTS console, click  Develop     App Packages .  Make sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.  Click  TopNWordCount  in the name column to see the application details.\n    Click  create new application  button.\n    Type a name for your application, for example,  Top N words , and click\n    Create . The  Application Canvas  should open.   The existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For each JSON application, there are\nthree additional buttons that allow editing, deleting, and cloning operations.\nThese operations are not supported for JAVA applications.", 
            "title": "Step I: Create top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-ii-drag-operators-to-the-application-canvas", 
            "text": "Wait till the Application Canvas opens.\n    From the Operator Library list on the left, locate the desired\n   operators by either:   Exploring the categories.  -or-    Using the  quick find  feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type  file   into the search box to see a list of\n   matching operators. For example, the first operator is  LineReader .\n       Drag the operator onto the canvas.  Repeat this process for all the operators described in Appendix entitled\n     Operators in Top N words application , and arrange them on the canvas.  To magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.", 
            "title": "Step II: Drag operators to the application canvas"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iii-connect-the-operators", 
            "text": "Observe that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.  Connect the operators as shown in the diagram below. Note the following points about\nthe  FileWordCount  operator which has the largest number of connections:   The control port is connected to the control port of  LineReader .  The input port is connected to the output port of  WindowWordCount .  The  fileOutput  port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of  WordCountWriter .  The  outputGlobal  port emits the global top N pairs and so is connected to\n  the input port of  AppDataSnapshotMapServer .  The  outputPerFile  port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to  ConsoleOutput  as\n  well as to  AppDataSnapshotMapServer .   Note : As you make changes, the top left corner displays  All\nchanges saved to HDFS . No explicit save step is needed.  After you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.", 
            "title": "Step III: Connect the operators"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iv-configure-the-operator-properties", 
            "text": "The last step before running this application is to configure\nproperties of the operators.   Click the first operator ( LineReader ) in the canvas to see the list of\n   configurable properties in the right panel.   Locate the property labelled  Directory  and enter the path to the input\n   directory:  /tmp/test/input-dir :     Configure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.          Operator  Property Name  Value    5  File Path  /tmp/test/output-dir    2  Non Word Str  [\\p{Punct}\\s]+    9  Topic  TopNWordsQueryFile    10  Topic  TopNWordsQueryGlobal    11  Topic  TopNWordsQueryFileResult    12  Topic  TopNWordsQueryGlobalResult    7, 8  Snapshot Schema JSON  {  values : [{ name :  word ,  type :  string },  { name :  count ,  type :  integer }] }    4  Top N  10       Click Stream 8 and Stream 9, and change  Stream Locality  from\n   AUTOMATIC  to  CONTAINER_LOCAL  to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.    Click the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to  Master Memory Mb , and\n   change its value to 500.    Click each operator, navigate to the  Memory Mb  attribute in the\n    Attributes  section, and change the value to 200 except for  Operator\n    4  for which the value is 512.    Click  launch  in the top-left corner.  Note : Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n      On the launch application dialog window, type a name for your application.\n      (Optional) To configure the application using a configuration file, select\n     Use a config file  checkbox. To specify individual properties, select\n     Specify custom properties  checkbox.   Click  Launch .   A transient pop-up at the top-right indicating that the launch was successful\nshould appear.  After a successful launch, monitor the application following\ninstructions in the Chapter entitled  Monitoring with dtManage .", 
            "title": "Step IV: Configure the operator properties"
        }, 
        {
            "location": "/tutorials/topnwords-c4/", 
            "text": "Monitoring top N words using dtManage\n\n\ndtManage\n is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.\n\n\nMonitor the Application\n\n\nTo monitor the top N words application\n\n\n\n\nLog on to the Datatorrent Console (the default username and password\n   are both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nUnder \nDatatorrent Applications\n, check if the application started.\n\n\nWait till the state entry changes to \nRUNNING\n.\n\n\nClick \nTopNWordsWithQueries\n to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and \nmetric-view\n.\n\n\nUnder \nStramEvents\n, ensure that all the operators have started.\n    \n\n\n\n\nDAGs and widgets\n\n\nWhen monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets: \nStram Events\n, \nApplication\nOverview\n, \nLogical DAG\n, \nLogical Operators\n, \nStreams\n, and \nMetrics Chart\n.\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).\n\n\nLogical view and associated widgets (panels)\n\n\nThis section describes the widgets that you see when you select the logical\ntab.\n\n\nStram Events\n\n\nAs shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled \ndetails\n\nappears next to the event; click on it for additional details about the\nfailure.\n\n\nApplication Overview\n\n\nThis panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The \nvisualize\n button allows you to\ncreate one or more custom dashboards to visualize the application output.\n\n\n\nLogical DAG\n\n\nThe logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.\n\n\nTo customize these properties\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator, click the Top list and\n   select a metric.\n\n\nTo display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.\n\n\n\n\n\n\nLogical Operators\n\n\nThis panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.\n\n\n\n\nStreams\n\n\nThis panel displays details of each stream such as the name, locality, source,\nand sinks.\n\n\n\n\nMetrics Chart\n\n\nThis panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:\n\n\n\n\nPhysical view and associated widgets\n\n\nThe physical tab displays the \nApplication Overview\n and \nMetrics Chart\n\ndiscussed above along with additional panels: \nPhysical Operators\n and\n\nContainers\n. The \nPhysical Operators\n table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.\n\n\nPhysical Operators\n\n\n\n\nContainers\n\n\nFor each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the \nContainers\n table.\n\n\n\n\nIf the state of all the physical operators and containers is \nACTIVE\n\nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.\n\n\nThe physical-dag-view\n\n\nThe \nphysical-dag-view\n tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.\n\n\nThe metric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nView application logs\n\n\nWhen debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines. \ndtManage\n simplifies\nthis task by making all relevant logs accessible from the console.\n\n\nFor example, to examine logs for the \nFileWordCount\n operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.\n\n\nThe numeric values in the \ncontainer\n column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the \nContainer Overview\n panel, you should see a blue \nlogs\n dropdown\nbutton; click on it to see a menu containing three entries: \ndt.log\n, \nstderr\n,\nand \nstdout\n.\n\n\n\n\nAll messages output using \nlog4j\n classes will appear in \ndt.log\n\nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "Monitoring with dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitoring-top-n-words-using-dtmanage", 
            "text": "dtManage  is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.", 
            "title": "Monitoring top N words using dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitor-the-application", 
            "text": "To monitor the top N words application   Log on to the Datatorrent Console (the default username and password\n   are both  dtadmin ).  On the top navigation bar, click  Monitor .  Under  Datatorrent Applications , check if the application started.  Wait till the state entry changes to  RUNNING .  Click  TopNWordsWithQueries  to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and  metric-view .  Under  StramEvents , ensure that all the operators have started.", 
            "title": "Monitor the Application"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#dags-and-widgets", 
            "text": "When monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets:  Stram Events ,  Application\nOverview ,  Logical DAG ,  Logical Operators ,  Streams , and  Metrics Chart .\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).  Logical view and associated widgets (panels)  This section describes the widgets that you see when you select the logical\ntab.  Stram Events  As shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled  details \nappears next to the event; click on it for additional details about the\nfailure.  Application Overview  This panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The  visualize  button allows you to\ncreate one or more custom dashboards to visualize the application output.  Logical DAG  The logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.  To customize these properties   Click an operator for which you want to display additional details.  To display a detail on the top of this operator, click the Top list and\n   select a metric.  To display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.    Logical Operators  This panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.   Streams  This panel displays details of each stream such as the name, locality, source,\nand sinks.   Metrics Chart  This panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:   Physical view and associated widgets  The physical tab displays the  Application Overview  and  Metrics Chart \ndiscussed above along with additional panels:  Physical Operators  and Containers . The  Physical Operators  table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.  Physical Operators   Containers  For each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the  Containers  table.   If the state of all the physical operators and containers is  ACTIVE \nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.  The physical-dag-view  The  physical-dag-view  tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.  The metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "DAGs and widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#view-application-logs", 
            "text": "When debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines.  dtManage  simplifies\nthis task by making all relevant logs accessible from the console.  For example, to examine logs for the  FileWordCount  operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.  The numeric values in the  container  column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the  Container Overview  panel, you should see a blue  logs  dropdown\nbutton; click on it to see a menu containing three entries:  dt.log ,  stderr ,\nand  stdout .   All messages output using  log4j  classes will appear in  dt.log \nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "View application logs"
        }, 
        {
            "location": "/tutorials/topnwords-c5/", 
            "text": "Visualizing the application output using dtDashboard\n\n\nThis chapter covers how to add input files to the monitored input directory and\nvisualize the output.\n\n\nWhen adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled \nFurther Explorations\n.\n\n\nStep 1: Add files to the monitored directory\n\n\nTo add the files to the monitored input directory\n\n\n\n\nLog on to the Datatorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nClick TopNWordsWithQueries to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and  \nmetric-view\n.\n\n\nClick the \nlogical\n tab and make sure that the DAG is visible.\n\n\nCreate the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands:\nhdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir\n\n\n\n\n\n\n\nYou should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.\n\n\nYou can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:\n\n\nhdfs dfs -cat /tmp/test/output-dir/rfc4844.txt\n\n\n\nFor operating on these input and output directories, you may find the following\nshell aliases and functions useful:\n\n\nin=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}\n\n\n\nPut them in a file called, say, \naliases\n and read them into your shell with:\n\nsource aliases\n.\n\n\nThereafter, you can list contents of the input and output directories with\n\nls-input\n and \nls-output\n, remove all files from them with \nclean-input\n and\n\nclean-output\n, drop an input file \nfoo.txt\n into the input directory with\n\nput-file foo.txt\n and finally, retrieve the corresponding output file with\n\nget-file foo.txt\n.\n\n\nNote\n: When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.\n\n\nStep II: Visualize the results by generating dashboards\n\n\nTo generate dashboards\n\n\n\n\nPerform step I above.\n\n\nMake sure that the logical tab is selected and the \nApplication Overview\n\n  panel is visible.\n\n\n\n\nClick \nvisualize\n to see a dropdown containing previously created dashboards\n (if any), as well as the \ngenerate new dashboard\n entry.\n\n\n\n\n\n\nSelect the \ngenerate new dashboard\n entry.\n\n\nYou should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.\n\n\n\n\n\n\n\nAdd more files, one at a time, to the input directory as described in\n  step I above.\n\n\n\n\nObserve the charts changing to reflect the new data.\n\n\n\n\nYou can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.\n\n\nStep III: Add widgets\n\n\nTo derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets: \nbar chart\n,\n\npie chart\n, \nhorizontal bar chart\n, \ntable\n, and \nnote\n.\n\n\nTo add a widget\n\n\n\n\nGenerate a dashboard by following instructions of Step II above.\n\n\nClick the \nadd widget\n button below the name of the dashboard.\n\n\nIn the \nData Source\n list, select a data source for your widget.\n\n\n\n\nSelect a widget type under \nAvailable Widgets\n.\n\n\n\n\n\n\n\n\nClick \nadd widget\n.\n\n\n\n\n\n\nThe widget is added to your dashboard.\n\n\nStep IV: Configure a widget\n\n\nAfter you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo configure a widget\n\n\n\n\nTo change the size of the widget, click the border of the widget, and\n  resize it.\n\n\nTo move the widget around, click the widget, and drag it to the desired\n  position.\n\n\nTo change the title and other properties, click the \nedit\n button in the\n  top-right corner of the widget.\n    \n\n  You can now enter a new title in the \nTitle\n box or configure the rest of the\n  options in any suitable way.\n\n\nClick \nOK\n.\n\n\nTo remove a widget, click the delete button in the top-right corner.\n\n\n\n\nPerform additional tasks on dashboards\n\n\nAt any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.\n\n\nTo perform additional tasks\n\n\n\n\nEnsure that you generated a dashboard as described in Step II above and\n   select it.\n\n\nClick \nsettings\n button (next to buttons named \nadd widget\n,\n   \nauto generate\n, and \nsave settings\n), below the name of the dashboard to see the \nDashboard Settings\n dialog:\n    \n\n\nType a new name for the dashboard in the \nName of dashboard\n box.\n\n\nType a suitable description in the box below.\n\n\nMake sure that \nTopNWordsWithQueries\n is selected under \nChoose apps to\n    visualize\n.\n\n\nClick \nSave\n.\n\n\n\n\nDelete a dashboard\n\n\nYou can delete a dashboard at any time.\n\n\n\n\nLog on to the DataTorrent Console (default username and password are both\n  \ndtadmin\n)\n\n\nOn the top navigation bar, click \nVisualize\n.\n\n\n\n\nSelect a dashboard.\n\n\n\n\n\n\n\n\nClick delete.\n\n\n\n\n\n\n\n\nNote: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Visualizing with dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#visualizing-the-application-output-using-dtdashboard", 
            "text": "This chapter covers how to add input files to the monitored input directory and\nvisualize the output.  When adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled  Further Explorations .", 
            "title": "Visualizing the application output using dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-1-add-files-to-the-monitored-directory", 
            "text": "To add the files to the monitored input directory   Log on to the Datatorrent Console (the default username and password are\n   both  dtadmin ).  On the top navigation bar, click  Monitor .  Click TopNWordsWithQueries to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and   metric-view .  Click the  logical  tab and make sure that the DAG is visible.  Create the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands: hdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir    You should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.  You can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:  hdfs dfs -cat /tmp/test/output-dir/rfc4844.txt  For operating on these input and output directories, you may find the following\nshell aliases and functions useful:  in=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}  Put them in a file called, say,  aliases  and read them into your shell with: source aliases .  Thereafter, you can list contents of the input and output directories with ls-input  and  ls-output , remove all files from them with  clean-input  and clean-output , drop an input file  foo.txt  into the input directory with put-file foo.txt  and finally, retrieve the corresponding output file with get-file foo.txt .  Note : When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.", 
            "title": "Step 1: Add files to the monitored directory"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-ii-visualize-the-results-by-generating-dashboards", 
            "text": "To generate dashboards   Perform step I above.  Make sure that the logical tab is selected and the  Application Overview \n  panel is visible.   Click  visualize  to see a dropdown containing previously created dashboards\n (if any), as well as the  generate new dashboard  entry.    Select the  generate new dashboard  entry.  You should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.    Add more files, one at a time, to the input directory as described in\n  step I above.   Observe the charts changing to reflect the new data.   You can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.", 
            "title": "Step II: Visualize the results by generating dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iii-add-widgets", 
            "text": "To derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets:  bar chart , pie chart ,  horizontal bar chart ,  table , and  note .  To add a widget   Generate a dashboard by following instructions of Step II above.  Click the  add widget  button below the name of the dashboard.  In the  Data Source  list, select a data source for your widget.   Select a widget type under  Available Widgets .     Click  add widget .    The widget is added to your dashboard.", 
            "title": "Step III: Add widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iv-configure-a-widget", 
            "text": "After you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To configure a widget   To change the size of the widget, click the border of the widget, and\n  resize it.  To move the widget around, click the widget, and drag it to the desired\n  position.  To change the title and other properties, click the  edit  button in the\n  top-right corner of the widget.\n     \n  You can now enter a new title in the  Title  box or configure the rest of the\n  options in any suitable way.  Click  OK .  To remove a widget, click the delete button in the top-right corner.", 
            "title": "Step IV: Configure a widget"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#perform-additional-tasks-on-dashboards", 
            "text": "At any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.  To perform additional tasks   Ensure that you generated a dashboard as described in Step II above and\n   select it.  Click  settings  button (next to buttons named  add widget ,\n    auto generate , and  save settings ), below the name of the dashboard to see the  Dashboard Settings  dialog:\n      Type a new name for the dashboard in the  Name of dashboard  box.  Type a suitable description in the box below.  Make sure that  TopNWordsWithQueries  is selected under  Choose apps to\n    visualize .  Click  Save .", 
            "title": "Perform additional tasks on dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#delete-a-dashboard", 
            "text": "You can delete a dashboard at any time.   Log on to the DataTorrent Console (default username and password are both\n   dtadmin )  On the top navigation bar, click  Visualize .   Select a dashboard.     Click delete.     Note: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Delete a dashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c6/", 
            "text": "Appendix\n\n\nOperators in Top N words application\n\n\nThis section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nImplementing class\n\n\nDescription\n\n\n\n\n\n\nlineReader\n\n\nLineReader\n\n\nReads lines from input files.\n\n\n\n\n\n\nwordReader\n\n\nWordReader\n\n\nSplits a line into words.\n\n\n\n\n\n\nwindowWordCount\n\n\nWindowWordCount\n\n\nComputes word frequencies for a single window.\n\n\n\n\n\n\nfileWordCount\n\n\nFileWordCount\n\n\nMaintains per-file and global word frequencies.\n\n\n\n\n\n\nwcWriter\n\n\nWcWriter\n\n\nWrites top N words and their frequencies to output files.\n\n\n\n\n\n\nconsole\n\n\nConsoleOutputOperator\n\n\nWrites received tuples to console.\n\n\n\n\n\n\nsnapshotServerFile\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last data set for the current file, and returns it in response to queries.\n\n\n\n\n\n\nsnapshotServerGlobal\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last global data set, and returns it in response to queries.\n\n\n\n\n\n\nQueryFile\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for per-file data.\n\n\n\n\n\n\nQueryGlobal\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for global data.\n\n\n\n\n\n\nwsResultFile\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for per-file queries.\n\n\n\n\n\n\nwsResultGlobal\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for global queries.\n\n\n\n\n\n\n\n\n\nWe now describe the process of wiring these operators together in the\n\npopulateDAG()\n method of the main application class\n\nApplicationWithQuerySupport\n. First, the operators are created and added to\nthe DAG via the \naddOperator\n method:\n\n\nLineReader lineReader = dag.addOperator(\nlineReader\n,new LineReader());\n\n\n\n\nThe first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.\n\n\nNext, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the \naddStream\n function, for example:\n\n\ndag.addStream(\nlines\n, lineReader.output, wordReader.input);\n...\ndag.addStream(\nWordCountsFile\n, fileWordCount.outputPerFile, snapshotServerFile.input, console.input);\n\n\n\n\nNotice that the stream from \nfileWordCount.outputPerFile\n (which consists of\nthe top N words for the current file as the file is being read) goes to\n\nsnapshotServerFile.input\n (where it will be saved to respond to queries) and to\n\nconsole.input\n (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.\n\n\nThis section provides detailed information about each operator.\n\n\nLineReader\n\n\nThis class extends \nAbstractFileInputOperator\nString\n to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.\n\n\nThe base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods: \nopenFile\n, \ncloseFile\n, \nreadEntity\n, and \nemit\n. Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the \nFileWordCount\n operator.\n\n\nWordReader\n\n\nThis operator receives lines from \nLineReader\n on the input port and emits\nwords on the output port. It has a configurable property called \nnonWordStr\n\nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property\n\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n.\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe \nprocess\n method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.\n\n\nWindowWordCount\n\n\nThis operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when\n\nendWindow\n is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the\n\nendWindow\n call, but rather emit output tuples as each input tuple is\nprocessed.\n\n\nFileWordCount\n\n\nThis operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen \nLineReader\n reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next \nendWindow\n. The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we \ndo\n\nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing\n\nbeginWindow\n and \nendWindow\n calls by the upstream operator.\n\n\nThis operator also has three output ports: the \noutputPerFile\n port for the top\nN pairs for the current file as it is being read; the \noutputGlobal\n port for\nthe global top N pairs, and the \nfileOutput\n port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.\n\n\nFileWordCount\n also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.\n\n\nFileWordCount\n has a configurable property \ntopN\n for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name: \ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n\n\nIn the \nendWindow\n call, both maps are passed to the \ngetTopNList\n function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.\n\n\nWordCountWriter\n\n\nThis operator extends \nAbstractFileOutputOperator\nMap\nString,Object\n, and\nsimply writes the final top N pairs to the output file. As with \nLineReader\n,\nmost of the complexity of \nWordCountWriter\n is hidden in the base class. You must\nprovide implementations for 3 methods: \nendWindow\n, \ngetFileName\n, and\n\ngetBytesForTuple\n. The first method calls the base class method \nrequestFinalize\n.\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The \ngetFileName\n\nmethod retrieves the file name from the tuple, and the \ngetBytesForTuple\n\nmethod converts the list of pairs to a string in the desired format.\n\n\nConsoleOutputOperator\n\n\nThis is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.\n\n\nAppDataSnapshotServerMap\n\n\nThis operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots \n  one for a per-file top N snapshot and one for a\nglobal snapshot.\n\n\nPubSubWebSocketAppDataQuery\n\n\nThis is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:\n\n\nws://gateway-host:port/pubsub\n\n\n\n\nwhere \ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message to the URL where the value of the \ndata\n key\nis the desired message content. The JSON might look like this:\n\n\n{\ntype\n:\npublish\n, \ntopic\n:\nfoobar\n, \ndata\n: ...}\n\n\n\n\nCorrespondingly, subscribers send messages like this to retrieve published\nmessage data:\n\n\n{\ntype\n:\nsubscribe\n, \ntopic\n:\nfoobar\n}\n\n\n\n\nTopic names need not be pre-registered anywhere but the same topic\nname (for example, \nfoobar\n in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.\n\n\nFor this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values \nTopNWordsQueryFile\n and \nTopNWordsQueryGlobal\n under the\nrespective names:\n\n\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n\n\n\n\nPubSubWebSocketAppDataResult\n\n\nAnalogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values\n\nTopNWordsQueryFileResult\n and \nTopNWordsQueryGlobalResult\n corresponding to\nthe respective names:\n\n\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n\n\n\n\nFurther Exploration\n\n\nIn this tutorial, the property values in the \nproperties.xml\n file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.\n\n\nAnother aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the \nFileWordCount\n operator gets an EOF on the\ncontrol port, it waits for an \nendWindow\n call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.\n\n\nDataTorrent terminology\n\n\nOperators\n\n\nOperators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.\n\n\nStreams\n\n\nA stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.\n\n\nPorts\n\n\nPorts are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.\n\n\nDirected Acyclic Graph (DAG)\n\n\nA DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.\n\n\nLogical Plan or DAG\n\n\nLogical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.\n\n\nPhysical Plan or DAG\n\n\nA physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.\n\n\nData Tuples Processed\n\n\nThis is the number of data objects processed by real-time stream processing\napplications.\n\n\nData Tuples Emitted\n\n\nThis is the number of data objects emitted after real-time stream processing\napplications complete processing operations.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.\n\n\nStreaming Window\n\n\nA streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.\n\n\nSliding Application Window\n\n\nSliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.\n\n\nDemo Applications\n\n\nThe real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.\n\n\nCommand-line Interface\n\n\nCommand line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.\n\n\nWeb services\n\n\nDataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#operators-in-top-n-words-application", 
            "text": "This section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.          Operator  Implementing class  Description    lineReader  LineReader  Reads lines from input files.    wordReader  WordReader  Splits a line into words.    windowWordCount  WindowWordCount  Computes word frequencies for a single window.    fileWordCount  FileWordCount  Maintains per-file and global word frequencies.    wcWriter  WcWriter  Writes top N words and their frequencies to output files.    console  ConsoleOutputOperator  Writes received tuples to console.    snapshotServerFile  AppDataSnapshotServerMap  Caches the last data set for the current file, and returns it in response to queries.    snapshotServerGlobal  AppDataSnapshotServerMap  Caches the last global data set, and returns it in response to queries.    QueryFile  PubSubWebSocketAppDataQuery  Receives queries for per-file data.    QueryGlobal  PubSubWebSocketAppDataQuery  Receives queries for global data.    wsResultFile  PubSubWebSocketAppDataResult  Returns results for per-file queries.    wsResultGlobal  PubSubWebSocketAppDataResult  Returns results for global queries.     We now describe the process of wiring these operators together in the populateDAG()  method of the main application class ApplicationWithQuerySupport . First, the operators are created and added to\nthe DAG via the  addOperator  method:  LineReader lineReader = dag.addOperator( lineReader ,new LineReader());  The first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.  Next, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the  addStream  function, for example:  dag.addStream( lines , lineReader.output, wordReader.input);\n...\ndag.addStream( WordCountsFile , fileWordCount.outputPerFile, snapshotServerFile.input, console.input);  Notice that the stream from  fileWordCount.outputPerFile  (which consists of\nthe top N words for the current file as the file is being read) goes to snapshotServerFile.input  (where it will be saved to respond to queries) and to console.input  (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.  This section provides detailed information about each operator.  LineReader  This class extends  AbstractFileInputOperator String  to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.  The base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods:  openFile ,  closeFile ,  readEntity , and  emit . Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the  FileWordCount  operator.  WordReader  This operator receives lines from  LineReader  on the input port and emits\nwords on the output port. It has a configurable property called  nonWordStr \nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr .\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe  process  method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.  WindowWordCount  This operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when endWindow  is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the endWindow  call, but rather emit output tuples as each input tuple is\nprocessed.  FileWordCount  This operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen  LineReader  reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next  endWindow . The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we  do \nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing beginWindow  and  endWindow  calls by the upstream operator.  This operator also has three output ports: the  outputPerFile  port for the top\nN pairs for the current file as it is being read; the  outputGlobal  port for\nthe global top N pairs, and the  fileOutput  port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.  FileWordCount  also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.  FileWordCount  has a configurable property  topN  for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name:  dt.application.TopNWordsWithQueries.operator.fileWordCount.topN  In the  endWindow  call, both maps are passed to the  getTopNList  function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.  WordCountWriter  This operator extends  AbstractFileOutputOperator Map String,Object , and\nsimply writes the final top N pairs to the output file. As with  LineReader ,\nmost of the complexity of  WordCountWriter  is hidden in the base class. You must\nprovide implementations for 3 methods:  endWindow ,  getFileName , and getBytesForTuple . The first method calls the base class method  requestFinalize .\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The  getFileName \nmethod retrieves the file name from the tuple, and the  getBytesForTuple \nmethod converts the list of pairs to a string in the desired format.  ConsoleOutputOperator  This is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.  AppDataSnapshotServerMap  This operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots    one for a per-file top N snapshot and one for a\nglobal snapshot.  PubSubWebSocketAppDataQuery  This is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:  ws://gateway-host:port/pubsub  where  gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message to the URL where the value of the  data  key\nis the desired message content. The JSON might look like this:  { type : publish ,  topic : foobar ,  data : ...}  Correspondingly, subscribers send messages like this to retrieve published\nmessage data:  { type : subscribe ,  topic : foobar }  Topic names need not be pre-registered anywhere but the same topic\nname (for example,  foobar  in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.  For this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values  TopNWordsQueryFile  and  TopNWordsQueryGlobal  under the\nrespective names:  dt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic  PubSubWebSocketAppDataResult  Analogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values TopNWordsQueryFileResult  and  TopNWordsQueryGlobalResult  corresponding to\nthe respective names:  dt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic", 
            "title": "Operators in Top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#further-exploration", 
            "text": "In this tutorial, the property values in the  properties.xml  file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.  Another aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the  FileWordCount  operator gets an EOF on the\ncontrol port, it waits for an  endWindow  call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.", 
            "title": "Further Exploration"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#datatorrent-terminology", 
            "text": "Operators  Operators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.  Streams  A stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.  Ports  Ports are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.  Directed Acyclic Graph (DAG)  A DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.  Logical Plan or DAG  Logical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.  Physical Plan or DAG  A physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.  Data Tuples Processed  This is the number of data objects processed by real-time stream processing\napplications.  Data Tuples Emitted  This is the number of data objects emitted after real-time stream processing\napplications complete processing operations.  Streaming Application Manager (STRAM)  Streaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.  Streaming Window  A streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.  Sliding Application Window  Sliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.  Demo Applications  The real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.  Command-line Interface  Command line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.  Web services  DataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "DataTorrent terminology"
        }, 
        {
            "location": "/tutorials/sales_dimensions/", 
            "text": "Sales Dimensions - Transform, Analyze and Alert\n\n\nSales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.\n\n\nA large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.\n\n\nIn order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.\n\n\nApplication setup will require following steps:\n\n\n\n\nInput\n - receive individual sales transactions\n\n\nTransform\n - convert incoming records into consumable form\n\n\nEnrich\n - provide additional information for each record by performing additional lookups\n\n\nCompute\n - perform aggregate computations on all possible key field combinations\n\n\nStore\n - store computed results for further analysis and visualizations\n\n\nAnalyze\n, Alert \n Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.\n\n\n\n\nCreate New Application\n\n\nDataTorrent platforms supports building new applications with \nGraphical Application Builder\n, which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.\n\n\nGo to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.\n\n\nThis will bring up the Application Builder interface\n\n\n\n\nAdd and Connect Operators\n\n\nFrom the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.\n\n\n\n\nJSON Sales Event Generator (Input)\n - generates synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.\n\n\nEnrichment (Enrich)\n - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n - stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n - dashboard connector for visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n - dashboard connector for visualization data results.\n\n\n\n\nConnect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.\n\n\n\n\nCustomize Application and Operator Settings\n\n\nBy clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:\n\n\n\n\n\n\nCopy the Sales schema below and paste the contents into the \nEvent Schema JSON\n field of \nInput\n operator, and \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n operators.\n\n\n{\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.  Set the \nBase Path\n to \nSalesDimensionsDemoStore\n value.  This sets the HDHT storage path to write dimensions computation results to the \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n\n\n\n\n\n\n\n\nClick on the stream and set the \nStream Locality\n to \nCONTAINER_LOCAL\n for all the streams between \nInput\n and \nCompute\n operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.\n\n\n\n\n\n\nLaunch Application\n\n\nOnce application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it.\n\n\n\n\nAfter launching, go to \nSales Dimensions\n application operations page in the \nMonitor\n section.\n\n\n\n\nConfirm that the application is launched successfully by looking for \nRunning\n state in the \nApplication Overview\n section, and all confirming all the operators are successfully started under \nStram Events\n section.  By navigating to \nPhysical\n view tab, and looking at \nInput\n, \nParse\n, \nEnrich\n, or \nCompute\n operators, notice that they are all deployed to the single container, thanks to the stream locality setting of \nCONTAINER_LOCAL\n we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.\n\n\n\n\nVisualize Data\n\n\nDataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see \nData Visualization\n tutorial.\n\n\nAfter application is started, a \nvisualize\n button, available in the \nApplication Overview\n section, can be used to quickly generate a new dashboard for the Sales Dimensions application.\n\n\n\n\nOnce dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Sales Dimensions"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#sales-dimensions-transform-analyze-and-alert", 
            "text": "Sales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.  A large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.  In order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.  Application setup will require following steps:   Input  - receive individual sales transactions  Transform  - convert incoming records into consumable form  Enrich  - provide additional information for each record by performing additional lookups  Compute  - perform aggregate computations on all possible key field combinations  Store  - store computed results for further analysis and visualizations  Analyze , Alert   Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.", 
            "title": "Sales Dimensions - Transform, Analyze and Alert"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#create-new-application", 
            "text": "DataTorrent platforms supports building new applications with  Graphical Application Builder , which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.  Go to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.  This will bring up the Application Builder interface", 
            "title": "Create New Application"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#add-and-connect-operators", 
            "text": "From the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.   JSON Sales Event Generator (Input)  - generates synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.  Enrichment (Enrich)  - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.  Simple App Data Dimensions Store (Store)  - stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)  - dashboard connector for visualization queries.  App Data Pub Sub Result (Result)  - dashboard connector for visualization data results.   Connect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.", 
            "title": "Add and Connect Operators"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#customize-application-and-operator-settings", 
            "text": "By clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:    Copy the Sales schema below and paste the contents into the  Event Schema JSON  field of  Input  operator, and  Configuration Schema JSON  of the  Compute  and  Store  operators.  {\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to  SalesDimensionsQuery  and  SalesDimensionsResult  respectively.    Select the  Store  operator, and edit the  File Store  property.  Set the  Base Path  to  SalesDimensionsDemoStore  value.  This sets the HDHT storage path to write dimensions computation results to the  /user/ username /SalesDimensionsDemoStore  on HDFS.     Click on the stream and set the  Stream Locality  to  CONTAINER_LOCAL  for all the streams between  Input  and  Compute  operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.", 
            "title": "Customize Application and Operator Settings"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#launch-application", 
            "text": "Once application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it.   After launching, go to  Sales Dimensions  application operations page in the  Monitor  section.   Confirm that the application is launched successfully by looking for  Running  state in the  Application Overview  section, and all confirming all the operators are successfully started under  Stram Events  section.  By navigating to  Physical  view tab, and looking at  Input ,  Parse ,  Enrich , or  Compute  operators, notice that they are all deployed to the single container, thanks to the stream locality setting of  CONTAINER_LOCAL  we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.", 
            "title": "Launch Application"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#visualize-data", 
            "text": "DataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see  Data Visualization  tutorial.  After application is started, a  visualize  button, available in the  Application Overview  section, can be used to quickly generate a new dashboard for the Sales Dimensions application.   Once dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Visualize Data"
        }, 
        {
            "location": "/apex_development_setup/", 
            "text": "Apache Apex Development Environment Setup\n\n\nThis document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.\n\n\nMicrosoft Windows\n\n\nThere are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:\n\n\n\n\n\n\ngit\n -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows (\nhttp://git-scm.com/download/win\n for example), so download and install a client of your choice.\n\n\n\n\n\n\njava JDK\n (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.\n\n\n\n\n\n\nmaven\n -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from \nhttps://maven.apache.org/download.cgi\n.\n\n\n\n\n\n\nVirtualBox\n -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from \nhttps://www.virtualbox.org/wiki/Downloads\n. It is needed to run the Data Torrent Sandbox.\n\n\n\n\n\n\nDataTorrent Sandbox\n -- The sandbox can be downloaded from \nhttps://www.datatorrent.com/download\n. It is useful for testing simple applications since it contains Apache Hadoop and Data Torrent RTS 3.1.1 pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.\n\n\n\n\n\n\n(Optional) If you prefer to use an IDE (Integrated Development Environment) such as \nNetBeans\n, \nEclipse\n or \nIntelliJ\n, install that as well.\n\n\n\n\n\n\nAfter installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like \njava\n and \njavac\n, the directory might be something like \nC:\\\\Program Files\\\\Java\\\\jdk1.7.0\\_80\\\\bin\n; for \ngit\n it might be \nC:\\\\Program Files\\\\Git\\\\bin\n; and for maven it might be \nC:\\\\Users\\\\user\\\\Software\\\\apache-maven-3.3.3\\\\bin\n. Open a console window and enter the command:\n\n\necho %PATH%\n\n\n\nto see the value of the \nPATH\n variable and verify that the above directories are present. If not, you can change its value clicking on the button at \nControl Panel\n \n \nAdvanced System Settings\n \n \nAdvanced tab\n \n \nEnvironment Variables\n.\n\n\nNow run the following commands and ensure that the output is something similar to that shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput\n\n\n\n\n\n\njavac -version\n\n\njavac 1.7.0_80\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_80\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n\n\n\n\n\ngit --version\n\n\ngit version 2.6.1.windows.1\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)\n\n\nMaven home: C:\\Users\\ram\\Software\\apache-maven-3.3.3\\bin\\..\n\n\nJava version: 1.7.0_80, vendor: Oracle Corporation\n\n\nJava home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre\n\n\nDefault locale: en_US, platform encoding: Cp1252\n\n\nOS name: \nwindows 8\n, version: \n6.2\n, arch: \namd64\n, family: \nwindows\n\n\n\n\n\n\n\n\n\nTo install the sandbox, first download it from \nhttps://www.datatorrent.com/download\n and import the downloaded file into VirtualBox. Once the import completes, you can select it and click the  Start button to start the sandbox.\n\n\nThe sandbox is configured with 6GB RAM; if your development machine has 16GB or more, you can increase the sandbox RAM to 8GB or more using the VirtualBox console. This will yield better performance and support larger applications. Additionally, you can change the network adapter from \nNAT\n to \nBridged Adapter\n; this will allow you to login to the sandbox from your host machine using an \nssh\n tool like \nPuTTY\n and also to transfer files to and from the host using \npscp\n on Windows. Of course all such configuration must be done when when the sandbox is not running.\n\n\nYou can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g. \njdk\n, \ngit\n, \nmaven\n) are pre-installed and also the package files created by your project are directly available to the Data Torrent tools such as  \ndtManage\n and \ndtcli\n. The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.\n\n\nYou can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example, \nnewapp.cmd\n and run it:\n\n\n@echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases ^\n  -DarchetypeGroupId=com.datatorrent ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.1.1 ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal\n\n\n\nThe caret (^) at the end of some lines indicates that a continuation line follows. When you run this file, the properties will be displayed and you will be prompted with \nY: :\n; just press \nEnter\n to complete the project generation.\n\n\nThis command file also exists in the Data Torrent \nexamples\n repository which you can check out with:\n\n\ngit clone https://github.com/DataTorrent/examples\n\n\n\nYou will find the script under \nexamples\\tutorials\\topnwords\\scripts\\newapp.cmd\n.\n\n\nYou can also, if you prefer, use an IDE to generate the project as described in Section 3 of \nApplication Packages\n but use the archetype version 3.1.1 instead of 3.0.0.\n\n\nWhen the run completes successfully, you should see a new directory named \nmyapexapp\n containing a maven project for building a basic Apache Apex application. It includes 3 source files:\nApplication.java\n,  \nRandomNumberGenerator.java\n and \nApplicationTest.java\n. You can now build the application by stepping into the new directory and running the appropriate maven command:\n\n\ncd myapexapp\nmvn clean package -DskipTests\n\n\n\nThe build should create the application package file \nmyapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa\n. This file can then be uploaded to the Data Torrent GUI tool on the sandbox (called \ndtManage\n) and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string  \nhello world:\n.  If you built this package on the host, you can transfer it to the sandbox using the \npscp\n tool bundled with \nPuTTY\n mentioned earlier.\n\n\nIf you want to checkout the Apache Apex source repositories and build them, you can do so by running the script \nbuild-apex.cmd\n located in the same place in the examples repository described above. The source repositories contain more substantial demo applications and the associated source code. Alternatively, if you do not want to use the script, you can follow these simple manual steps:\n\n\n\n\n\n\nCheck out the source code repositories:\n\n\ngit clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\n\n\n\n\nSwitch to the appropriate release branch and build each repository:\n\n\npushd incubator-apex-core\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\n\n\n\n\n\n\n\nThe \ninstall\n argument to the \nmvn\n command installs resources from each project to your local maven repository (typically \n.m2/repository\n under your home directory), and \nnot\n to the system directories, so Administrator privileges are not required. The  \n-DskipTests\n argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.\n\n\nAfter the build completes, you should see the demo application package files in the target directory under each demo subdirectory in \nincubator-apex-malhar\\demos\\\n.\n\n\nLinux\n\n\nMost of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.\n\n\nThe pre-requisites (such as \ngit\n, \nmaven\n, etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is: \necho $PATH\n).\n\n\nThe maven archetype command is the same except that continuation lines use a backslash (\n\\\n) instead of caret (\n^\n); the script for it is available in the same location and is named \nnewapp\n (without the \n.cmd\n extension). The script to checkout and build the Apache Apex repositories is named \nbuild-apex\n.", 
            "title": "Apex Development Setup"
        }, 
        {
            "location": "/apex_development_setup/#apache-apex-development-environment-setup", 
            "text": "This document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.", 
            "title": "Apache Apex Development Environment Setup"
        }, 
        {
            "location": "/apex_development_setup/#microsoft-windows", 
            "text": "There are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:    git  -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows ( http://git-scm.com/download/win  for example), so download and install a client of your choice.    java JDK  (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.    maven  -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from  https://maven.apache.org/download.cgi .    VirtualBox  -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from  https://www.virtualbox.org/wiki/Downloads . It is needed to run the Data Torrent Sandbox.    DataTorrent Sandbox  -- The sandbox can be downloaded from  https://www.datatorrent.com/download . It is useful for testing simple applications since it contains Apache Hadoop and Data Torrent RTS 3.1.1 pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.    (Optional) If you prefer to use an IDE (Integrated Development Environment) such as  NetBeans ,  Eclipse  or  IntelliJ , install that as well.    After installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like  java  and  javac , the directory might be something like  C:\\\\Program Files\\\\Java\\\\jdk1.7.0\\_80\\\\bin ; for  git  it might be  C:\\\\Program Files\\\\Git\\\\bin ; and for maven it might be  C:\\\\Users\\\\user\\\\Software\\\\apache-maven-3.3.3\\\\bin . Open a console window and enter the command:  echo %PATH%  to see the value of the  PATH  variable and verify that the above directories are present. If not, you can change its value clicking on the button at  Control Panel     Advanced System Settings     Advanced tab     Environment Variables .  Now run the following commands and ensure that the output is something similar to that shown in the table below:         Command  Output    javac -version  javac 1.7.0_80    java -version  java version  1.7.0_80  Java(TM) SE Runtime Environment (build 1.7.0_80-b15)  Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)    git --version  git version 2.6.1.windows.1    mvn --version  Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)  Maven home: C:\\Users\\ram\\Software\\apache-maven-3.3.3\\bin\\..  Java version: 1.7.0_80, vendor: Oracle Corporation  Java home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre  Default locale: en_US, platform encoding: Cp1252  OS name:  windows 8 , version:  6.2 , arch:  amd64 , family:  windows     To install the sandbox, first download it from  https://www.datatorrent.com/download  and import the downloaded file into VirtualBox. Once the import completes, you can select it and click the  Start button to start the sandbox.  The sandbox is configured with 6GB RAM; if your development machine has 16GB or more, you can increase the sandbox RAM to 8GB or more using the VirtualBox console. This will yield better performance and support larger applications. Additionally, you can change the network adapter from  NAT  to  Bridged Adapter ; this will allow you to login to the sandbox from your host machine using an  ssh  tool like  PuTTY  and also to transfer files to and from the host using  pscp  on Windows. Of course all such configuration must be done when when the sandbox is not running.  You can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g.  jdk ,  git ,  maven ) are pre-installed and also the package files created by your project are directly available to the Data Torrent tools such as   dtManage  and  dtcli . The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.  You can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example,  newapp.cmd  and run it:  @echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases ^\n  -DarchetypeGroupId=com.datatorrent ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.1.1 ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal  The caret (^) at the end of some lines indicates that a continuation line follows. When you run this file, the properties will be displayed and you will be prompted with  Y: : ; just press  Enter  to complete the project generation.  This command file also exists in the Data Torrent  examples  repository which you can check out with:  git clone https://github.com/DataTorrent/examples  You will find the script under  examples\\tutorials\\topnwords\\scripts\\newapp.cmd .  You can also, if you prefer, use an IDE to generate the project as described in Section 3 of  Application Packages  but use the archetype version 3.1.1 instead of 3.0.0.  When the run completes successfully, you should see a new directory named  myapexapp  containing a maven project for building a basic Apache Apex application. It includes 3 source files: Application.java ,   RandomNumberGenerator.java  and  ApplicationTest.java . You can now build the application by stepping into the new directory and running the appropriate maven command:  cd myapexapp\nmvn clean package -DskipTests  The build should create the application package file  myapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa . This file can then be uploaded to the Data Torrent GUI tool on the sandbox (called  dtManage ) and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string   hello world: .  If you built this package on the host, you can transfer it to the sandbox using the  pscp  tool bundled with  PuTTY  mentioned earlier.  If you want to checkout the Apache Apex source repositories and build them, you can do so by running the script  build-apex.cmd  located in the same place in the examples repository described above. The source repositories contain more substantial demo applications and the associated source code. Alternatively, if you do not want to use the script, you can follow these simple manual steps:    Check out the source code repositories:  git clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar    Switch to the appropriate release branch and build each repository:  pushd incubator-apex-core\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd    The  install  argument to the  mvn  command installs resources from each project to your local maven repository (typically  .m2/repository  under your home directory), and  not  to the system directories, so Administrator privileges are not required. The   -DskipTests  argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.  After the build completes, you should see the demo application package files in the target directory under each demo subdirectory in  incubator-apex-malhar\\demos\\ .", 
            "title": "Microsoft Windows"
        }, 
        {
            "location": "/apex_development_setup/#linux", 
            "text": "Most of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.  The pre-requisites (such as  git ,  maven , etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is:  echo $PATH ).  The maven archetype command is the same except that continuation lines use a backslash ( \\ ) instead of caret ( ^ ); the script for it is available in the same location and is named  newapp  (without the  .cmd  extension). The script to checkout and build the Apache Apex repositories is named  build-apex .", 
            "title": "Linux"
        }, 
        {
            "location": "/application_development/", 
            "text": "Application Developer Guide\n\n\nReal-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.\n\n\nThe DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.\n\n\nDataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.\n\n\nIn the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called \nOperators\n interconnected\nby the data-flow edges called  \nStreams\n.\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the \nOperator Development Guide\n.\n\n\nRunning A Test Application\n\n\nThis chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:\n\n\n\n\nOpen up platform files in your IDE (for example NetBeans, or Eclipse)\n\n\nOpen Demos project\n\n\nOpen Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package\n\n\nSee the results in your system console\n\n\n\n\nCongratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.\n\n\n// Generates random numbers\nRandomEventGenerator rand = dag.addOperator(\nrand\n, new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of \nx\n and \ny\n\nRoundRobinHashMap\nString,Object\n rrhm = dag.addOperator(\nrrhm\n, new RoundRobinHashMap\nString, Object\n());\nrrhm.setKeys(new String[] { \nx\n, \ny\n });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator(\npicalc\n, new Script());\ncalc.setPassThru(false);\ncalc.put(\ni\n,0);\ncalc.put(\ncount\n,0);\ncalc.addSetupScript(\nfunction pi() { if (x*x+y*y \n= \n+maxValue*maxValue+\n) { i++; } count++; return i / count * 4; }\n);\ncalc.setInvoke(\npi\n);\ndag.addStream(\nrand_rrhm\n, rand.integer_data, rrhm.data);\ndag.addStream(\nrrhm_calc\n, rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator(\nconsole\n, new ConsoleOutputOperator());\ndag.addStream(\nrand_console\n,calc.result, console.input);\n\n\n\n\nYou can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.\n\n\nTest Application: Yahoo! Finance Quotes\n\n\nThe PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from  \nYahoo! Finance\n \u00a0and computes the\nfollowing for four tickers, namely \nIBM\n,\n\nGOOG\n, \nYHOO\n.\n\n\n\n\nQuote: Consisting of last trade price, last trade time, and\n    total volume for the day\n\n\nPer-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute\n\n\nSimple Moving Average: trade price over 5 minutes\n\n\n\n\nTotal volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.\n\n\n\n\nThe operator StockTickerInput:\u00a0StockTickerInput\n\u00a0\nis\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:\n\n\n$ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1'\n\nIBM\n,203.966,1513041,\n1:43pm\n\n\nGOOG\n,762.68,1879741,\n1:43pm\n\n\nAAPL\n,444.3385,11738366,\n1:43pm\n\n\nYHOO\n,19.3681,14707163,\n1:43pm\n\n\n\n\n\nAmong all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.\n\n\nHere is the class implementation for StockTickInput:\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap\nString, Long\n lastVolume = new HashMap\nString, Long\n();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Double\n price = new DefaultOutputPort\nKeyValPair\nString, Double\n();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, Long\n volume = new DefaultOutputPort\nKeyValPair\nString, Long\n();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort\nKeyValPair\nString, String\n time = new DefaultOutputPort\nKeyValPair\nString, String\n();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO\nf=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str = \nhttp://download.finance.yahoo.com/d/quotes.csv?s=\n;\n    for (int i = 0; i \n symbols.length; i++) {\n      if (i != 0) {\n        str += \n,\n;\n      }\n      str += symbols[i];\n    }\n    str += \nf=sl1vt1\ne=.csv\n;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter(\nhttp.protocol.cookie-policy\n, CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println(\nMethod failed: \n + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List\nString[]\n myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList\nString\n tuple = new ArrayList\nString\n(Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is \nSymbol\n,\nPrice\n,\nVolume\n,\nTime\n\n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol \n 0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair\nString, Double\n(symbol, currentPrice));\n            volume.emit(new KeyValPair\nString, Long\n(symbol, vol));\n            time.emit(new KeyValPair\nString, String\n(symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}\n\n\n\n\nThe operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.\n\n\nImportant: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.\n\n\nThe method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.\n\n\nMethod\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.\n\n\nNote that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.\n\n\nThe operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal\nK,V\n\u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal\nString,Long\n, where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if  cumulativewas set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.\n\n\nThe operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal\nK\n\u00a0from the\nstream\u00a0package.\n\n\nThe operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal\nK,V\n\u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal\nString,Double\n.\n\n\nThe operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal\nString,Long\n, but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.\n\n\nThe operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.\n\n\nThe operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage\nString,Double\n, which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.\n\n\nThe operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.\n\n\nConnecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.\n\n\npackage com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo. \np\n\n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal\nString, Long\n getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal\nString, Long\n getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal\nString, Long\n oper = dag.addOperator(name, new SumKeyVal\nString, Long\n());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal\nString, Double\n getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal\nString, Double\n oper = dag.addOperator(name, new RangeKeyVal\nString, Double\n());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal\nString,Double,Long,String,?,?\n getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,Double,Long,String,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n oper = dag.addOperator(name, new ConsolidatorKeyVal\nString,HighLow,Long,Object,Object,Object\n());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage\nString, Double\n getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage\nString, Double\n oper = dag.addOperator(name, new SimpleMovingAverage\nString, Double\n());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort\nObject\n getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix + \n: %s\n);\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n       DefaultPartitionCodec\nString, Double\n codec = new DefaultPartitionCodec\nString, Double\n();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n\n}\n\n\n\n\nNote that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.\n\n\nIn the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the \nInstallation Guide\n.\n\n\nRunning a Test Application\n\n\nWe will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).\n\n\nThe platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.\n\n\nThe instructions below assume that the platform was installed in a\ndirectory \nINSTALL_DIR\n and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in\n\nlocal mode\n\u00a0\n(in IDE or from command line) or on a  \nHadoop cluster\n \n.\n\n\nTo start the dtCli run\n\n\nINSTALL_DIR\n/bin/dtcli\n\n\n\nThe command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)\n\n\ndt\n launch -local \nINSTALL_DIR\n/yahoo-finance-demo-3.2.0-SNAPSHOT.apa\n\n\n\nTo terminate the application in local mode, enter Ctrl-C\n\n\nTu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)\n\n\ndt\n launch \nINSTALL_DIR\n/yahoo-finance-demo-3.2.0-SNAPSHOT.apa\n\n\n\nTo stop the application running in Hadoop, terminate it in the dtCli:\n\n\ndt\n kill-app\n\n\n\nExecuting the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.\n\n\nLocal Mode\n\n\nIn local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.\n\n\nHadoop Cluster\n\n\nIn this section we discuss various Hadoop cluster setups.\n\n\nSingle Node Cluster\n\n\nIn a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.\n\n\nIn this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.\n\n\nMulti-Node Cluster\n\n\nIn a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.\n\n\nBefore you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.\n\n\n\n\nApache Apex Platform Overview\n\n\nStreaming Computational Model\n\n\nIn this chapter, we describe the the basics of the real-time streaming platform and its computational model.\n\n\nThe platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .\n\n\nApplications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.\n\n\nThe streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.\n\n\nA fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0\n\n\nThis atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.\n\n\nThe platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.\n\n\nNote that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.\n\n\n\n\nAlongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave  \nproperties\n\n\n\u00a0\nthat can be set to specify the\ndesired computation. Those interested in details, should refer to\n\nApex Malhar Operator Library\n\n.\n\n\nThe platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.\n\n\nA streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.\n\n\nAn operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include\n\n\n\n\n\n\nRunning the Application\n\n\n\n\nRead the\u00a0logical plan\u00a0of the application (DAG) submitted by the client\n\n\nValidate the logical plan\n\n\nTranslate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.\n\n\nRequest resources (Hadoop containers) from Resource Manager,\n    per physical plan\n\n\nBased on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.\n\n\nExecutes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.\n\n\n\n\n\n\n\n\nContinually monitoring the application via heartbeats from each StreamingContainer\n\n\n\n\nCollecting Application System Statistics and Logs\n\n\nLogging all application-wide decisions taken\n\n\nProviding system data on the state of the application via a  Web Service.\n\n\n\n\nSupporting \nFault Tolerance\n\n\na.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper\n\n\n\n\n\n\nSupporting \nDynamic\n    Partitioning\n:\n\u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).\n\n\n\n\nEnabling \nSecurity\n:\n\u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.\n\n\nEnabling \nDynamic  modification\n\u00a0\nof\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.\n\n\n\n\nAn example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.\n\n\n\n\nAn example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.\n\n\n\n\nHadoop Components\n\n\nIn this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.\n\n\nA streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future\n\n\nAll investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.\n\n\nYARN\n\n\nYARN\nis\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through Yarn's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).\n\n\nResource Manager (RM)\n\n\nResourceManager\n(RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.\n\n\nApplication Master (AM)\n\n\nThe AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.\n\n\nNode Managers (NM)\n\n\nThere is one \nNodeManager\n(NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.\n\n\nRPC Protocol\n\n\nCommunication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.\n\n\nHDFS\n\n\nHadoop includes a highly fault tolerant, high throughput\ndistributed file system (\nHDFS\n).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.\n\n\nDeveloping An Application\n\n\nIn this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.\n\n\nDevelopment Process\n\n\nWhile the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.\n\n\nDesign\n\n\n\n\nIdentify common, reusable operators. Use a library\n    if possible.\n\n\nIdentify scalability and performance requirements before\n    designing the DAG.\n\n\nLeverage attributes that the platform supports for scalability\n    and performance.\n\n\nUse operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.\n\n\nUse THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completly. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.\n\n\nThe overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.\n\n\nDo not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.\n\n\nPersist key information to HDFS if possible; it may be useful\n    for debugging later.\n\n\nDecide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.\n\n\n\n\nCreating New Project\n\n\nPlease refer to the \nApex Application Packages\n\u00a0for\nthe basic steps for creating a new project.\n\n\nWriting the application code\n\n\nPreferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.\n\n\nTesting\n\n\nWrite test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.\n\n\nGood test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)\n\n\nRunning an application\n\n\nThe platform provides a commandline tool called dtcli\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.\n\n\nDtcli can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.\n\n\nFor more details on CLI please refer to the \ndtCli Guide\n.\n\n\nApplication API\n\n\nThis section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are\n\n\n\n\n\n\nInstantiate an application (DAG)\n\n\n\n\n\n\n(Optional) Set Attributes\n\n\n\n\nAssign application name\n\n\nSet any other attributes as per application requirements\n\n\n\n\n\n\n\n\nCreate/re-use and instantiate operators\n\n\n\n\nAssign operator name that is unique within the  application\n\n\nDeclare schema upfront for each operator (and thereby its  \nports\n)\n\n\n(Optional) Set \nproperties\n\u00a0\n and \nattributes\n\u00a0\n on the dag as per specification\n\n\nConnect ports of operators via streams\n\n\nEach stream connects one output port of an operator to one or  more input ports of other operators.\n\n\n(Optional) Set attributes on the streams\n\n\n\n\n\n\n\n\n\n\n\n\nTest the application.\n\n\n\n\n\n\nThere are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.\n\n\nJava API\n\n\nThe Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.\n\n\nThe developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.\n\n\nLet us revisit how the Yahoo! Finance test application constructs the DAG:\n\n\npublic class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator(\nStockTickInput\n, dag);\n    SumKeyVal\nString, Long\n dailyVolume = getDailyVolumeOperator(\nDailyVolume\n, dag);\n    ConsolidatorKeyVal\nString,Double,Long,String,?,?\n quoteOperator = getQuoteOperator(\nQuote\n, dag);\n\n    RangeKeyVal\nString, Double\n highlow = getHighLowOperator(\nHighLow\n, dag, appWindowCountMinute);\n    SumKeyVal\nString, Long\n minuteVolume = getMinuteVolumeOperator(\nMinuteVolume\n, dag, appWindowCountMinute);\n    ConsolidatorKeyVal\nString,HighLow,Long,?,?,?\n chartOperator = getChartOperator(\nChart\n, dag);\n\n    SimpleMovingAverage\nString, Double\n priceSMA = getPriceSimpleMovingAverageOperator(\nPriceSMA\n, dag, appWindowCountSMA);\n\n    dag.addStream(\nprice\n, tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream(\nvol\n, tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream(\ntime\n, tick.time, quoteOperator.in3);\n    dag.addStream(\ndaily_vol\n, dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream(\nquote_data\n, quoteOperator.out, getConsole(\nquoteConsole\n, dag, \nQUOTE\n));\n\n    dag.addStream(\nhigh_low\n, highlow.range, chartOperator.in1);\n    dag.addStream(\nvol_1min\n, minuteVolume.sum, chartOperator.in2);\n    dag.addStream(\nchart_data\n, chartOperator.out, getConsole(\nchartConsole\n, dag, \nCHART\n));\n\n    dag.addStream(\nsma_price\n, priceSMA.doubleSMA, getConsole(\npriceSMAConsole\n, dag, \nPrice SMA\n));\n\n    return dag;\n  }\n}\n\n\n\n\nProperty File API\n\n\nThe platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.\n\n\nCreate an application (DAG): myApplication.properties\n\n\n# input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort\n\n\n\n\nAbove snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.\n\n\nAttributes\n\n\nAttributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.\n\n\nOperators\n\n\nOperators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the  \nOperator Developer Guide\n. As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.\n\n\nAll operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.\n\n\nEach operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith  dtcli\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.\n\n\nOperator Interface\n\n\nOperator interface in a DAG consists of \nports\n,\n\u00a0\nproperties\n,\n\u00a0and\n \nattributes\n\n\n.\n\u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.\n\n\nPorts\n\n\nPorts are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.\n\n\nHere are examples of an input and an output port from the operator\nSum.\n\n\n@InputPortFieldAnnotation(name = \ndata\n)\npublic final transient DefaultInputPort\nV\n data = new DefaultInputPort\nV\n() {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort\nV\n sum = new DefaultOutputPort\nV\n(){ \u2026 };\n\n\n\n\nThe process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.\n\n\nThere is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.\n\n\nPort connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.\n\n\nAttributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe \nParallel\nPartitions\n\u00a0\nsection.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.\n\n\nProperties\n\n\nProperties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.\n\n\nAll non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.\n\n\nAttributes\n\n\nAttributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in  \nConfiguration Guide\n.\n\n\nOperator State\n\n\nThe state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe \ncheckpointed\n\u00a0\nevery\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.\n\n\nThe distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.\n\n\nStateless\n\n\nA Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.\n\n\nStateful\n\n\nA Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.\n\n\nOperator API\n\n\nThe Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.\n\n\nThe APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.\n\n\nIn the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.\n\n\nStreaming Window\n\n\nStreaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows\n\n\npublic void process(\ntuple_type\n tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown\n\n\n\n\nA tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.\n\n\nAggregate Application Window\n\n\nAn operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.\n\n\nSliding Application Window\n\n\nA sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.\n\n\nSingle vs Multi-Input Operator\n\n\nA single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.\n\n\nRecovery Mechanisms\n\n\nApplication developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely\n\n\n\n\nAt-least-once: All atomic batches are processed at least once.\n    No data loss occurs.\n\n\nAt-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.\n\n\nExactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.\n\n\n\n\nAt-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.\n\n\nRecovery mechanisms can be specified per Operator while writing\nthe application as shown below.\n\n\nOperator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);\n\n\n\n\nAlso note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.\n\n\nDetails are explained in the chapter on Fault Tolerance\nbelow\n.\n\n\nStreams\n\n\nA stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics\n\n\n\n\nTuples are always delivered in the same order in which they\n    were emitted.\n\n\nConsists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.\n\n\nA stream that connects two containers passes through a\n    buffer server.\n\n\nAll streams can be persisted (by default in HDFS).\n\n\nExactly one output port writes to the stream.\n\n\nCan be read by one or more input ports.\n\n\nConnects operators within an application, not outside\n    an application.\n\n\nHas an unique name within an application.\n\n\nHas attributes which act as hints to STRAM.\n\n\n\n\nStreams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:\n\n\n\n\nTHREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.\n\n\nCONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.\n\n\nNODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.\n\n\nRACK_LOCAL: On nodes in the same rack; also called\n    in-rack.\n\n\nunspecified: No guarantee. Could be anywhere within the\n    cluster\n\n\n\n\n\n\n\n\nAn example of a stream declaration is given below\n\n\nDAG dag = new DAG();\n \u2026\ndag.addStream(\nviews\n, viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality\n\n\n\n\nThe platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.\n\n\nIn a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of  \nstandard operator template\nlibrary\n \n\u00a0\nfollow\nthese principles.\n\n\nA logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.\n\n\nModes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.\n\n\nTHREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.\n\n\nValidating an Application\n\n\nThe platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely\n\n\n\n\nCompile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.\n\n\nInitialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.\n\n\nRun Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.\n\n\n\n\nCompile Time\n\n\nCompile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include\n\n\n\n\nSchema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.\n\n\nStream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream\n\n\nNaming: Compile time checks ensures that applications\n    components operators, streams are named\n\n\n\n\nInitialization/Instantiation Time\n\n\nInitialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.\n\n\nExamples include\n\n\n\n\n\n\nJavaBeans Validation\n:\n    Examples include\n\n\n\n\n@Max(): Value must be less than or equal to the number\n\n\n@Min(): Value must be greater than or equal to the\n    number\n\n\n@NotNull: The value of the field or property must not be\n    null\n\n\n@Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression\n\n\nInput port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)\n\n\nOutput Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)\n\n\n\n\n\n\n\n\nUnique names in application scope: Operators, streams, must have\n    unique names.\n\n\n\n\nCycles in the dag: DAG cannot have a cycle.\n\n\nUnique names in operator scope: Ports, properties, annotations\n    must have unique names.\n\n\nOne stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.\n\n\nApplication Window Period: Has to be an integral multiple the\n    streaming window period.\n\n\n\n\nRun Time\n\n\nRun time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to  \ndemos\n \nt\no\nillustrate these.\n\n\nError ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.\n\n\n\n\nMulti-Tenancy and Security\n\n\nHadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the\n\nOperation and Installation\nGuide\n\n.\n\n\nSecurity\n\n\nThe platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.\n\n\nResource Limits\n\n\nHadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.\n\n\n\n\nScalability and Partitioning\n\n\nScalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.\n\n\nDaily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.\n\n\nThe platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.\n\n\nPartitioning\n\n\nIf all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.\n\n\nTo address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition\n\n\n\n\nLoad balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.\n\n\nSticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.\n\n\n\n\nWe plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.\n\n\nSticky Partition vs Round Robin\n\n\nAs noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.\n\n\nStream Codec\n\n\nThe platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.\n\n\nStatic Partitioning\n\n\nDAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.\n\n\nDynamic Partitioning\n\n\nIn streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.\n\n\nSince partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.\n\n\nDefault Partitioning\n\n\nThe platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.\n\n\nTypically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.\n\n\nDefault Dynamic Partitioning\n\n\nTriggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.\n\n\nThe default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\napartition split\u00a0occurs, resulting in 00\nand  10with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the01 partition, leading to a split into 001  and101\nwith mask 111, etc.\n\n\nShould load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.\n\n\nNxM Partitions\n\n\nWhen two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.\n\n\nFigure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.\n\n\n\n\nParallel\n\n\nIn cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.\n\n\nIn Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.\n\n\nSince operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.\n\n\n\n\nThe following code shows an example of creating a parallel partition.\n\n\ndag.addStream(\nDenormalizedUserId\n, idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);\n\n\n\n\nParallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.\n\n\nParallel Partitions with Streams Modes\n\n\nParallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1-\n2 and 2-\n3 significantly impacts the performance.\n\n\nCONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.\n\n\nA NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.\n\n\nA RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.\n\n\nParallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.\n\n\n\n\nParallel-Partition\n\n\nParallel-Partition with THREAD_LOCAL stream\n\n\nParallel-Partition with CONTAINER_LOCAL stream\n\n\nParallel-Partition with NODE_LOCAL stream\n\n\nParallel-Partition with RACK_LOCAL stream\n\n\n\n\nThese attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.\n\n\n\n\nSkew Balancing Partition\n\n\nSkew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.\n\n\nFigure 7 shows an example of skew balancing partition. An example\nof 3x1 paritition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.\n\n\nLet's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.\n\n\n\n\nSkew Unifier Partition\n\n\nIn this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.\n\n\nTo trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.\n\n\nFigure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.\n\n\nIn the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.\n\n\n\n\nCascading Unifier\n\n\nLet's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.\n\n\nCascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.\n\n\n\n\nFigure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1\n\n F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk) \n F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following\n\n\n\n\nI/O limit on containers to allow proper behavior in an\n    multi-tenant environment\n\n\nLoad on oprD instance\n\n\nBuffer server limits on fan-in, fan-out\n\n\nSize of reservoir buffer for inbound fan-in\n\n\n\n\nA more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.\n\n\nSLA\n\n\nA Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.\n\n\n\n\nFault Tolerance\n\n\nFault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.\n\n\nState of the Application\n\n\nThe state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).\n\n\nOperators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.\n\n\nRecovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.\n\n\nCheckpointing\n\n\nSTRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).\n\n\nThe only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.\n\n\nIn case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.\n\n\nIf an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.\n\n\nCheckpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.\n\n\nAn operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.\n\n\nThe serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.\n\n\nA complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.\n\n\nRecovery Mechanisms\n\n\nRecovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.\n\n\nIn general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notiion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.\n\n\nAt Least Once\n\n\nAt least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.\n\n\nIn general for this recovery mode, the average time lag on a node\noutage is\n\n\n= (CP/2*SW)*T + HC\n\n\nwhere\n\n\n\n\nCP\n\u00a0\u00a0- Checkpointing period (default value is 30 seconds)\n\n\nSW\n\u00a0\u00a0- Streaming window period (default value is 0.5 seconds)\n\n\nT\n\u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory\n\n\nHC\n\u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones\n\n\n\n\nA lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.\n\n\nAt Most Once\n\n\nThis recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.\n\n\nFor multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.\n\n\nIn general, in this recovery mode, the average time lag on a node\noutage is\n\n\n= SW/2 + HC\n\n\nwhere\n\n\n\n\n\n\nSW\n\u00a0- Streaming window period (default value is 0.5\nseconds)\n\n\n\n\n\n\nHC\n\u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones\n\n\n\n\n\n\nExactly Once\n\n\nThis recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.\n\n\nSpeculative Execution\n\n\nIn future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.\n\n\n\n\n\n\nAt an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways\n\n\n\n\nStatically as dictated by STRAM\n\n\nDynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality\n\n\n\n\n\n\n\n\nAt a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner\n\n\n\n\nEntire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.\n\n\n\n\nIn all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.\n\n\n\n\n9: Dynamic Application Modifications\n\n\nDynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.\n\n\nSome examples are\n\n\n\n\nDynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.\n\n\nModification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.\n\n\nModification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.\n\n\nModification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.\n\n\nQuery Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).\n\n\n\n\nDynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to  \nConfiguration Guide\n\n.\n\n\n\n\nUser Interface\n\n\nThe platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to  \nUI Console Guide\n.\n\n\nDemos\n\n\nIn this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source\n\nApache Apex-Malhar repository\n.\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.\n\n\n\n\nComputation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).\n\n\nYahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).\n\n\nEchoserver Reads messages from a\n    network connection and echoes them back out.\n\n\nTwitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes\n\n\nTwitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes\n\n\nTwitter top N frequent words:\n    Computes top N frequent words in a sliding window\n\n\nWord count: Computes word count for\n    all words within a large file\n\n\nMobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).\n\n\nFrauddetect: Analyzes a stream of\n    credit card merchant transactions.\n\n\nMroperator:Contains several\n    map-reduce applications.\n\n\nR: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).\n\n\nMachinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Applications"
        }, 
        {
            "location": "/application_development/#application-developer-guide", 
            "text": "Real-time big data processing is not only important but has become\ncritical for businesses which depend on accurate and timely analysis of\ntheir business data. A few businesses have yielded to very expensive\nsolutions like building an in-house, real-time analytics infrastructure\nsupported by an internal development team, or buying expensive\nproprietary software. A large number of businesses are dealing with the\nrequirement just by trying to make Hadoop do their batch jobs in smaller\niterations. Over the last few years, Hadoop has become ubiquitous in the\nbig data processing space, replacing expensive proprietary hardware and\nsoftware solutions for massive data processing with very cost-effective,\nfault-tolerant, open-sourced, and commodity-hardware-based solutions.\nWhile Hadoop has been a game changer for companies, it is primarily a\nbatch-oriented system, and does not yet have a viable option for\nreal-time data processing. \u00a0Most companies with real-time data\nprocessing end up having to build customized solutions in addition to\ntheir Hadoop infrastructure.  The DataTorrent platform is designed to process massive amounts of\nreal-time events natively in Hadoop. This can be event ingestion,\nprocessing, and aggregation for real-time data analytics, or can be\nreal-time business logic decisioning such as cell tower load balancing,\nreal-time ads bidding, or fraud detection. \u00a0The platform has the ability\nto repair itself in real-time (without data loss) if hardware fails, and\nadapt to changes in load by adding and removing computing resources\nautomatically.  DataTorrent is a native Hadoop application. It runs as a YARN\n(Hadoop 2.x) application and leverages Hadoop as a distributed operating\nsystem. All the basic distributed operating system capabilities of\nHadoop like resource allocation (Resource Manager, distributed file system (HDFS),\nmulti-tenancy, security, fault-tolerance, scalability,\u00a0etc.\nare supported natively in all streaming applications. \u00a0Just as Hadoop\nfor map-reduce handles all the details of the application allowing you\nto only focus on writing the application (the mapper and reducer\nfunctions), the platform handles all the details of streaming execution,\nallowing you to only focus on your business logic. Using the platform\nremoves the need to maintain separate clusters for real-time\napplications.  In the platform, building a streaming application can be extremely\neasy and intuitive. \u00a0The application is represented as a Directed\nAcyclic Graph (DAG) of computation units called  Operators  interconnected\nby the data-flow edges called   Streams .\u00a0The operators process input\nstreams and produce output streams. A library of common operators is\nprovided to enable quick application development. \u00a0In case the desired\nprocessing is not available in the Operator Library, one can easily\nwrite a custom operator. We refer those interested in creating their own\noperators to the  Operator Development Guide .", 
            "title": "Application Developer Guide"
        }, 
        {
            "location": "/application_development/#running-a-test-application", 
            "text": "This chapter will help you with a quick start on running an\napplication. If you are starting with the platform for the first time,\nit would be informative to open an existing application and see it run.\nDo the following steps to run the PI demo, which computes the value of\nPI \u00a0in a simple\nmanner:   Open up platform files in your IDE (for example NetBeans, or Eclipse)  Open Demos project  Open Test Packages and run ApplicationTest.java\u00a0in pi\u00a0package  See the results in your system console   Congratulations, you just ran your first real-time streaming demo\n:) This demo is very simple and has four operators. The first operator\nemits random integers between 0 to 30, 000. The second operator receives\nthese coefficients and emits a hashmap with x and y values each time it\nreceives two values. The third operator takes these values and computes\nx**2+y**2. The last operator counts how many computed values from\nthe previous operator were less than or equal to 30, 000**2. Assuming\nthis count is N, then PI is computed as N/number of values received.\nHere is the code snippet for the PI application. This code populates the\nDAG. Do not worry about what each line does, we will cover these\nconcepts later in this document.  // Generates random numbers\nRandomEventGenerator rand = dag.addOperator( rand , new RandomEventGenerator());\nrand.setMinvalue(0);\nrand.setMaxvalue(30000);\n\n// Generates a round robin HashMap of  x  and  y \nRoundRobinHashMap String,Object  rrhm = dag.addOperator( rrhm , new RoundRobinHashMap String, Object ());\nrrhm.setKeys(new String[] {  x ,  y  });\n\n// Calculates pi from x and y\nJavaScriptOperator calc = dag.addOperator( picalc , new Script());\ncalc.setPassThru(false);\ncalc.put( i ,0);\ncalc.put( count ,0);\ncalc.addSetupScript( function pi() { if (x*x+y*y  =  +maxValue*maxValue+ ) { i++; } count++; return i / count * 4; } );\ncalc.setInvoke( pi );\ndag.addStream( rand_rrhm , rand.integer_data, rrhm.data);\ndag.addStream( rrhm_calc , rrhm.map, calc.inBindings);\n\n// puts results on system console\nConsoleOutputOperator console = dag.addOperator( console , new ConsoleOutputOperator());\ndag.addStream( rand_console ,calc.result, console.input);  You can review the other demos and see what they do. The examples\ngiven in the Demos project cover various features of the platform and we\nstrongly encourage you to read these to familiarize yourself with the\nplatform. In the remaining part of this document we will go through\ndetails needed for you to develop and run streaming applications in\nMalhar.", 
            "title": "Running A Test Application"
        }, 
        {
            "location": "/application_development/#test-application-yahoo-finance-quotes", 
            "text": "The PI\u00a0application was to\nget you started. It is a basic application and does not fully illustrate\nthe features of the platform. For the purpose of describing concepts, we\nwill consider the test application shown in Figure 1. The application\ndownloads tick data from   Yahoo! Finance  \u00a0and computes the\nfollowing for four tickers, namely  IBM , GOOG ,  YHOO .   Quote: Consisting of last trade price, last trade time, and\n    total volume for the day  Per-minute chart data: Highest trade price, lowest trade\n    price, and volume during that minute  Simple Moving Average: trade price over 5 minutes   Total volume must ensure that all trade volume for that day is\nadded, i.e. data loss would result in wrong results. Charting data needs\nall the trades in the same minute to go to the same slot, and then on it\nstarts afresh, so again data loss would result in wrong results. The\naggregation for charting data is done over 1 minute. Simple moving\naverage computes the average price over a 5 minute sliding window; it\ntoo would produce wrong results if there is data loss. Figure 1 shows\nthe application with no partitioning.   The operator StockTickerInput:\u00a0StockTickerInput \u00a0 is\nthe input operator that reads live data from Yahoo! Finance once per\ninterval (user configurable in milliseconds), and emits the price, the\nincremental volume, and the last trade time of each stock symbol, thus\nemulating real ticks from the exchange. \u00a0We utilize the Yahoo! Finance\nCSV web service interface. \u00a0For example:  $ GET 'http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1' IBM ,203.966,1513041, 1:43pm  GOOG ,762.68,1879741, 1:43pm  AAPL ,444.3385,11738366, 1:43pm  YHOO ,19.3681,14707163, 1:43pm   Among all the operators in Figure 1, StockTickerInput is the only\noperator that requires extra code because it contains a custom mechanism\nto get the input data. \u00a0Other operators are used unchanged from the\nMalhar library.  Here is the class implementation for StockTickInput:  package com.datatorrent.demos.yahoofinance;\n\nimport au.com.bytecode.opencsv.CSVReader;\nimport com.datatorrent.annotation.OutputPortFieldAnnotation;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DefaultOutputPort;\nimport com.datatorrent.api.InputOperator;\nimport com.datatorrent.lib.util.KeyValPair;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.util.*;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.httpclient.HttpStatus;\nimport org.apache.commons.httpclient.cookie.CookiePolicy;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport org.apache.commons.httpclient.params.DefaultHttpParams;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * This operator sends price, volume and time into separate ports and calculates incremental volume.\n */\npublic class StockTickInput implements InputOperator\n{\n  private static final Logger logger = LoggerFactory.getLogger(StockTickInput.class);\n  /**\n   * Timeout interval for reading from server. 0 or negative indicates no timeout.\n   */\n  public int readIntervalMillis = 500;\n  /**\n   * The URL of the web service resource for the POST request.\n   */\n  private String url;\n  public String[] symbols;\n  private transient HttpClient client;\n  private transient GetMethod method;\n  private HashMap String, Long  lastVolume = new HashMap String, Long ();\n  private boolean outputEvenIfZeroVolume = false;\n  /**\n   * The output port to emit price.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Double  price = new DefaultOutputPort KeyValPair String, Double ();\n  /**\n   * The output port to emit incremental volume.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, Long  volume = new DefaultOutputPort KeyValPair String, Long ();\n  /**\n   * The output port to emit last traded time.\n   */\n  @OutputPortFieldAnnotation(optional = true)\n  public final transient DefaultOutputPort KeyValPair String, String  time = new DefaultOutputPort KeyValPair String, String ();\n\n  /**\n   * Prepare URL from symbols and parameters. URL will be something like: http://download.finance.yahoo.com/d/quotes.csv?s=IBM,GOOG,AAPL,YHOO f=sl1vt1\n   *\n   * @return the URL\n   */\n  private String prepareURL()\n  {\n    String str =  http://download.finance.yahoo.com/d/quotes.csv?s= ;\n    for (int i = 0; i   symbols.length; i++) {\n      if (i != 0) {\n        str +=  , ;\n      }\n      str += symbols[i];\n    }\n    str +=  f=sl1vt1 e=.csv ;\n    return str;\n  }\n\n  @Override\n  public void setup(OperatorContext context)\n  {\n    url = prepareURL();\n    client = new HttpClient();\n    method = new GetMethod(url);\n    DefaultHttpParams.getDefaultParams().setParameter( http.protocol.cookie-policy , CookiePolicy.BROWSER_COMPATIBILITY);\n  }\n\n  @Override\n  public void teardown()\n  {\n  }\n\n  @Override\n  public void emitTuples()\n  {\n\n    try {\n      int statusCode = client.executeMethod(method);\n      if (statusCode != HttpStatus.SC_OK) {\n        System.err.println( Method failed:   + method.getStatusLine());\n      }\n      else {\n        InputStream istream = method.getResponseBodyAsStream();\n        // Process response\n        InputStreamReader isr = new InputStreamReader(istream);\n        CSVReader reader = new CSVReader(isr);\n        List String[]  myEntries = reader.readAll();\n        for (String[] stringArr: myEntries) {\n          ArrayList String  tuple = new ArrayList String (Arrays.asList(stringArr));\n          if (tuple.size() != 4) {\n            return;\n          }\n          // input csv is  Symbol , Price , Volume , Time \n          String symbol = tuple.get(0);\n          double currentPrice = Double.valueOf(tuple.get(1));\n          long currentVolume = Long.valueOf(tuple.get(2));\n          String timeStamp = tuple.get(3);\n          long vol = currentVolume;\n          // Sends total volume in first tick, and incremental volume afterwards.\n          if (lastVolume.containsKey(symbol)) {\n            vol -= lastVolume.get(symbol);\n          }\n\n          if (vol   0 || outputEvenIfZeroVolume) {\n            price.emit(new KeyValPair String, Double (symbol, currentPrice));\n            volume.emit(new KeyValPair String, Long (symbol, vol));\n            time.emit(new KeyValPair String, String (symbol, timeStamp));\n            lastVolume.put(symbol, currentVolume);\n          }\n        }\n      }\n      Thread.sleep(readIntervalMillis);\n    }\n    catch (InterruptedException ex) {\n      logger.debug(ex.toString());\n    }\n    catch (IOException ex) {\n      logger.debug(ex.toString());\n    }\n  }\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n  }\n\n  @Override\n  public void endWindow()\n  {\n  }\n\n  public void setOutputEvenIfZeroVolume(boolean outputEvenIfZeroVolume)\n  {\n       this.outputEvenIfZeroVolume = outputEvenIfZeroVolume;\n  }\n\n}  The operator has three output ports that emit the price of the\nstock, the volume of the stock and the last trade time of the stock,\ndeclared as public member variables price, volume\u00a0and  time\u00a0of the class. \u00a0The tuple of the\nprice\u00a0output port is a key-value\npair with the stock symbol being the key, and the price being the value.\n\u00a0The tuple of the volume\u00a0output\nport is a key value pair with the stock symbol being the key, and the\nincremental volume being the value. \u00a0The tuple of the  time\u00a0output port is a key value pair with the\nstock symbol being the key, and the last trade time being the\nvalue.  Important: Since operators will be\nserialized, all input and output ports need to be declared transient\nbecause they are stateless and should not be serialized.  The method\u00a0setup(OperatorContext)\ncontains the code that is necessary for setting up the HTTP\nclient for querying Yahoo! Finance.  Method\u00a0emitTuples() contains\nthe code that reads from Yahoo! Finance, and emits the data to the\noutput ports of the operator. \u00a0emitTuples()\u00a0will be called one or more times\nwithin one application window as long as time is allowed within the\nwindow.  Note that we want to emulate the tick input stream by having\nincremental volume data with Yahoo! Finance data. \u00a0We therefore subtract\nthe previous volume from the current volume to emulate incremental\nvolume for each tick.  The operator\nDailyVolume:\u00a0This operator\nreads from the input port, which contains the incremental volume tuples\nfrom StockTickInput, and\naggregates the data to provide the cumulative volume. \u00a0It uses the\nlibrary class  SumKeyVal K,V \u00a0provided in math\u00a0package. \u00a0In this case,\nSumKeyVal String,Long , where K is the stock symbol, V is the\naggregated volume, with cumulative\nset to true. (Otherwise if  cumulativewas set to false, SumKeyVal would\nprovide the sum for the application window.) \u00a0Malhar provides a number\nof built-in operators for simple operations like this so that\napplication developers do not have to write them. \u00a0More examples to\nfollow. This operator assumes that the application restarts before the\nmarket opens every day.  The operator Quote:\nThis operator has three input ports, which are price (from\nStockTickInput), daily_vol (from\nDaily Volume), and time (from\n StockTickInput). \u00a0This operator\njust consolidates the three data items and and emits the consolidated\ndata. \u00a0It utilizes the class ConsolidatorKeyVal K \u00a0from the\nstream\u00a0package.  The operator HighLow:\u00a0This operator reads from the input port,\nwhich contains the price tuples from StockTickInput, and provides the high and the\nlow price within the application window. \u00a0It utilizes the library class\n RangeKeyVal K,V \u00a0provided\nin the math\u00a0package. In this case,\nRangeKeyVal String,Double .  The operator MinuteVolume:\nThis operator reads from the input port, which contains the\nvolume tuples from StockTickInput,\nand aggregates the data to provide the sum of the volume within one\nminute. \u00a0Like the operator  DailyVolume, this operator also uses\nSumKeyVal String,Long , but\nwith cumulative set to false. \u00a0The\nApplication Window is set to one minute. We will explain how to set this\nlater.  The operator Chart:\nThis operator is very similar to the operator Quote, except that it takes inputs from\nHigh Low\u00a0and  Minute Vol\u00a0and outputs the consolidated tuples\nto the output port.  The operator PriceSMA:\nSMA stands for - Simple Moving Average. It reads from the\ninput port, which contains the price tuples from StockTickInput, and\nprovides the moving average price of the stock. \u00a0It utilizes\nSimpleMovingAverage String,Double , which is provided in the\n multiwindow\u00a0package.\nSimpleMovingAverage keeps track of the data of the previous N\napplication windows in a sliding manner. \u00a0For each end window event, it\nprovides the average of the data in those application windows.  The operator Console:\nThis operator just outputs the input tuples to the console\n(or stdout). \u00a0In this example, there are four console\u00a0operators, which connect to the output\nof  Quote, Chart, PriceSMA and VolumeSMA. \u00a0In\npractice, they should be replaced by operators that use the data to\nproduce visualization artifacts like charts.  Connecting the operators together and constructing the\nDAG:\u00a0Now that we know the\noperators used, we will create the DAG, set the streaming window size,\ninstantiate the operators, and connect the operators together by adding\nstreams that connect the output ports with the input ports among those\noperators. \u00a0This code is in the file  YahooFinanceApplication.java. Refer to Figure 1\nagain for the graphical representation of the DAG. \u00a0The last method in\nthe code, namely getApplication(),\ndoes all that. \u00a0The rest of the methods are just for setting up the\noperators.  package com.datatorrent.demos.yahoofinance;\n\nimport com.datatorrent.api.ApplicationFactory;\nimport com.datatorrent.api.Context.OperatorContext;\nimport com.datatorrent.api.DAG;\nimport com.datatorrent.api.Operator.InputPort;\nimport com.datatorrent.lib.io.ConsoleOutputOperator;\nimport com.datatorrent.lib.math.RangeKeyVal;\nimport com.datatorrent.lib.math.SumKeyVal;\nimport com.datatorrent.lib.multiwindow.SimpleMovingAverage;\nimport com.datatorrent.lib.stream.ConsolidatorKeyVal;\nimport com.datatorrent.lib.util.HighLow;\nimport org.apache.hadoop.conf.Configuration;\n\n/**\n * Yahoo! Finance application demo.  p \n *\n * Get Yahoo finance feed and calculate minute price range, minute volume, simple moving average of 5 minutes.\n */\npublic class Application implements StreamingApplication\n{\n  private int streamingWindowSizeMilliSeconds = 1000; // 1 second (default is 500ms)\n  private int appWindowCountMinute = 60;   // 1 minute\n  private int appWindowCountSMA = 5 * 60;  // 5 minute\n\n  /**\n   * Get actual Yahoo finance ticks of symbol, last price, total daily volume, and last traded price.\n   */\n  public StockTickInput getStockTickInputOperator(String name, DAG dag)\n  {\n    StockTickInput oper = dag.addOperator(name, StockTickInput.class);\n    oper.readIntervalMillis = 200;\n    return oper;\n  }\n\n  /**\n   * This sends total daily volume by adding volumes from each ticks.\n   */\n  public SumKeyVal String, Long  getDailyVolumeOperator(String name, DAG dag)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setCumulative(true);\n    return oper;\n  }\n\n  /**\n   * Get aggregated volume of 1 minute and send at the end window of 1 minute.\n   */\n  public SumKeyVal String, Long  getMinuteVolumeOperator(String name, DAG dag, int appWindowCount)\n  {\n    SumKeyVal String, Long  oper = dag.addOperator(name, new SumKeyVal String, Long ());\n    oper.setType(Long.class);\n    oper.setEmitOnlyWhenChanged(true);\ndag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    return oper;\n  }\n\n  /**\n   * Get High-low range for 1 minute.\n   */\n  public RangeKeyVal String, Double  getHighLowOperator(String name, DAG dag, int appWindowCount)\n  {\n    RangeKeyVal String, Double  oper = dag.addOperator(name, new RangeKeyVal String, Double ());\n    dag.getOperatorMeta(name).getAttributes().put(OperatorContext.APPLICATION_WINDOW_COUNT,appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Quote (Merge price, daily volume, time)\n   */\n  public ConsolidatorKeyVal String,Double,Long,String,?,?  getQuoteOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,Double,Long,String,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,Double,Long,String,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Chart (Merge minute volume and minute high-low)\n   */\n  public ConsolidatorKeyVal String,HighLow,Long,?,?,?  getChartOperator(String name, DAG dag)\n  {\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  oper = dag.addOperator(name, new ConsolidatorKeyVal String,HighLow,Long,Object,Object,Object ());\n    return oper;\n  }\n\n  /**\n   * Get simple moving average of price.\n   */\n  public SimpleMovingAverage String, Double  getPriceSimpleMovingAverageOperator(String name, DAG dag, int appWindowCount)\n  {\n    SimpleMovingAverage String, Double  oper = dag.addOperator(name, new SimpleMovingAverage String, Double ());\n    oper.setWindowSize(appWindowCount);\n    oper.setType(Double.class);\n    return oper;\n  }\n\n  /**\n   * Get console for output.\n   */\n  public InputPort Object  getConsole(String name, /*String nodeName,*/ DAG dag, String prefix)\n  {\n    ConsoleOutputOperator oper = dag.addOperator(name, ConsoleOutputOperator.class);\n    oper.setStringFormat(prefix +  : %s );\n    return oper.input;\n  }\n\n  /**\n   * Create Yahoo Finance Application DAG.\n   */\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().put(DAG.STRAM_WINDOW_SIZE_MILLIS,streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n       DefaultPartitionCodec String, Double  codec = new DefaultPartitionCodec String, Double ();\n    dag.setInputPortAttribute(highlow.data, PortContext.STREAM_CODEC, codec);\n    dag.setInputPortAttribute(priceSMA.data, PortContext.STREAM_CODEC, codec);\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n\n}  Note that we also set a user-specific sliding window for SMA that\nkeeps track of the previous N data points. \u00a0Do not confuse this with the\nattribute APPLICATION_WINDOW_COUNT.  In the rest of this chapter we will run through the process of\nrunning this application. We assume that \u00a0you are familiar with details\nof your Hadoop infrastructure. For installation\ndetails please refer to the  Installation Guide .", 
            "title": "Test Application: Yahoo! Finance Quotes"
        }, 
        {
            "location": "/application_development/#running-a-test-application_1", 
            "text": "We will now describe how to run the yahoo\nfinance application\u00a0described above in different modes\n(local mode, single node on Hadoop, and multi-nodes on Hadoop).  The platform runs streaming applications under the control of a\nlight-weight Streaming Application Manager (STRAM). Each application has\nits own instance of STRAM. STRAM launches the application and\ncontinually provides run time monitoring, analysis, and takes action\nsuch as load scaling or outage recovery as needed. \u00a0We will discuss\nSTRAM in more detail in the next chapter.  The instructions below assume that the platform was installed in a\ndirectory  INSTALL_DIR  and the command line interface (CLI) will\nbe used to launch the demo application. An application can be run in local mode \u00a0 (in IDE or from command line) or on a   Hadoop cluster   .  To start the dtCli run  INSTALL_DIR /bin/dtcli  The command line prompt appears.  To start the application in local mode (the actual version number in the file name may differ)  dt  launch -local  INSTALL_DIR /yahoo-finance-demo-3.2.0-SNAPSHOT.apa  To terminate the application in local mode, enter Ctrl-C  Tu run the application on the Hadoop cluster (the actual version\nnumber in the file name may differ)  dt  launch  INSTALL_DIR /yahoo-finance-demo-3.2.0-SNAPSHOT.apa  To stop the application running in Hadoop, terminate it in the dtCli:  dt  kill-app  Executing the application in either mode includes the following\nsteps. At a top level, STRAM (Streaming Application Manager) validates\nthe application (DAG), translates the logical plan to the physical plan\nand then launches the execution engine. The mode determines the\nresources needed and how how they are used.", 
            "title": "Running a Test Application"
        }, 
        {
            "location": "/application_development/#local-mode", 
            "text": "In local mode, the application is run as a single-process\u00a0with multiple threads. Although a\nfew Hadoop classes are needed, there is no dependency on a Hadoop\ncluster or Hadoop services. The local file system is used in place of\nHDFS. This mode allows a quick run of an application in a single process\nsandbox, and hence is the most suitable to debug and analyze the\napplication logic. This mode is recommended for developing the\napplication and can be used for running applications within the IDE for\nfunctional testing purposes. Due to limited resources and lack \u00a0of\nscalability an application running in this single process mode is more\nlikely to encounter throughput bottlenecks. A distributed cluster is\nrecommended for benchmarking and production testing.", 
            "title": "Local Mode"
        }, 
        {
            "location": "/application_development/#hadoop-cluster", 
            "text": "In this section we discuss various Hadoop cluster setups.  Single Node Cluster  In a single node Hadoop cluster all services are deployed on a\nsingle server (a developer can use his/her development machine as a\nsingle node cluster). The platform does not distinguish between a single\nor multi-node setup and behaves exactly the same in both cases.  In this mode, the resource manager, name node, data node, and node\nmanager occupy one process each. This is an example of running a\nstreaming application as a multi-process\u00a0application on the same server.\nWith prevalence of fast, multi-core systems, this mode is effective for\ndebugging, fine tuning, and generic analysis before submitting the job\nto a larger Hadoop cluster. In this mode, execution uses the Hadoop\nservices and hence is likely to identify issues that are related to the\nHadoop environment (such issues will not be uncovered in local mode).\nThe throughput will obviously not be as high as on a multi-node Hadoop\ncluster. Additionally, since each container (i.e. Java process) requires\na significant amount of memory, you will be able to run a much smaller\nnumber of containers than on a multi-node cluster.  Multi-Node Cluster  In a multi-node Hadoop cluster all the services of Hadoop are\ntypically distributed across multiple nodes in a production or\nproduction-level test environment. Upon launch the application is\nsubmitted to the Hadoop cluster and executes as a  multi-processapplication on\u00a0multiple nodes.  Before you start deploying, testing and troubleshooting your\napplication on a cluster, you should ensure that Hadoop (version 2.2.0\nor later)\u00a0is properly installed and\nyou have basic skills for working with it.", 
            "title": "Hadoop Cluster"
        }, 
        {
            "location": "/application_development/#apache-apex-platform-overview", 
            "text": "", 
            "title": "Apache Apex Platform Overview"
        }, 
        {
            "location": "/application_development/#streaming-computational-model", 
            "text": "In this chapter, we describe the the basics of the real-time streaming platform and its computational model.  The platform is designed to enable completely asynchronous real time computations\u00a0done in as unblocked a way as possible with\nminimal overhead .  Applications running in the platform are represented by a Directed\nAcyclic Graph (DAG) made up of \u00a0operators and streams. All computations\nare done in memory on arrival of\nthe input data, with an option to save the output to disk (HDFS) in a\nnon-blocking way. The data that flows between operators consists of\natomic data elements. Each data element along with its type definition\n(henceforth called  schema) is\ncalled a tuple.\u00a0An application is a\ndesign of the flow of these tuples to and from\nthe appropriate compute units to enable the computation of the final\ndesired results.\u00a0A message queue (henceforth called\n\u00a0buffer server) manages tuples streaming\nbetween compute units in different processes.This server keeps track of\nall consumers, publishers, partitions, and enables replay. More\ninformation is given in later section.  The streaming application is monitored by a decision making entity\ncalled STRAM (streaming application\nmanager).\u00a0STRAM is designed to be a light weight\ncontroller that has minimal but sufficient interaction with the\napplication. This is done via periodic heartbeats. The\nSTRAM does the initial launch and periodically analyzes the system\nmetrics to decide if any run time action needs to be taken.  A fundamental building block for the streaming platform\nis the concept of breaking up a stream into equal finite time slices\ncalled streaming windows. Each window contains the ordered\nset of tuples in that time slice. A typical duration of a window is 500\nms, but can be configured per application (the Yahoo! Finance\napplication configures this value in the  properties.xml\u00a0file to be 1000ms = 1s). Each\nwindow is preceded by a begin_window\u00a0event and is terminated by an\nend_window\u00a0event, and is assigned\na unique window ID. Even though the platform performs computations at\nthe tuple level, bookkeeping is done at the window boundary, making the\ncomputations within a window an atomic event in the platform. \u00a0We can\nthink of each window as an  atomic\nmicro-batch\u00a0of tuples, to be processed together as one\natomic operation (See Figure 2). \u00a0  This atomic batching allows the platform to avoid the very steep\nper tuple bookkeeping cost and instead has a manageable per batch\nbookkeeping cost. This translates to higher throughput, low recovery\ntime, and higher scalability. Later in this document we illustrate how\nthe atomic micro-batch concept allows more efficient optimization\nalgorithms.  The platform also has in-built support for\napplication windows.\u00a0 An application window is part of the\napplication specification, and can be a small or large multiple of the\nstreaming window. \u00a0An example from our Yahoo! Finance test application\nis the moving average, calculated over a sliding application window of 5\nminutes which equates to 300 (= 5 * 60) streaming windows.  Note that these two window concepts are distinct. \u00a0A streaming\nwindow is an abstraction of many tuples into a higher atomic event for\neasier management. \u00a0An application window is a group of consecutive\nstreaming windows used for data aggregation (e.g. sum, average, maximum,\nminimum) on a per operator level.   Alongside the platform,\u00a0a set of\npredefined, benchmarked standard library operator templates is provided\nfor ease of use and rapid development of application.\u00a0These\noperators are open sourced to Apache Software Foundation under the\nproject name \u201cMalhar\u201d as part of our efforts to foster community\ninnovation. These operators can be used in a DAG as is, while others\nhave   properties  \u00a0 that can be set to specify the\ndesired computation. Those interested in details, should refer to Apex Malhar Operator Library \n.  The platform is a Hadoop YARN native\napplication. It runs in a Hadoop cluster just like any\nother YARN application (MapReduce etc.) and is designed to seamlessly\nintegrate with rest of Hadoop technology stack. It leverages Hadoop as\nmuch as possible and relies on it as its distributed operating system.\nHadoop dependencies include resource management, compute/memory/network\nallocation, HDFS, security, fault tolerance, monitoring, metrics,\nmulti-tenancy, logging etc. Hadoop classes/concepts are reused as much\nas possible.  The aim is to enable enterprises\nto leverage their existing Hadoop infrastructure for real time streaming\napplications. The platform is designed to scale with big\ndata applications and scale with Hadoop.  A streaming application is an asynchronous execution of\ncomputations across distributed nodes. All computations are done in\nparallel on a distributed cluster. The computation model is designed to\ndo as many parallel computations as possible in a non-blocking fashion.\nThe task of monitoring of the entire application is done on (streaming)\nwindow boundaries with a streaming window as an atomic entity. A window\ncompletion is a quantum of work done. There is no assumption that an\noperator can be interrupted at precisely a particular tuple or window.  An operator itself also\ncannot assume or predict the exact time a tuple that it emitted would\nget consumed by downstream operators. The operator processes the tuples\nit gets and simply emits new tuples based on its business logic. The\nonly guarantee it has is that the upstream operators are processing\neither the current or some later window, and the downstream operator is\nprocessing either the current or some earlier window. The completion of\na window (i.e. propagation of the  end_window\u00a0event through an operator) in any\noperator guarantees that all upstream operators have finished processing\nthis window. Thus, the end_window\u00a0event is blocking on an operator\nwith multiple outputs, and is a synchronization point in the DAG. The\n begin_window\u00a0event does not have\nany such restriction, a single begin_window\u00a0event from any upstream operator\ntriggers the operator to start processing tuples.", 
            "title": "Streaming Computational Model"
        }, 
        {
            "location": "/application_development/#streaming-application-manager-stram", 
            "text": "Streaming Application Manager (STRAM) is the Hadoop YARN native\napplication master. STRAM is the first process that is activated upon\napplication launch and orchestrates the streaming application on the\nplatform. STRAM is a lightweight controller process. The\nresponsibilities of STRAM include    Running the Application   Read the\u00a0logical plan\u00a0of the application (DAG) submitted by the client  Validate the logical plan  Translate the logical plan into a physical plan, where certain operators may  be partitioned (i.e. replicated) to multiple operators for  handling load.  Request resources (Hadoop containers) from Resource Manager,\n    per physical plan  Based on acquired resources and application attributes, create\n    an execution plan\u00a0by partitioning the DAG into fragments,\n    each assigned to different containers.  Executes the application by deploying each fragment to\n    its container. Containers then start stream processing and run\n    autonomously, processing one streaming window after another. Each\n    container is represented as an instance of the  StreamingContainer\u00a0class, which updates\n    STRAM via the heartbeat protocol and processes directions received\n    from STRAM.     Continually monitoring the application via heartbeats from each StreamingContainer   Collecting Application System Statistics and Logs  Logging all application-wide decisions taken  Providing system data on the state of the application via a  Web Service.   Supporting  Fault Tolerance  a.  Detecting a node outage\nb.  Requesting a replacement resource from the Resource Manager\n    and scheduling state restoration for the streaming operators\nc.  Saving state to Zookeeper    Supporting  Dynamic\n    Partitioning : \u00a0Periodically\n    evaluating the SLA and modifying the physical plan if required\n    (logical plan does not change).   Enabling  Security : \u00a0Distributing\n    security tokens for distributed components of the execution engine\n    and securing web service requests.  Enabling  Dynamic  modification \u00a0 of\n    DAG: In the future, we intend to allow for user initiated\n    modification of the logical plan to allow for changes to the\n    processing logic and functionality.   An example of the Yahoo! Finance Quote application scheduled on a\ncluster of 5 Hadoop containers (processes) is shown in Figure 3.   An example for the translation from a logical plan to a physical\nplan and an execution plan for a subset of the application is shown in\nFigure 4.", 
            "title": "Streaming Application Manager (STRAM)"
        }, 
        {
            "location": "/application_development/#hadoop-components", 
            "text": "In this section we cover some aspects of Hadoop that your\nstreaming application interacts with. This section is not meant to\neducate the reader on Hadoop, but just get the reader acquainted with\nthe terms. We strongly advise readers to learn Hadoop from other\nsources.  A streaming application runs as a native Hadoop 2.2 application.\nHadoop 2.2 does not differentiate between a map-reduce job and other\napplications, and hence as far as Hadoop is concerned, the streaming\napplication is just another job. This means that your application\nleverages all the bells and whistles Hadoop provides and is fully\nsupported within Hadoop technology stack. The platform is responsible\nfor properly integrating itself with the relevant components of Hadoop\nthat exist today and those that may emerge in the future  All investments that leverage multi-tenancy (for example quotas\nand queues), security (for example kerberos), data flow integration (for\nexample copying data in-out of HDFS), monitoring, metrics collections,\netc. will require no changes when streaming applications run on\nHadoop.  YARN  YARN is\nthe core library of Hadoop 2.2 that is tasked with resource management\nand works as a distributed application framework. In this section we\nwill walk through Yarn's components. In Hadoop 2.2, the old jobTracker\nhas been replaced by a combination of ResourceManager (RM) and\nApplicationMaster (AM).  Resource Manager (RM)  ResourceManager (RM)\nmanages all the distributed resources. It allocates and arbitrates all\nthe slots and the resources (cpu, memory, network) of these slots. It\nworks with per-node NodeManagers (NMs) and per-application\nApplicationMasters (AMs). Currently memory usage is monitored by RM; in\nupcoming releases it will have CPU as well as network management. RM is\nshared by map-reduce and streaming applications. Running streaming\napplications requires no changes in the RM.  Application Master (AM)  The AM is the watchdog or monitoring process for your application\nand has the responsibility of negotiating resources with RM and\ninteracting with NodeManagers to get the allocated containers started.\nThe AM is the starting point of your application and is considered user\ncode (not system Hadoop code). The AM itself runs in one container. All\nresource management within the application are managed by the AM. This\nis a critical feature for Hadoop 2.2 where tasks done by jobTracker in\nHadoop 1.0 have been distributed allowing Hadoop 2.2 to scale much\nbeyond Hadoop 1.0. STRAM is a native YARN ApplicationManager.  Node Managers (NM)  There is one  NodeManager (NM)\nper node in the cluster. All the containers (i.e. processes) on that\nnode are monitored by the NM. It takes instructions from RM and manages\nresources of that node as per RM instructions. NMs interactions are same\nfor map-reduce and for streaming applications. Running streaming\napplications requires no changes in the NM.  RPC Protocol  Communication among RM, AM, and NM is done via the Hadoop RPC\nprotocol. Streaming applications use the same protocol to send their\ndata. No changes are needed in RPC support provided by Hadoop to enable\ncommunication done by components of your application.  HDFS  Hadoop includes a highly fault tolerant, high throughput\ndistributed file system ( HDFS ).\nIt runs on commodity hardware, and your streaming application will, by\ndefault, use it. There is no difference between files created by a\nstreaming application and those created by map-reduce.", 
            "title": "Hadoop Components"
        }, 
        {
            "location": "/application_development/#developing-an-application", 
            "text": "In this chapter we describe the methodology to develop an\napplication using the Realtime Streaming Platform. The platform was\ndesigned to make it easy to build and launch sophisticated streaming\napplications with the developer having to deal only with the\napplication/business logic. The platform deals with details of where to\nrun what operators on which servers and how to correctly route streams\nof data among them.", 
            "title": "Developing An Application"
        }, 
        {
            "location": "/application_development/#development-process", 
            "text": "While the platform does not mandate a specific methodology or set\nof development tools, we have recommendations to maximize productivity\nfor the different phases of application development.  Design   Identify common, reusable operators. Use a library\n    if possible.  Identify scalability and performance requirements before\n    designing the DAG.  Leverage attributes that the platform supports for scalability\n    and performance.  Use operators that are benchmarked and tested so that later\n    surprises are minimized. If you have glue code, create appropriate\n    unit tests for it.  Use THREAD_LOCAL locality for high throughput streams. If all\n    the operators on that stream cannot fit in one container,\n    try\u00a0NODE_LOCAL\u00a0locality. Both THREAD_LOCAL and\n    NODE_LOCAL streams avoid the Network Interface Card (NIC)\n    completly. The former uses intra-process communication to also avoid\n    serialization-deserialization overhead.  The overall throughput and latencies are are not necessarily\n    correlated to the number of operators in a simple way -- the\n    relationship is more nuanced. A lot depends on how much work\n    individual operators are doing, how many are able to operate in\n    parallel, and how much data is flowing through the arcs of the DAG.\n    It is, at times, better to break a computation down into its\n    constituent simple parts and then stitch them together via streams\n    to better utilize the compute resources of the cluster. Decide on a\n    per application basis the fine line between complexity of each\n    operator vs too many streams. Doing multiple computations in one\n    operator does save network I/O, while operators that are too complex\n    are hard to maintain.  Do not use operators that depend on the order of two streams\n    as far as possible. In such cases behavior is not idempotent.  Persist key information to HDFS if possible; it may be useful\n    for debugging later.  Decide on an appropriate fault tolerance mechanism. If some\n    data loss is acceptable, use the at-most-once mechanism as it has\n    fastest recovery.   Creating New Project  Please refer to the  Apex Application Packages \u00a0for\nthe basic steps for creating a new project.  Writing the application code  Preferably use an IDE (Eclipse, Netbeans etc.) that allows you to\nmanage dependencies and assists with the Java coding. Specific benefits\ninclude ease of managing operator library jar files, individual operator\nclasses, ports and properties. It will also highlight and assist to\nrectify issues such as type mismatches when adding streams while\ntyping.  Testing  Write test cases with JUnit or similar test framework so that code\nis tested as it is written. For such testing, the DAG can run in local\nmode within the IDE. Doing this may involve writing mock input or output\noperators for the integration points with external systems. For example,\ninstead of reading from a live data stream, the application in test mode\ncan read from and write to files. This can be done with a single\napplication DAG by instrumenting a test mode using settings in the\nconfiguration that is passed to the application factory\ninterface.  Good test coverage will not only eliminate basic validation errors\nsuch as missing port connections or property constraint violations, but\nalso validate the correct processing of the data. The same tests can be\nre-run whenever the application or its dependencies change (operator\nlibraries, version of the platform etc.)  Running an application  The platform provides a commandline tool called dtcli\u00a0for managing applications (launching,\nkilling, viewing, etc.). This tool was already discussed above briefly\nin the section entitled Running the Test Application. It will introspect\nthe jar file specified with the launch command for applications (classes\nthat implement ApplicationFactory) or property files that define\napplications. It will also deploy the dependency jar files from the\napplication package to the cluster.  Dtcli can run the application in local mode (i.e. outside a\ncluster). It is recommended to first run the application in local mode\nin the development environment before launching on the Hadoop cluster.\nThis way some of the external system integration and correct\nfunctionality of the application can be verified in an easier to debug\nenvironment before testing distributed mode.  For more details on CLI please refer to the  dtCli Guide .", 
            "title": "Development Process"
        }, 
        {
            "location": "/application_development/#application-api", 
            "text": "This section introduces the API to write a streaming application.\nThe work involves connecting operators via streams to form the logical\nDAG. The steps are    Instantiate an application (DAG)    (Optional) Set Attributes   Assign application name  Set any other attributes as per application requirements     Create/re-use and instantiate operators   Assign operator name that is unique within the  application  Declare schema upfront for each operator (and thereby its   ports )  (Optional) Set  properties \u00a0  and  attributes \u00a0  on the dag as per specification  Connect ports of operators via streams  Each stream connects one output port of an operator to one or  more input ports of other operators.  (Optional) Set attributes on the streams       Test the application.    There are two methods to create an application, namely Java, and\nProperties file. Java API is for applications being developed by humans,\nand properties file (Hadoop like) is more suited for DAGs generated by\ntools.  Java API  The Java API is the most common way to create a streaming\napplication. It is meant for application developers who prefer to\nleverage the features of Java, and the ease of use and enhanced\nproductivity provided by IDEs like NetBeans or Eclipse. Using Java to\nspecify the application provides extra validation abilities of Java\ncompiler, such as compile time checks for type safety at the time of\nwriting the code. Later in this chapter you can read more about\nvalidation support in the platform.  The developer specifies the streaming application by implementing\nthe ApplicationFactory interface, which is how platform tools (CLI etc.)\nrecognize and instantiate applications. Here we show how to create a\nYahoo! Finance application that streams the last trade price of a ticker\nand computes the high and low price in every 1 min window. Run above\n test application\u00a0to execute the\nDAG in local mode within the IDE.  Let us revisit how the Yahoo! Finance test application constructs the DAG:  public class Application implements StreamingApplication\n{\n\n  ...\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    dag.getAttributes().attr(DAG.STRAM_WINDOW_SIZE_MILLIS).set(streamingWindowSizeMilliSeconds);\n\n    StockTickInput tick = getStockTickInputOperator( StockTickInput , dag);\n    SumKeyVal String, Long  dailyVolume = getDailyVolumeOperator( DailyVolume , dag);\n    ConsolidatorKeyVal String,Double,Long,String,?,?  quoteOperator = getQuoteOperator( Quote , dag);\n\n    RangeKeyVal String, Double  highlow = getHighLowOperator( HighLow , dag, appWindowCountMinute);\n    SumKeyVal String, Long  minuteVolume = getMinuteVolumeOperator( MinuteVolume , dag, appWindowCountMinute);\n    ConsolidatorKeyVal String,HighLow,Long,?,?,?  chartOperator = getChartOperator( Chart , dag);\n\n    SimpleMovingAverage String, Double  priceSMA = getPriceSimpleMovingAverageOperator( PriceSMA , dag, appWindowCountSMA);\n\n    dag.addStream( price , tick.price, quoteOperator.in1, highlow.data, priceSMA.data);\n    dag.addStream( vol , tick.volume, dailyVolume.data, minuteVolume.data);\n    dag.addStream( time , tick.time, quoteOperator.in3);\n    dag.addStream( daily_vol , dailyVolume.sum, quoteOperator.in2);\n\n    dag.addStream( quote_data , quoteOperator.out, getConsole( quoteConsole , dag,  QUOTE ));\n\n    dag.addStream( high_low , highlow.range, chartOperator.in1);\n    dag.addStream( vol_1min , minuteVolume.sum, chartOperator.in2);\n    dag.addStream( chart_data , chartOperator.out, getConsole( chartConsole , dag,  CHART ));\n\n    dag.addStream( sma_price , priceSMA.doubleSMA, getConsole( priceSMAConsole , dag,  Price SMA ));\n\n    return dag;\n  }\n}  Property File API  The platform also supports specification of a DAG via a property\nfile. The aim here to make it easy for tools to create and run an\napplication. This method of specification does not have the Java\ncompiler support of compile time check, but since these applications\nwould be created by software, they should be correct by construction.\nThe syntax is derived from Hadoop properties and should be easy for\nfolks who are used to creating software that integrated with\nHadoop.  Create an application (DAG): myApplication.properties  # input operator that reads from a file\ndt.operator.inputOp.classname=com.acme.SampleInputOperator\ndt.operator.inputOp.fileName=somefile.txt\n\n# output operator that writes to the console\ndt.operator.outputOp.classname=com.acme.ConsoleOutputOperator\n\n# stream connecting both operators\ndt.stream.inputStream.source=inputOp.outputPort\ndt.stream.inputStream.sinks=outputOp.inputPort  Above snippet is intended to convey the basic idea of specifying\nthe DAG without using Java. Operators would come from a predefined\nlibrary and referenced in the specification by class name and port names\n(obtained from the library providers documentation or runtime\nintrospection by tools). For those interested in details, see later\nsections and refer to the  Operation and\nInstallation Guide\u00a0mentioned above.  Attributes  Attributes impact the runtime behavior of the application. They do\nnot impact the functionality. An example of an attribute is application\nname. Setting it changes the application name. Another example is\nstreaming window size. Setting it changes the streaming window size from\nthe default value to the specified value. Users cannot add new\nattributes, they can only choose from the ones that come packaged and\npre-supported by the platform. Details of attributes are covered in the\n Operation and Installation\nGuide.", 
            "title": "Application API"
        }, 
        {
            "location": "/application_development/#operators", 
            "text": "Operators\u00a0are basic compute units.\nOperators process each incoming tuple and emit zero or more tuples on\noutput ports as per the business logic. The data flow, connectivity,\nfault tolerance (node outage), etc. is taken care of by the platform. As\nan operator developer, all that is needed is to figure out what to do\nwith the incoming tuple and when (and which output port) to send out a\nparticular output tuple. Correctly designed operators will most likely\nget reused. Operator design needs care and foresight. For details, refer\nto the   Operator Developer Guide . As an application developer you need to connect operators\nin a way that it implements your business logic. You may also require\noperator customization for functionality and use attributes for\nperformance/scalability etc.  All operators process tuples asynchronously in a distributed\ncluster. An operator cannot assume or predict the exact time a tuple\nthat it emitted will get consumed by a downstream operator. An operator\nalso cannot predict the exact time when a tuple arrives from an upstream\noperator. The only guarantee is that the upstream operators are\nprocessing the current or a future window, i.e. the windowId of upstream\noperator is equals or exceeds its own windowId. Conversely the windowId\nof a downstream operator is less than or equals its own windowId. The\nend of a window operation, i.e. the API call to endWindow on an operator\nrequires that all upstream operators have finished processing this\nwindow. This means that completion of processing a window propagates in\na blocking fashion through an operator. Later sections provides more\ndetails on streams and data flow of tuples.  Each operator has a unique name within the DAG as provided by the\nuser. This is the name of the operator in the logical plan. The name of\nthe operator in the physical plan is an integer assigned to it by STRAM.\nThese integers are use the sequence from 1 to N, where N is total number\nof physically unique operators in the DAG. \u00a0Following the same rule,\neach partitioned instance of a logical operator has its own integer as\nan id. This id along with the Hadoop container name uniquely identifies\nthe operator in the execution plan of the DAG. The logical names and the\nphysical names are required for web service support. Operators can be\naccessed via both names. These same names are used while interacting\nwith  dtcli\u00a0to access an operator.\nIdeally these names should be self-descriptive. For example in Figure 1,\nthe node named \u201cDaily volume\u201d has a physical identifier of 2.  Operator Interface  Operator interface in a DAG consists of  ports , \u00a0 properties , \u00a0and\n  attributes  . \u00a0Operators interact with other\ncomponents of the DAG via ports. Functional behavior of the operators\ncan be customized via parameters. Run time performance and physical\ninstantiation is controlled by attributes. Ports and parameters are\nfields (variables) of the Operator class/object, while attributes are\nmeta information that is attached to the operator object via an\nAttributeMap. An operator must have at least one port. Properties are\noptional. Attributes are provided by the platform and always have a\ndefault value that enables normal functioning of operators.  Ports  Ports are connection points by which an operator receives and\nemits tuples. These should be transient objects instantiated in the\noperator object, that implement particular interfaces. Ports should be\ntransient as they contain no state. They have a pre-defined schema and\ncan only be connected to other ports with the same schema. An input port\nneeds to implement the interface  Operator.InputPort\u00a0and\ninterface Sink. A default\nimplementation of these is provided by the abstract class DefaultInputPort. An output port needs to\nimplement the interface  Operator.OutputPort. A default implementation\nof this is provided by the concrete class DefaultOutputPort. These two are a quick way to\nimplement the above interfaces, but operator developers have the option\nof providing their own implementations.  Here are examples of an input and an output port from the operator\nSum.  @InputPortFieldAnnotation(name =  data )\npublic final transient DefaultInputPort V  data = new DefaultInputPort V () {\n  @Override\n  public void process(V tuple)\n  {\n    ...\n  }\n}\n@OutputPortFieldAnnotation(optional=true)\npublic final transient DefaultOutputPort V  sum = new DefaultOutputPort V (){ \u2026 };  The process call is in the Sink interface. An emit on an output\nport is done via emit(tuple) call. For the above example it would be\nsum.emit(t), where the type of t is the generic parameter V.  There is no limit on how many ports an operator can have. However\nany operator must have at least one port. An operator with only one port\nis called an Input Adapter if it has no input port and an Output Adapter\nif it has no output port. These are special operators needed to get/read\ndata from outside system/source into the application, or push/write data\ninto an outside system/sink. These could be in Hadoop or outside of\nHadoop. These two operators are in essence gateways for the streaming\napplication to communicate with systems outside the application.  Port connectivity can be validated during compile time by adding\nPortFieldAnnotations shown above. By default all ports have to be\nconnected, to allow a port to go unconnected, you need to add\n\u201coptional=true\u201d to the annotation.  Attributes can be specified for ports that affect the runtime\nbehavior. An example of an attribute is parallel partition that specifes\na parallel computation flow per partition. It is described in detail in\nthe  Parallel\nPartitions \u00a0 section.\nAnother example is queue capacity that specifies the buffer size for the\nport. Details of attributes are covered in  Operation and Installation Guide.  Properties  Properties are the abstractions by which functional behavior of an\noperator can be customized. They should be non-transient objects\ninstantiated in the operator object. They need to be non-transient since\nthey are part of the operator state and re-construction of the operator\nobject from its checkpointed state must restore the operator to the\ndesired state. Properties are optional, i.e. an operator may or may not\nhave properties; they are part of user code and their values are not\ninterpreted by the platform in any way.  All non-serializable objects should be declared transient.\nExamples include sockets, session information, etc. These objects should\nbe initialized during setup call, which is called every time the\noperator is initialized.  Attributes  Attributes are values assigned to the operators that impact\nrun-time. This includes things like the number of partitions, at most\nonce or at least once or exactly once recovery modes, etc. Attributes do\nnot impact functionality of the operator. Users can change certain\nattributes in runtime. Users cannot add attributes to operators; they\nare pre-defined by the platform. They are interpreted by the platform\nand thus cannot be defined in user created code (like properties).\nDetails of attributes are covered in   Configuration Guide .  Operator State  The state of an operator is defined as the data that it transfers\nfrom one window to a future window. Since the computing model of the\nplatform is to treat windows like micro-batches, the operator state can\nbe  checkpointed \u00a0 every\nNth window, or every T units of time, where T is significantly greater\nthan the streaming window. \u00a0When an operator is checkpointed, the entire\nobject is written to HDFS. \u00a0The larger the amount of state in an\noperator, the longer it takes to recover from a failure. A stateless\noperator can recover much quicker than a stateful one. The needed\nwindows are preserved by the upstream buffer server and are used to\nrecompute the lost windows, and also rebuild the buffer server in the\ncurrent container.  The distinction between Stateless and Stateful is based solely on\nthe need to transfer data in the operator from one window to the next.\nThe state of an operator is independent of the number of ports.  Stateless  A Stateless operator is defined as one where no data is needed to\nbe kept at the end of every window. This means that all the computations\nof a window can be derived from all the tuples the operator receives\nwithin that window. This guarantees that the output of any window can be\nreconstructed by simply replaying the tuples that arrived in that\nwindow. Stateless operators are more efficient in terms of fault\ntolerance, and cost to achieve SLA.  Stateful  A Stateful operator is defined as one where data is needed to be\nstored at the end of a window for computations occurring in later\nwindow; a common example is the computation of a sum of values in the\ninput tuples.  Operator API  The Operator API consists of methods that operator developers may\nneed to override. In this section we will discuss the Operator APIs from\nthe point of view of an application developer. Knowledge of how an\noperator works internally is critical for writing an application. Those\ninterested in the details should refer to  Malhar Operator Developer Guide.  The APIs are available in three modes, namely Single Streaming\nWindow, Sliding Application Window, and Aggregate Application Window.\nThese are not mutually exclusive, i.e. an operator can use single\nstreaming window as well as sliding application window. A physical\ninstance of an operator is always processing tuples from a single\nwindow. The processing of tuples is guaranteed to be sequential, no\nmatter which input port the tuples arrive on.  In the later part of this section we will evaluate three common\nuses of streaming windows by applications. They have different\ncharacteristics and implications on optimization and recovery mechanisms\n(i.e. algorithm used to recover a node after outage) as discussed later\nin the section.  Streaming Window  Streaming window is atomic micro-batch computation period. The API\nmethods relating to a streaming window are as follows  public void process( tuple_type  tuple) // Called on the input port on which the tuple arrives\npublic void beginWindow(long windowId) // Called at the start of the window as soon as the first begin_window tuple arrives\npublic void endWindow() // Called at the end of the window after end_window tuples arrive on all input ports\npublic void setup(OperatorContext context) // Called once during initialization of the operator\npublic void teardown() // Called once when the operator is being shutdown  A tuple can be emitted in any of the three streaming run-time\ncalls, namely beginWindow, process, and endWindow but not in setup or\nteardown.  Aggregate Application Window  An operator with an aggregate window is stateful within the\napplication window timeframe and possibly stateless at the end of that\napplication window. An size of an aggregate application window is an\noperator attribute and is defined as a multiple of the streaming window\nsize. The platform recognizes this attribute and optimizes the operator.\nThe beginWindow, and endWindow calls are not invoked for those streaming\nwindows that do not align with the application window. For example in\ncase of streaming window of 0.5 second and application window of 5\nminute, an application window spans 600 streaming windows (5*60*2 =\n600). At the start of the sequence of these 600 atomic streaming\nwindows, a beginWindow gets invoked, and at the end of these 600\nstreaming windows an endWindow gets invoked. All the intermediate\nstreaming windows do not invoke beginWindow or endWindow. Bookkeeping,\nnode recovery, stats, UI, etc. continue to work off streaming windows.\nFor example if operators are being checkpointed say on an average every\n30th window, then the above application window would have about 20\ncheckpoints.  Sliding Application Window  A sliding window is computations that requires previous N\nstreaming windows. After each streaming window the Nth past window is\ndropped and the new window is added to the computation. An operator with\nsliding window is a stateful operator at end of any window. The sliding\nwindow period is an attribute and is a multiple of streaming window. The\nplatform recognizes this attribute and leverages it during bookkeeping.\nA sliding aggregate window with tolerance to data loss does not have a\nvery high bookkeeping cost. The cost of all three recovery mechanisms,\n at most once\u00a0(data loss tolerant),\nat least once\u00a0(data loss\nintolerant), and exactly once\u00a0(data\nloss intolerant and no extra computations) is same as recovery\nmechanisms based on streaming window. STRAM is not able to leverage this\noperator for any extra optimization.  Single vs Multi-Input Operator  A single-input operator by definition has a single upstream\noperator, since there can only be one writing port for a stream. \u00a0If an\noperator has a single upstream operator, then the beginWindow on the\nupstream also blocks the beginWindow of the single-input operator. For\nan operator to start processing any window at least one upstream\noperator has to start processing that window. A multi-input operator\nreads from more than one upstream ports. Such an operator would start\nprocessing as soon as the first begin_window event arrives. However the\nwindow would not close (i.e. invoke endWindow) till all ports receive\nend_window events for that windowId. Thus the end of a window is a\nblocking event. As we saw earlier, a multi-input operator is also the\npoint in the DAG where windows of all upstream operators are\nsynchronized. The windows (atomic micro-batches) from a faster (or just\nahead in processing) upstream operators are queued up till the slower\nupstream operator catches up. STRAM monitors such bottlenecks and takes\ncorrective actions. The platform ensures minimal delay, i.e processing\nstarts as long as at least one upstream operator has started\nprocessing.  Recovery Mechanisms  Application developers can set any of the recovery mechanisms\nbelow to deal with node outage. In general, the cost of recovery depends\non the state of the operator, while data integrity is dependant on the\napplication. The mechanisms are per window as the platform treats\nwindows as atomic compute units. Three recovery mechanisms are\nsupported, namely   At-least-once: All atomic batches are processed at least once.\n    No data loss occurs.  At-most-once: All atomic batches are processed at most once.\n    Data loss is possible; this is the most efficient setting.  Exactly-once: All atomic batches are processed exactly once.\n    No data loss occurs; this is the least efficient setting since\n    additional work is needed to ensure proper semantics.   At-least-once is the default. During a recovery event, the\noperator connects to the upstream buffer server and asks for windows to\nbe replayed. At-least-once and exactly-once mechanisms start from its\ncheckpointed state. At-most-once starts from the next begin-window\nevent.  Recovery mechanisms can be specified per Operator while writing\nthe application as shown below.  Operator o = dag.addOperator(\u201coperator\u201d, \u2026);\ndag.setAttribute(o,  OperatorContext.PROCESSING_MODE,  ProcessingMode.AT_MOST_ONCE);  Also note that once an operator is attributed to AT_MOST_ONCE,\nall the operators downstream to it have to be AT_MOST_ONCE. The client\nwill give appropriate warnings or errors if that\u2019s not the case.  Details are explained in the chapter on Fault Tolerance\nbelow .", 
            "title": "Operators"
        }, 
        {
            "location": "/application_development/#streams", 
            "text": "A stream\u00a0is a connector\n(edge) abstraction, and is a fundamental building block of the platform.\nA stream consists of tuples that flow from one port (called the\noutput\u00a0port) to one or more ports\non other operators (called  input\u00a0ports) another -- so note a potentially\nconfusing aspect of this terminology: tuples enter a stream through its\noutput port and leave via one or more input ports. A stream has the\nfollowing characteristics   Tuples are always delivered in the same order in which they\n    were emitted.  Consists of a sequence of windows one after another. Each\n    window being a collection of in-order tuples.  A stream that connects two containers passes through a\n    buffer server.  All streams can be persisted (by default in HDFS).  Exactly one output port writes to the stream.  Can be read by one or more input ports.  Connects operators within an application, not outside\n    an application.  Has an unique name within an application.  Has attributes which act as hints to STRAM.   Streams have four modes, namely in-line, in-node, in-rack,\n    and other. Modes may be overruled (for example due to lack\n    of containers). They are defined as follows:   THREAD_LOCAL: In the same thread, uses thread\n    stack (intra-thread). This mode can only be used for a downstream\n    operator which has only one input port connected; also called\n    in-line.  CONTAINER_LOCAL: In the same container (intra-process); also\n    called in-container.  NODE_LOCAL: In the same Hadoop node (inter processes, skips\n    NIC); also called in-node.  RACK_LOCAL: On nodes in the same rack; also called\n    in-rack.  unspecified: No guarantee. Could be anywhere within the\n    cluster     An example of a stream declaration is given below  DAG dag = new DAG();\n \u2026\ndag.addStream( views , viewAggregate.sum, cost.data).setLocality(CONTAINER_LOCAL); // A container local  stream\ndag.addStream(\u201cclicks\u201d, clickAggregate.sum, rev.data); // An example of unspecified locality  The platform guarantees in-order delivery of tuples in a stream.\nSTRAM views each stream as collection of ordered windows. Since no tuple\ncan exist outside a window, a replay of a stream consists of replay of a\nset of windows. When multiple input ports read the same stream, the\nexecution plan of a stream ensures that each input port is logically not\nblocked by the reading of another input port. The schema of a stream is\nsame as the schema of the tuple.  In a stream all tuples emitted by an operator in a window belong\nto that window. A replay of this window would consists of an in-order\nreplay of all the tuples. Thus the tuple order within a stream is\nguaranteed. However since an operator may receive multiple streams (for\nexample an operator with two input ports), the order of arrival of two\ntuples belonging to different streams is not guaranteed. In general in\nan asynchronous distributed architecture this is expected. Thus the\noperator (specially one with multiple input ports) should not depend on\nthe tuple order from two streams. One way to cope with this\nindeterminate order, if necessary, is to wait to get all the tuples of a\nwindow and emit results in endWindow call. All operator templates\nprovided as part of   standard operator template\nlibrary   \u00a0 follow\nthese principles.  A logical stream gets partitioned into physical streams each\nconnecting the partition to the upstream operator. If two different\nattributes are needed on the same stream, it should be split using\nStreamDuplicator\u00a0operator.  Modes of the streams are critical for performance. An in-line\nstream is the most optimal as it simply delivers the tuple as-is without\nserialization-deserialization. Streams should be marked\ncontainer_local, specially in case where there is a large tuple volume\nbetween two operators which then on drops significantly. Since the\nsetLocality call merely provides a hint, STRAM may ignore it. An In-node\nstream is not as efficient as an in-line one, but it is clearly better\nthan going off-node since it still avoids the potential bottleneck of\nthe network card.  THREAD_LOCAL and CONTAINER_LOCAL streams do not use a buffer\nserver as this stream is in a single process. The other two do.", 
            "title": "Streams"
        }, 
        {
            "location": "/application_development/#validating-an-application", 
            "text": "The platform provides various ways of validating the application\nspecification and data input. An understanding of these checks is very\nimportant for an application developer since it affects productivity.\nValidation of an application is done in three phases, namely   Compile Time: Caught during application development, and is\n    most cost effective. These checks are mainly done on declarative\n    objects and leverages the Java compiler. An example is checking that\n    the schemas specified on all ports of a stream are\n    mutually compatible.  Initialization Time: When the application is being\n    initialized, before submitting to Hadoop. These checks are related\n    to configuration/context of an application, and are done by the\n    logical DAG builder implementation. An example is the checking that\n    all non-optional ports are connected to other ports.  Run Time: Validations done when the application is running.\n    This is the costliest of all checks. These are checks that can only\n    be done at runtime as they involve data. For example divide by 0\n    check as part of business logic.   Compile Time  Compile time validations apply when an application is specified in\nJava code and include all checks that can be done by Java compiler in\nthe development environment (including IDEs like NetBeans or Eclipse).\nExamples include   Schema Validation: The tuples on ports are POJO (plain old\n    java objects) and compiler checks to ensure that all the ports on a\n    stream have the same schema.  Stream Check: Single Output Port and at least one Input port\n    per stream. A stream can only have one output port writer. This is\n    part of the addStream api. This\n    check ensures that developers only connect one output port to\n    a stream. The same signature also ensures that there is at least one\n    input port for a stream  Naming: Compile time checks ensures that applications\n    components operators, streams are named   Initialization/Instantiation Time  Initialization time validations include various checks that are\ndone post compile, and before the application starts running in a\ncluster (or local mode). These are mainly configuration/contextual in\nnature. These checks are as critical to proper functionality of the\napplication as the compile time validations.  Examples include    JavaBeans Validation :\n    Examples include   @Max(): Value must be less than or equal to the number  @Min(): Value must be greater than or equal to the\n    number  @NotNull: The value of the field or property must not be\n    null  @Pattern(regexp = \u201c....\u201d): Value must match the regular\n    expression  Input port connectivity: By default, every non-optional input\n    port must be connected. A port can be declared optional by using an\n    annotation: \u00a0 \u00a0 @InputPortFieldAnnotation(name = \"...\", optional\n    = true)  Output Port Connectivity: Similar. The annotation here is: \u00a0 \u00a0\n    @OutputPortFieldAnnotation(name = \"...\", optional = true)     Unique names in application scope: Operators, streams, must have\n    unique names.   Cycles in the dag: DAG cannot have a cycle.  Unique names in operator scope: Ports, properties, annotations\n    must have unique names.  One stream per port: A port can connect to only one stream.\n    This check applies to input as well as output ports even though an\n    output port can technically write to two streams. If you must have\n    two streams originating from a single output port, use \u00a0a\u00a0streamDuplicator operator.  Application Window Period: Has to be an integral multiple the\n    streaming window period.   Run Time  Run time checks are those that are done when the application is\nrunning. The real-time streaming platform provides rich run time error\nhandling mechanisms. The checks are exclusively done by the application\nbusiness logic, but the platform allows applications to count and audit\nthese. Some of these features are in the process of development (backend\nand UI) and this section will be updated as they are developed. Upon\ncompletion examples will be added to   demos   t o\nillustrate these.  Error ports are output ports with error annotations. Since they\nare normal ports, they can be monitored and tuples counted, persisted\nand counts shown in the UI.", 
            "title": "Validating an Application"
        }, 
        {
            "location": "/application_development/#multi-tenancy-and-security", 
            "text": "Hadoop is a multi-tenant distributed operating system. Security is\nan intrinsic element of multi-tenancy as without it a cluster cannot be\nreasonably be shared among enterprise applications. Streaming\napplications follow all multi-tenancy security models used in Hadoop as\nthey are native Hadoop applications. For details refer to the Operation and Installation\nGuide \n.", 
            "title": "Multi-Tenancy and Security"
        }, 
        {
            "location": "/application_development/#security", 
            "text": "The platform includes Kerberos support. Both access points, namely\nSTRAM and Bufferserver are secure. STRAM passes the token over to\nStreamingContainer, which then gives it to the Bufferserver. The most\nimportant aspect for an application developer is to note that STRAM is\nthe single point of access to ensure security measures are taken by all\ncomponents of the platform.", 
            "title": "Security"
        }, 
        {
            "location": "/application_development/#resource-limits", 
            "text": "Hadoop enforces quotas on resources. This includes hard-disk (name\nspace and total disk quota) as well as priority queues for schedulers.\nThe platform uses Hadoop resource limits to manage a streaming\napplication. In addition network I/O quotas can be enforced. An operator\ncan be dynamically partitioned if it reaches its resource limits; these\nlimits may be expressed in terms of throughput, latency, or just\naggregate resource utilization of a container.", 
            "title": "Resource Limits"
        }, 
        {
            "location": "/application_development/#scalability-and-partitioning", 
            "text": "Scalability is a foundational element of this platform and is a\nbuilding block for an eco-system where big-data meets real-time.\nEnterprises need to continually meet SLA as data grows. Without the\nability to scale as load grows, or new applications with higher loads\ncome to fruition, enterprise grade SLA cannot be met. A big issue with\nthe streaming application space is that, it is not just about high load,\nbut also the fluctuations in it. There is no way to guarantee future\nload requirements and there is a big difference between high and low\nload within a day for the same feed. Traditional streaming platforms\nsolve these two cases by simply throwing more hardware at the\nproblem.  Daily spikes are managed by ensuring enough hardware for peak\nload, which then idles during low load, and future needs are handled by\na very costly re-architecture, or investing heavily in building a\nscalable distributed operating system. Another salient and often\noverlooked cost is the need to manage SLA -- let\u2019s call it  buffer capacity. Since this means computing the\npeak load within required time, that translates to allocating enough\nresources over and above peak load as daily peaks fluctuate. For example\nan average peak load of 100 resource units (cpu and/or memory and/or\nnetwork) may mean allocating about 200 resource units to be safe. A\ndistributed cluster that cannot dynamically scale up and down, in effect\npays buffer capacity per application. Another big aspect of streaming\napplications is that the load is not just ingestion rate, more often\nthan not, the internal operators produce lot more events than the\ningestion rate. For example a dimensional data (with, say  d\u00a0dimensions) computation needs 2*d -1\u00a0computations per ingested event. A lot\nof applications have over 10 dimensions, i.e over 1000 computations per\nincoming event and these need to be distributed across the cluster,\nthereby causing an explosion in the throughput (events/sec) that needs\nto be managed.  The platform is designed to handle such cases at a very low cost.\nThe platform scales linearly with Hadoop. If applications need more\nresources, the enterprise can simply add more commodity nodes to Hadoop\nwithout any downtime, and the Hadoop native platform will take care of\nthe rest. If some nodes go bad, these can be removed without downtime.\nThe daily peaks and valleys in the load are managed by the platform by\ndynamically scaling at the peak and then giving the resources back to\nHadoop during low load. This means that a properly designed Hadoop\ncluster does several things for enterprises: (a) reduces the cost of\nhardware due to use of commodity hardware (b) shares buffer capacity\nacross all applications as peaks of all applications may not align and\n(c) raises the average CPU usage on a 24x7 basis. As a general design\nthis is similar to scale that a map-reduce application can deliver. In\nthe following sections of this chapter we will see how this is\ndone.", 
            "title": "Scalability and Partitioning"
        }, 
        {
            "location": "/application_development/#partitioning", 
            "text": "If all tuples sent through the stream(s) that are connected to the\ninput port(s) of an operator in the DAG are received by a single\nphysical instance of that operator, that operator can become a\nperformance bottleneck. This leads to scalability issues when\nthroughput, memory, or CPU needs exceed the processing capacity of that\nsingle instance.  To address the problem, the platform offers the capability to\npartition the inflow of data so that it is divided across multiple\nphysical instances of a logical operator in the DAG. There are two\nfunctional ways to partition   Load balance: Incoming load is simply partitioned\n    into stream(s) that go to separate instances of physical operators\n    and scalability is achieved via adding more physical operators. Each\n    tuple is sent to physical operator (partition) based on a\n    round-robin or other similar algorithm. This scheme scales linearly.\n    A lot of key based computations can load balance in the platform due\n    to the ability to insert  Unifiers. For many computations, the\n    endWindow and Unifier setup is similar to the combiner and reducer\n    mechanism in a Map-Reduce computation.  Sticky Key: The key assertion is that distribution of tuples\n    are sticky, i.e the data with\n    same key will always be processed by the same physical operator, no\n    matter how many times it is sent through the stream. This stickiness\n    will continue even if the number of partitions grows dynamically and\n    can eventually be leveraged for advanced features like\n    bucket testing. How this is accomplished and what is required to\n    develop compliant operators will be explained below.   We plan to add more partitioning mechanisms proactively to the\nplatform over time as needed by emerging usage patterns. The aim is to\nallow enterprises to be able to focus on their business logic, and\nsignificantly reduce the cost of operability. As an enabling technology\nfor managing high loads, this platform provides enterprises with a\nsignificant innovative edge. Scalability and Partitioning is a\nfoundational building block for this platform.  Sticky Partition vs Round Robin  As noted above, partitioning via sticky key is data aware but\nround-robin partitioning is not. An example for non-sticky load\nbalancing would be round robin distribution over multiple instances,\nwhere for example a tuple stream of  A, A,\nA with 3 physical operator\ninstances would result in processing of a single A by each of the instances, In contrast, sticky\npartitioning means that exactly one instance of the operators will\nprocess all of the  Atuples if they\nfall into the same bucket, while B\nmay be processed by another operator. Data aware mapping of\ntuples to partitions (similar to distributed hash table) is accomplished\nvia Stream Codecs. In later sections we would show how these two\napproaches can be used in combination.  Stream Codec  The platform does not make assumptions about the tuple\ntype, it could be any Java object. The operator developer knows what\ntuple type an input port expects and is capable of processing. Each\ninput port has a stream codec \u00a0associated thatdefines how data is serialized when transmitted over a socket\nstream; it also defines another\nfunction that computes the partition hash key for the tuple. The engine\nuses that key to determine which physical instance(s) \u00a0(for a\npartitioned operator) receive that \u00a0tuple. For this to work, consistent hashing is required.\nThe default codec uses the Java Object#hashCode function, which is\nsufficient for basic types such as Integer, String etc. It will also\nwork with custom tuple classes as long as they implement hashCode\nappropriately. Reliance on hashCode may not work when generic containers\nare used that do not hash the actual data, such as standard collection\nclasses (HashMap etc.), in which case a custom stream codec must be\nassigned to the input port.  Static Partitioning  DAG designers can specify at design time how they would like\ncertain operators to be partitioned. STRAM then instantiates the DAG\nwith the physical plan which adheres to the partitioning scheme defined\nby the design. This plan is the initial partition of the application. In\nother words, Static Partitioning is used to tell STRAM to compute the\nphysical DAG from a logical DAG once, without taking into consideration\nruntime states or loads of various operators.  Dynamic Partitioning  In streaming applications the load changes during the day, thus\ncreating situations where the number of partitioned operator instances\nneeds to adjust dynamically. The load can be measured in terms of\nprocessing within the DAG based on throughput, or latency, or\nconsiderations in external system components (time based etc.) that the\nplatform may not be aware of. Whatever the trigger, the resource\nrequirement for the current processing needs to be adjusted at run-time.\nThe platform may detect that operator instances are over or under\nutilized and may need to dynamically adjust the number of instances on\nthe fly. More instances of a logical operator may be required (partition\nsplit) or underutilized operator instances may need decommissioning\n(partition merge). We refer to either of the changes as dynamic\npartitioning. The default partitioning scheme supports split and merge\nof partitions, but without state transfer. The contract of the\nPartitioner\u00a0interface allows the operator\ndeveloper to implement split/merge and the associated state transfer, if\nnecessary.  Since partitioning is a key scalability measure, our goal is to\nmake it as simple as possible without removing the flexibility needed\nfor sophisticated applications. Basic partitioning can be enabled at\ncompile time through the DAG specification. A slightly involved\npartitioning involves writing custom codecs to calculate data aware\npartitioning scheme. More complex partitioning cases may require users\nto provide a custom implementation of Partitioner, which gives the\ndeveloper full control over state transfer between multiple instances of\nthe partitioned operator.  Default Partitioning  The platform provides a default partitioning implementation that\ncan be enabled without implementing Partitioner\u00a0(or writing any other extra Java\ncode), which is designed to support simple sticky partitioning out of\nthe box for operators with logic agnostic to the partitioning scheme\nthat can be enabled by means of DAG construction alone.  Typically an operator that can work with the default partitioning\nscheme would have a single input port. If there are multiple input\nports, only one port will be partitioned (the port first connected in\nthe DAG). The number of partitions will be calculated based on the\ninitial partition count - set as attribute on the operator in the DAG\n(if the attribute is not present, partitioning is off). Each partition\nwill handle tuples based on matching the lower bits of the hash code.\nFor example, if the tuple type was Integer and 2 partitions requested,\nall even numbers would go to one operator instance and all odd numbers\nto the other.  Default Dynamic Partitioning  Triggering partition load evaluation and repartitioning action\nitself are separate concerns. Triggers are not specified further here,\nwe are planning to support it in a customizable fashion that, for\nexample, allows latency or SLA based implementations. Triggers calculate\na load indicator (signed number) that tells the framework that a given\npartition is either underutilized, operating normally within the\nexpected thresholds or overloaded and becoming a bottleneck. The\nindicator is then presented to the partitioning logic (default or custom\nimplementation of Partitioner) to provide the opportunity to make any\nneeded adjustments.  The default partitioning logic divides the key space\naccording to the lower bits of the hash codes that are generated by the\nstream codec, by assigning each partitioned operator instance via a bit\nmask and the respective value. For example, the operator may have\ninitially two partitions,  0and 1, each\nwith a bit mask of 1.\nIn the case where load evaluation flags partition\n0  as over utilized\n(most data tuples processed yield a hash code with lowest bit cleared),\napartition split\u00a0occurs, resulting in 00\nand  10with mask 11. Operator instance 0 will be replaced with 2 new instances and partition\n1  remains unchanged,\nresulting in three active partitions. The same process could repeat if\nmost tuples fall into the01 partition, leading to a split into 001  and101\nwith mask 111, etc.  Should load decrease in two sibling partitions, a\npartition merge\u00a0could\nreverse the split, reducing the mask length and replacing two operators\nwith one. Should only one of two sibling partitions be underutilized,\n it cannot be\u00a0merged.\nInstead, the platform can attempt to deploy the affected operator\ninstance along with other operator instances for resource sharing\namongst underutilized partitions (not implemented yet). Keeping separate\noperator instances allows\u00a0us  to\npin load increases directly to the affected instance with a single\nspecific partition key, which would not be the case had we assigned a\nshared instance to handle multiple keys.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/application_development/#nxm-partitions", 
            "text": "When two consecutive logical operators are partitioned a special\noptimization is done. Technically the output of the first operator\nshould be unified and streamed to the next logical node. But that can\ncreate a network bottleneck. The platform optimizes this by partitioning\nthe output stream of each partition of the first operator as per the\npartitions needed by the next operator. For example if the first\noperator has N partitions and the second operator has M partitions then\neach of the N partitions would send out M streams. The first of each of\nthese M streams would be unified and routed to the first of the M\npartitions, and so on. Such an optimization allows for higher\nscalability and eliminates a network bottleneck (one unifier in between\nthe two operators) by having M unifiers. This also enables the\napplication to perform within the resource limits enforced by YARN.\nSTRAM has a much better understanding and estimation of unifier resource\nneeds and is thus able to optimize for resource constraints.  Figure 5 shows a case where we have a 3x2 partition; the single\nintermediate unifier between operator 1\u00a0and 2\u00a0is\noptimized away. The partition computation for operator  2\u00a0is executed on outbound streams of each\npartitions of operator 1. Each\npartition of operator 2\u00a0has its own\nCONTAINER_LOCAL unifier. In such a situation, the in-bound network\ntuple flow is split between containers for  2a\u00a0and 2b\u00a0each of which take half the traffic. STRAM\ndoes this by default since it always has better performance.", 
            "title": "NxM Partitions"
        }, 
        {
            "location": "/application_development/#parallel", 
            "text": "In cases where all the downstream operators use the same\npartitioning scheme and the DAG is network bound an optimization called\nparallel partition\u00a0is very\neffective. In such a scenario all the downstream operators are also\npartitioned to create computation flow per partition. This optimization\nis extremely efficient for network bound streams, In some cases this\noptimization would also apply for CPU or RAM bounded\napplications.  In Figure 6a, operator 1\u00a0is\npartitioned into 1a\u00a0and\n1b. Both the downstream operators\n2\u00a0and  3\u00a0follow the same partition scheme as\n1, however the network I/O between\n1\u00a0and 2, and between 2\u00a0and  3\u00a0is\nhigh. Then users can decide to optimize using parallel partitions. This\nallows STRAM to completely skip the insertion of intermediate Unifier\noperators between 1 and 2 as well as between 2 and 3; a single unifier\njust before operator  4, is\nadequate by which time tuple flow volume is low.  Since operator 4 has sufficient resources to manage the combined\noutput of multiple instances of operator 3, it need not be partitioned. A further\noptimization can be done by declaring operators  1, 2, and\n3\u00a0as THREAD_LOCAL (intra-thread)\nor CONTAINER_LOCAL (intra-process) or NODE_LOCAL (intra-node).\nParallel partition is not used by default, users have to specify it\nexplicitly via an attribute of the input port (reader) of the stream as\nshown below.   The following code shows an example of creating a parallel partition.  dag.addStream( DenormalizedUserId , idAssigner.userid, uniqUserCount.data);\ndag.setInputPortAttribute(uniqUserCount.data, PortContext.PARTITION_PARALLEL, partitionParallel);  Parallel partitions can be used with other partitions, for example\na parallel partition could be sticky key or load balanced.", 
            "title": "Parallel"
        }, 
        {
            "location": "/application_development/#parallel-partitions-with-streams-modes", 
            "text": "Parallel partitions can be further optimized if the parallel\npartitions are combined with streams being in-line or in-node or in-rack\nmode. This is very powerful feature and should be used if operators have\nvery high throughput within them and the outbound merge does an\naggregation. For example in Figure 6b, if operator 3 significantly\nreduces the throughput, which usually is a reason to do parallel\npartition, then making the streams in-line or in-node within nodes\n1- 2 and 2- 3 significantly impacts the performance.  CONTAINER_LOCAL stream has high bandwidth, and can manage to\nconsume massive tuple count without taxing the NIC and networking stack.\nThe downside is that all operators (1,2,3) in this case need to be able\nto fit within the resource limits of CPU and memory enforced on a Hadoop\ncontainer. A way around this is to request RM to provide a big\ncontainer. On a highly used Hadoop grid, getting a bigger container may\nbe a problem, and operational complexities of managing a Hadoop cluster\nwith different container sizes may be higher. If THREAD_LOCAL or\nCONTAINER_LOCAL streams are needed to get the throughput, increasing\nthe partition count should be considered. In future STRAM may take this\ndecision automatically. Unless there is a very bad skew and sticky key\npartitioning is in use, the approach to partition till each container\nhas enough resources works well.  A NODE_LOCAL stream has lower bandwidth compared to a\nCONTAINER_LOCAL stream, but it works well with the RM in terms of\nrespecting container size limits. A NODE_LOCAL parallel partition uses\nlocal loop back for streams and is much better than using NIC. Though\nNODE_LOCAL stream fits well with similar size containers, it does need\nRM to be able to deliver two containers on the same Hadoop node. On a\nheavily used Hadoop cluster, this may not always be possible. In future\nSTRAM would do these trade-offs automatically at run-time.  A RACK_LOCAL stream has much lower bandwidth than NODE_LOCAL\nstream, as events go through the NIC. But it still is able to better\nmanage SLA and latency. Moreover RM has much better ability to give a\nrack local container as opposed to the other two.  Parallel partitions with CONTAINER_LOCAL streams can be done by\nsetting all the intermediate streams to CONTAINER_LOCAL. Parallel\npartitions with THREAD_LOCAL streams can be done by setting all the\nintermediate streams to THREAD_LOCAL. Platform supports the following\nvia attributes.   Parallel-Partition  Parallel-Partition with THREAD_LOCAL stream  Parallel-Partition with CONTAINER_LOCAL stream  Parallel-Partition with NODE_LOCAL stream  Parallel-Partition with RACK_LOCAL stream   These attributes would nevertheless be initial starting point and\nSTRAM can improve on them at run time.", 
            "title": "Parallel Partitions with Streams Modes"
        }, 
        {
            "location": "/application_development/#skew-balancing-partition", 
            "text": "Skew balancing partition is useful to manage skews in the stream\nthat is load balanced using a sticky key. Incoming events may have a\nskew, and these may change depending on various factors like time of the\nday or other special circumstances. To manage the uneven load, users can\nset a limit on the ratio of maximum load on a partition to the minimum\nload on a partition. STRAM would use this to dynamically change the\npartitions. For example suppose there are 6 partitions, and the load\nhappens to be distributed as follows: one with 40%, and the rest with\n12% each. The ratio of maximum to minimum is 3.33. If the desired ratio\nis set to 2, STRAM would partition the first instance into two\npartitions, each with 20% load to bring the ratio down to the desired\nlevel. This will be tried repeatedly till partitions are balanced. The\ntime period between each attempt is controlled via an attribute to avoid\nrebalancing too frequently. As mentioned earlier, dynamic operations\ninclude both splitting a partition as well as merging partitions with\nlow load.  Figure 7 shows an example of skew balancing partition. An example\nof 3x1 paritition is shown. Let's say that skew balance is kept at \u201cno\npartition to take up more than 50% load. If in runtime the load type\nchanges to create a skew. For example, consider an application in the US\nthat is processing a website clickstream. At night in the US, the\nmajority of accesses come from the Far East, while in the daytime it\ncomes from the Americas. Similarly, in the early morning, the majority\nof the accesses are from east coast of the US, with the skew shifting to\nthe west coast as the day progresses. Assume operator 1 is partitioned\ninto 1a, 1b, and 1c.  Let's see what happens if the logical operator 1 gets into a 20%,\n20%, 60% skew as shown in Figure 7. This would trigger the skew\nbalancing partition. One example of attaining balance is to merge 1a,\nand 1b to get 1a+1b in a single partition to take the load to 40%; then\nsplit 1c into two partitions 1ca and 1cb to get 30% on each of them.\nThis way STRAM is able to get back to under 50% per partition. As a live\n24x7 application, this kind of skew partitioning can be applied several\ntimes in a day. Skew-balancing at runtime is a critical feature for SLA\ncompliance; it also enables cost savings. This partitioning scheme will\nbe available in later release.", 
            "title": "Skew Balancing Partition"
        }, 
        {
            "location": "/application_development/#skew-unifier-partition", 
            "text": "In this section we would take a look at another way to balance the\nskew. This method is a little less disruptive, but is useful in\naggregate operators. Let us take the same example as in Figure 7 with\nskew 20%, 20%, and 60%. To manage the load we could have either worked\non rebalancing the partition, which involves a merge and split of\npartitions to get to a new distribution or by partitioning  only\u00a0the partition with the big skew. Since the\nbest way to manage skew is to load balance, if possible, this scheme\nattempts to do so. The method is less useful than the others we discusse\n-- the main reason being that if the developer has chosen a sticky key\npartition to start with, it is unlikely that a load balancing scheme can\nhelp. Assuming that it is worthwhile to load balance, a special\none-purpose unifier can be inserted for the skew partition. If the cause\nof resource bottleneck is not the I/O, specially the I/O into the\ndownstream operator, but is the compute (memory, CPU) power of a\npartition, it makes sense to split the skew partition without having to\nchange the in-bound I/O to the upstream operator.  To trigger this users can set a limit on the ratio of maximum load\non a partition to the minimum load on a partition, and ask to use this\nscheme. STRAM would use this to load balance.The time period between\neach attempt is controlled via the same attribute to avoid rebalancing\ntoo frequently.  Figure 8 shows an example of skew load balancing partition with a\ndedicated unifier. The 20%, 20%, and 60% triggers the skew load\nbalancing partition with an unifier. Partition 1c would be split into\ntwo and it would get its own dedicated unifier. Ideally these two\nadditional partitions 1ca and 1cb will get 30% load. This way STRAM is\nable to get back to under 50% per partition. This scheme is very useful\nwhen the number of partitions is very high and we still have a bad\nskew.  In the steady state no physical partition is computing more than\n30% of the load. Memory and CPU resources are thus well distributed. The\nunifier that was inserted has to handle 60% of the load, distributed\nmore evenly, as opposed to the final unifier that had a 60% skew to\nmanage at a much higher total load. This partitioning scheme will be\navailable in later release.", 
            "title": "Skew Unifier Partition"
        }, 
        {
            "location": "/application_development/#cascading-unifier", 
            "text": "Let's take the case of an upstream operator oprU\u00a0that connects to a downstream operator\noprD. Let's assume the application\nis set to scale oprU by load balancing. So this could be either Nx1 or\nNxM partitioning scheme. The upstream operator oprU scales by increasing\nN. An increase in the load triggers more resource needs (CPU, Memory, or\nI/O), which in turn triggers more containers and raises N, the\ndownstream node may be impacted in a lot of situations. In this section\nwe review a method to shield oprD from dynamic changes in the execution\nplan of oprU. On aggregate operators (Sum, Count, Max, Min, Range \u2026) it\nis better to do load balanced partitioning to avoid impact of skew. This\nworks very well as each partition emits tuples at the order of number of\nkeys (range) in the incoming stream per application window. But as N\ngrows the in-bound I/O to the unifier of oprU that runs in the container\nof oprD goes up proportionately as each upstream partition sends tuples\nof the order of unique keys (range). This means that the partitioning\nwould not scale linearly. The platform has mechanisms to manage this and\nget the scale back to being linear.  Cascading unifiers are implemented by inserting a series of\nintermediate unifiers before the final unifier in the container of oprD.\nSince each unifier guarantees that the outbound I/O would be in order of\nthe number of unique keys, the unifier in the oprD container can expect\nto achieve an upper limit on the inbound I/O. The problem is the same\nirrespective of the value of M (1 or more), wherein the amount of\ninbound I/O is proportional to N, not M. Figure 8 illustrates how\ncascading unifier works.   Figure 8 shows an example where a 4x1 partition with single\nunifier is split into three 2x1 partitions to enable the final unifier\nin oprD container to get an upper limit on inbound I/O. This is useful\nto ensure that network I/O to containers is within limits, or within a\nlimit specified by users. The platform allows setting an upper limit of\nfan-in of the stream between oprU and oprD. Let's say that this is F (in\nthe figure F=2). STRAM would plan N/F (let's call it N1) containers,\neach with one unifier. The inbound fan-in to these unifiers is F. If N1  F, another level of unifiers would be inserted. Let's say at some\npoint N/(F1*F2*...Fk)   F, where K is the level of unifiers. The\noutbound I/O of each unifier is guaranteed to be under F, specially the\nunifier for oprD. This ensures that the application scales linearly as\nthe load grows. The downside is the additional latency imposed by each\nunifier level (a few milliseconds), but the SLA is maintained, and the\napplication is able to run within the resource limits imposed by YARN.\nThe value of F can be derived from any of the following   I/O limit on containers to allow proper behavior in an\n    multi-tenant environment  Load on oprD instance  Buffer server limits on fan-in, fan-out  Size of reservoir buffer for inbound fan-in   A more intriguing optimization comes when cascading unifiers are\ncombined with node-local execution plan, in which the bounds of two or\nmore containers are used and much higher local loopback limits are\nleveraged. In general the first level fan-in limit (F1) and the last\nstage fan-in limit (Fk) need not be same. In fact a much open and better\nleveraged execution plan may indeed have F1 != F2 != \u2026 != Fk, as Fk\ndetermines the fan-in for oprD, while F1, \u2026 Fk-1 are fan-ins for\nunifier-only containers. The platform will have these schemes in later\nversions.", 
            "title": "Cascading Unifier"
        }, 
        {
            "location": "/application_development/#sla", 
            "text": "A Service Level Agreement translates to guaranteeing that the\napplication would meet the requirements X% of the time. For example six\nsigma X is\u00a099.99966%. For\nreal-time streaming applications this translates to requirements for\nlatency, throughput, uptime, data loss etc. and that in turn indirectly\nleads to various resource requirements, recovery mechanisms, etc. The\nplatform is designed to handle these and features would be released in\nfuture as they get developed. At a top level, STRAM monitors throughput\nper operator, computes latency per operator, manages uptime and supports\nvarious recovery mechanisms to handle data loss. A lot of this decision\nmaking and algorithms will be customizable.", 
            "title": "SLA"
        }, 
        {
            "location": "/application_development/#fault-tolerance", 
            "text": "Fault tolerance in the platform is defined as the ability to\nrecognize the outage of any part of the application, get resources,\nre-initialize the failed operators, and re-compute the lost data. The\ndefault method is to bring the affected part of the DAG \u00a0back to a known\n(checkpointed) state and recompute atomic micro batches from there on.\nThus the default is  at least\nonce\u00a0processing mode. An operator can be configured for\nat most once\u00a0recovery, in which\ncase the re-initialized operator starts from next available window; or\nfor exactly once\u00a0recovery, in which\ncase the operator only recomputes the window it was processing when the\noutage happened.", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/application_development/#state-of-the-application", 
            "text": "The state of the application is traditionally defined as the state\nof all operators and streams at any given time. Monitoring state as\nevery tuple is processed asynchronously in a distributed environment\nbecomes a near impossible task, and cost paid to achieve it is very\nhigh. Consequently, in the platform, state is not saved per tuple, but\nrather at window boundaries. The platform treats windows as atomic micro\nbatches. The state saving task is delegated by STRAM to the individual\noperator or container. This ensures that the bookkeeping cost is very\nlow and works in a distributed way. Thus, the state of the application\nis defined as the collection of states of every operator and the set of\nall windows stored in the buffer server. This allows STRAM to rebuild\nany part of the application from the last saved state of the impacted\noperators and the windows retained by the buffer server. The state of an\noperator is intrinsically associated with a window id. Since operators\ncan override the default checkpointing period, operators may save state\nat the end of different windows. This works because the buffer server\nsaves all windows for as long as they are needed (state in the buffer\nserver is purged once STRAM determines that it is not longer needed\nbased on checkpointing in downstream operators).  Operators can be stateless or stateful. A stateless operator\nretains no data between windows. All results of all computations done in\na window are emitted in that window. Variables in such an operator are\neither transient or are cleared by an end_window event. Such operators\nneed no state restoration after an outage. A stateful operator retains\ndata between windows and has data in checkpointed state. This data\n(state) is used for computation in future windows. Such an operator\nneeds its state restored after an outage. By default the platform\nassumes the operator is stateful. In order to optimize recovery (skip\nprocessing related to state recovery) for a stateless operator, the\noperator needs to be declared as stateless to STRAM. Operators can\nexplicitly mark themselves stateless via an annotation or an\nattribute.  Recovery mechanisms are explained later in this section. Operator\ndevelopers have to ensure that there is no dependency on the order of\ntuples between two different streams. As mentioned earlier in this\ndocument, the platform guarantees in-order tuple delivery within a\nsingle stream, For operators with multiple input ports, a replay may\nresult in a different relative order of tuples among the different input\nports. If the output tuple computation is affected by this relative\norder, the operator may have to wait for the endWindow call (at which\npoint it would have seen all the tuples from all input ports in the\ncurrent window), perform order-dependent computations correctly and\nfinally, emit results.", 
            "title": "State of the Application"
        }, 
        {
            "location": "/application_development/#checkpointing", 
            "text": "STRAM provides checkpointing parameters to StreamingContainer\nduring initialization. A checkpoint period is given to the containers\nthat have the window generators. A control tuple is sent at the end of\ncheckpoint interval. This tuple traverses through the data path via\nstreams and triggers each StreamingContainer in the path to instrument a\ncheckpoint of the operator that receives this tuple. This ensures that\nall the operators checkpoint at exactly the same window boundary (except\nin those cases where a different checkpoint interval was configured for\nan operator by the user).  The only delay is the latency of the control tuple to reach all\nthe operators. Checkpoint is thus done between the endWindow call of a\nwindow and the beginWindow call of the next window. Since most operators\nare computing in parallel (with the exception of those connected by\nTHREAD_LOCAL streams) they each checkpoint as and when they are ready\nto process the \u201ccheckpoint\u201d control tuple. The asynchronous design of\nthe platform means that there is no guarantee that two operators would\ncheckpoint at exactly the same time, but there is a guarantee that by\ndefault they would checkpoint at the same window boundary. This feature\nalso ensures that purge of old data can be efficiently done: Once the\ncheckpoint window tuple is done traversing the DAG, the checkpoint state\nof the entire DAG increments to this window id at which point prior\ncheckpoint data can be discarded.  In case of an operator that has an application window size that is\nlarger than the size of the streaming window, the checkpointing by\ndefault still happens at same intervals as with other operators. To\nalign checkpointing with application window boundary, the application\ndeveloper should set the attribute \u201cCHECKPOINT_WINDOW_COUNT\u201d to\n\u201cAPPLICATION_WINDOW_COUNT\u201d. This ensures that the checkpoint happens\nat the  end\u00a0of the application\nwindow and not within\u00a0that window.\nSuch operators now treat the application window as an atomic computation\nunit. The downside is that it does need the upstream buffer server to\nkeep tuples for the entire application window.  If an operator is completely stateless, i.e. an outbound tuple is\nonly emitted in the process\u00a0call\nand only depends on the tuple of that call, there is no need to align\ncheckpointing with application window end. If the operator is stateful\nonly within a window, the operator developer should strongly consider\ncheckpointing only on the application window boundary.  Checkpointing involves pausing an operator, serializing the state\nto persistent storage and then resuming the operator. Thus checkpointing\nhas a latency cost that can negatively affect computational throughput;\nto minimize that impact, it is important to ensure that checkpointing is\ndone with minimal required objects. This means, as mentioned earlier,\nall data that is not part of the operator state should be declared as\ntransient so that it is not persisted.  An operator developer can also create a stateless operator (marked\nwith the Stateless annotation). Stateless operators are not\ncheckpointed. Obviously, in such an operator, computation should not\ndepend on state from a previous window.  The serialized \u00a0state of an operator is stored as a file, and is\nthe state to which that the operator is restored if an outage happens\nbefore the next checkpoint. The id of the last completed window (per\noperator) is sent back to STRAM in the next heartbeat. The default\nimplementation for serialization uses KRYO. Multiple past checkpoints\nare kept per operator. Depending on the downstream checkpoint, one of\nthese are chosen for recovery. Checkpoints and buffer server state are\npurged once STRAM sees windows as fully processed in the DAG.  A complete recovery of an operator needs the operator to be\ncreated, its checkpointed state restored and then all the lost atomic\nwindows replayed by the upstream buffer server(s). The above design\nkeeps the bookkeeping cost low with quick catch up time. In the next\nsection we will see how this simple abstraction allows applications to\nrecover under different requirements.", 
            "title": "Checkpointing"
        }, 
        {
            "location": "/application_development/#recovery-mechanisms_1", 
            "text": "Recovery mechanism are ways to recover from a container (or an\noperator) outage. In this section we discuss a single container outage.\nMultiple container outages are handled as independent events. Recovery\nrequires the upstream buffer server to replay windows and it would\nsimply go one more level upstream if the immediate upstream container\nhas also failed. If multiple operators are in a container (THREAD_LOCAL\nor CONTAINER_LOCAL stream) the container recovery treats each operator\nas an independent object when figuring out the recovery steps.\nApplication developers can set any of the recovery mechanisms discussed\nbelow for node outage.  In general, the cost of recovery depends on the state of the\noperator and the recovery mechanism selected, while data loss tolerance\nis specified by the application. For example a data-loss tolerant\napplication would prefer at most\nonce\u00a0recovery. All recovery mechanisms treat a streaming\nwindow as an atomic computation unit. In all three recovery mechanisms\nthe new operator connects to the upstream buffer server and asks for\ndata from a particular window onwards. Thus all recovery methods\ntranslate to deciding which atomic units to re-compute and which state\nthe new operator resumes from. A partially computed micro-batch is\nalways dropped. Such micro-batches are re-computed in at-least-once or\nexactly-once mode and skipped in at-most-once mode. The notiion of an\natomic micro-batch is a critical guiding principle as it enables very\nlow bookkeeping costs, high throughput, low recovery times, and high\nscalability. Within an application each operator can have its own\nrecovery mechanism.  At Least Once  At least once recovery is the default recovery mechanism, i.e it\nis used when no mechanism is specified. In this method, the lost\noperator is brought back to its latest viable checkpointed state and the\nupstream buffer server is asked to replay all subsequent windows. There\nis no data loss in recovery. The viable checkpoint state is defined as\nthe one whose window id is in the past as compared to all the\ncheckpoints of all the downstream operators. All downstream operators\nare restarted at their checkpointed state. They ignore all incoming data\nthat belongs to windows prior their checkpointed window. The lost\nwindows are thus recomputed and the application catches up with live\nincoming data. This is called \" at least\nonce\"\u00a0because lost windows are recomputed. For example if\nthe streaming window is 0.5 seconds and checkpointing is being done\nevery 30 seconds, then upon node outage all windows since the last\ncheckpoint (up to 60 windows) need to be re-processed. If the\napplication can handle loss of data, then this is not the most optimal\nrecovery mechanism.  In general for this recovery mode, the average time lag on a node\noutage is  = (CP/2*SW)*T + HC  where   CP \u00a0\u00a0- Checkpointing period (default value is 30 seconds)  SW \u00a0\u00a0- Streaming window period (default value is 0.5 seconds)  T \u00a0\u00a0\u00a0- \u00a0Time taken to re-compute one lost window from data in memory  HC \u00a0\u00a0- Time it takes to get a new Hadoop Container, or make do with the current ones   A lower CP is a trade off between cost of checkpointing and the\nneed to have to use it in case of outage. Input adapters cannot use\nat-least-once recovery without the support from sources outside Hadoop.\nFor an output adapter care may needed if the external system cannot\nhandle re-write of the same data.  At Most Once  This recovery mechanism is for applications that can tolerate\ndata-loss; they get the quickest recovery in return. The restarted node\nconnects to the upstream buffer server, subscribing to data from the\nstart of the next window. It then starts processing that window. The\ndownstream operators ignore the lost windows and continue to process\nincoming data normally. Thus, this mechanism forces all downstream\noperators to follow.  For multiple inputs, the operator waits for all ports with the\nat-most-once attribute to get responses from their respective buffer\nservers. Then, the operator starts processing till the end window of the\nlatest window id on each input port is reached. In this case the end\nwindow tuple is non-blocking till the common window id is reached. At\nthis point the input ports are now properly synchronized. Upstream nodes\nreconnect under  at most\nonce\u00a0paradigm in same way. \u00a0For example, assume an operator\nhas ports in1\u00a0and in2\u00a0and a checkpointed window of 95. Assume further that the buffer servers of\noperators upstream of  in1\u00a0and\nin2\u00a0respond with window id 100 and\n102 respectively. Then port in1\u00a0would continue to process till end window of\n101, while port  in2\u00a0will wait for in1\nto catch up to 102.\nFrom \u00a0then on, both ports process their tuples normally. So windows from\n96 to  99are lost. Window 100\nand 101 has only\nin1 active, and 102 onwards both ports are active. The other\nports of upstream nodes would also catch up till  102in a similar fashion. This operator may not\nneed to be checkpointed. Currently the option to not do checkpoint in\nsuch cases is not available.  In general, in this recovery mode, the average time lag on a node\noutage is  = SW/2 + HC  where    SW \u00a0- Streaming window period (default value is 0.5\nseconds)    HC \u00a0- Time it takes to get a new Hadoop Container, or make\ndo with the current ones    Exactly Once  This recovery mechanism is for applications that require no\ndata-loss as well are no recomputation. Since a window is an atomic\ncompute unit, exactly once applies to the window as a whole. In this\nrecovery mode, the operator is brought back to the start of the window\nin which the outage happened and the window is recomputed. The window is\nconsidered closed when all the data computations are done and end window\ntuple is emitted. \u00a0Exactly once requires every window to be\ncheckpointed. From then on, the operator asks the upstream buffer server\nto send data from the last checkpoint. The upstream node behaves the\nsame as in at-most-once recovery. Checkpointing after every streaming\nwindow is very costly, but users would most often do exactly once per\napplication window; if the application window size is substantially\nlarger than the streaming window size (which typically is the case) the\ncost of running an operator in this recovery mode may not be as\nhigh.  Speculative Execution  In future we looking at possibility of adding speculative execution for the applications. This would be enabled in multiple ways.    At an operator level: The upstream operator would emit to\n    two copies. The downstream operator would receive from both copies\n    and pick a winner. The winner (primary) would be picked in either of\n    the following ways   Statically as dictated by STRAM  Dynamically based on whose tuple arrives first. This mode\n    needs both copies to guarantee that the computation result would\n    have identical functionality     At a sub-query level: A part of the application DAG would be\n    run in parallel and all upstream operators would feed to two copies\n    and all downstream operators would receive from both copies. The\n    winners would again be picked in a static or dynamic manner   Entire DAG: Another copy of the application would be run by\n    STRAM and the winner would be decided outside the application. In\n    this mode the output adapters would both be writing\n    the result.   In all cases the two copies would run on different Hadoop nodes.\nSpeculative execution is under development and\nis not yet available.", 
            "title": "Recovery Mechanisms"
        }, 
        {
            "location": "/application_development/#9-dynamic-application-modifications", 
            "text": "Dynamic application modifications are being worked on and most of\nthe features discussed here are now available. The platform supports the\nability to modify the DAG of the application as per inputs as well as\nset constraints, and will continue to provide abilities to deepen\nfeatures based on this ability. All these changes have one thing in\ncommon and that is the application does not need to be restarted as\nSTRAM will instrument the changes and the streaming will catch-up and\ncontinue.  Some examples are   Dynamic Partitioning:\u00a0Automatic\n    changes in partitioning of computations to match constraints on a\n    run time basis. Examples includes STRAM adding resource during spike\n    in streams and returning them once spike is gone. Scale up and scale\n    down is done automatically without human intervention.  Modification via constraints: Attributes can be changed via\n    Webservices and STRAM would adapt the execution plan to meet these.\n    Examples include operations folks asking STRAM to reduce container\n    count, or changing network resource restrictions.  Modification via properties: Properties of operators can be\n    changed in run time. This enables application developers to trigger\n    a new behavior as need be. Examples include triggering an alert ON.\n    The platform supports changes to any property of an operator that\n    has a setter function defined.  Modification of DAG structure: Operators and streams can be\n    added to or removed from a running DAG, provided the code of the\n    operator being added is already in the classpath of the running\n    application master. \u00a0This enables application developers to add or\n    remove processing pipelines on the fly without having to restart\n    the application.  Query Insertion: Addition of sub-queries to currently\n    running application. This query would take current streams as inputs\n    and start computations as per their specs. Examples insertion of\n    SQL-queries on live data streams, dynamic query submission and\n    result from STRAM (not yet available).   Dynamic modifications to applications are foundational part of the\nplatform. They enable users to build layers over the applications. Users\ncan also save all the changes done since the application launch, and\ntherefore predictably get the application to its current state. For\ndetails refer to   Configuration Guide \n.", 
            "title": "9: Dynamic Application Modifications"
        }, 
        {
            "location": "/application_development/#user-interface", 
            "text": "The platform provides a rich user interface. This includes tools\nto monitor the application system metrics (throughput, latency, resource\nutilization, etc.); dashboards for application data, replay, errors; and\na Developer studio for application creation, launch etc. For details\nrefer to   UI Console Guide .", 
            "title": "User Interface"
        }, 
        {
            "location": "/application_development/#demos", 
            "text": "In this section we list some of the demos that come packaged with\ninstaller. The source code for the demos is available in the open-source Apache Apex-Malhar repository .\nAll of these do computations in real-time. Developers are encouraged to\nreview them as they use various features of the platform and provide an\nopportunity for quick learning.   Computation of PI:\n    Computes PI by generating a random location on X-Y plane and\n    measuring how often it lies within the unit circle centered\n    at (0,0).  Yahoo! Finance quote\u00a0computation:\n    Computes ticker quote, 1-day chart (per min), and simple moving\n    averages (per 5 min).  Echoserver Reads messages from a\n    network connection and echoes them back out.  Twitter top N tweeted urls: Computes\n    top N tweeted urls over last 5 minutes  Twitter trending hashtags: Computes\n    the top Twitter Hashtags over the last 5 minutes  Twitter top N frequent words:\n    Computes top N frequent words in a sliding window  Word count: Computes word count for\n    all words within a large file  Mobile location tracker: Tracks\n    100,000 cell phones within an area code moving at car speed (jumping\n    cell phone towers every 1-5 seconds).  Frauddetect: Analyzes a stream of\n    credit card merchant transactions.  Mroperator:Contains several\n    map-reduce applications.  R: Analyzes a synthetic stream of\n    eruption event data for the Old Faithful\n    geyser (https://en.wikipedia.org/wiki/Old_Faithful).  Machinedata: Analyzes a synthetic\n    stream of events to determine health of a machine.", 
            "title": "Demos"
        }, 
        {
            "location": "/application_packages/", 
            "text": "Apache Apex Application Packages\n\n\nAn Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the App Package)\n\n\nApache Apex 3.0.0 or later (for launching the App Package in your cluster)\n\n\n\n\nCreating Your First Apex App Package\n\n\nYou can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.\n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:\n\n\n$cd mydtapp; mvn package\n\n\n\nThis builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:\n\n\n -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n\n\n\n\nThe \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as shown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.2.0-incubating (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.\n\n\nWriting Your Own App Package\n\n\nPlease refer to the \nCreating Apps\n on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to \nOperator Development Guide\n), project dependencies, default and required configuration properties, pre-set configurations and other metadata.\n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--\n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\napex-engine\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n\n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]\n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\ndt.attr.\nattribute\n. The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.attr.\nattribute\n. The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10\n\n\nproperty\n\n  \nname\ndt.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\ndt.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.\n\n\n  \nproperty\n\n    \nname\ndt.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter \ndt.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and \ndt.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.\n\n\nproperty\n\n  \nname\ndt.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\ndt.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\ndt.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\ndt.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\ndt.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    dt.attr.APPLICATION_NAME\n\n\n\nThere are also other properties that can be set.  For details on\nproperties, refer to the \nOperation and Installation Guide\n.\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\nin your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n\n  \napex.version\n3.2.0-incubating\n/apex.version\n\n  \napex.apppackage.classpath\\\nlib*.jar\n/apex.apppackage.classpath\n\n\n/properties\n\n\n\n\n\napex.version is the Apache Apex version that are to be used\nwith this Application Package.\n\n\napex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n\n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\nZip Structure of Application Package\n\n\nApache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.\n\n\n\n\nManaging Application Packages Through DT Gateway\n\n\nThe DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.\n\n\nStoring an Application Package\n\n\nYou can store your Application Packages through DT Gateway using this\nREST call:\n\n\n POST /ws/v2/appPackages\n\n\n\n\nThe payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:\n\n\n$ curl -XPOST -T \napp-package-file\n http://localhost:9090/ws/v2/appPackages\n\n\n\n\nGetting Meta Information on Application Packages\n\n\nYou can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}\n\n\n\n\nThe parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this call.\n\n\nGET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}\n\n\n\n\nGetting List of Pre-Set Configurations in Application Package\n\n\nYou can get a list of pre-set configurations within the Application\nPackage using this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs\n\n\n\n\nYou can also get the content of a specific pre-set configuration within\nthe Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nChanging Pre-Set Configurations in Application Package\n\n\nYou can create or replace pre-set configurations within the Application\nPackage\n\n\n PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nThe payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.\n\n\n DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nRetrieving an Application Package\n\n\nYou can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download\n\n\n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\nPOST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}\n\n\n\n\nThe config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:\n\n\n {\nproperty-name\n:\nproperty-value\n, ... }\n\n\n\n\nHere is an example of launching an application through curl:\n\n\n $ curl -XPOST -d'{\ndt.operator.console.prop.stringFormat\n:\nxyz %s\n}'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app\n lications/MyFirstApplication/launch\n\n\n\n\nPlease refer to the \nGateway API reference\n for the complete specification of the REST API.\n\n\nExamining and Launching Application Packages Through Apex CLI\n\n\nIf you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (dtcli).  Please refer to the \nGateway API\n\nto see samples for these commands.\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis Apex CLI command.\n\n\n dt\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n dt\n get-app-package-operators \napp-package-file\n \npackage-prefix\n\n [parent-class]\n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\ndt\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\ndt\n launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package] \napp-package-file\n\n [matching-app-name]\n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/application_packages/#apache-apex-application-packages", 
            "text": "An Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.", 
            "title": "Apache Apex Application Packages"
        }, 
        {
            "location": "/application_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the App Package)  Apache Apex 3.0.0 or later (for launching the App Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/application_packages/#creating-your-first-apex-app-package", 
            "text": "You can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.", 
            "title": "Creating Your First Apex App Package"
        }, 
        {
            "location": "/application_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:  $cd mydtapp; mvn package  This builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:   -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  The \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/application_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as shown below.   Group ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.2.0-incubating (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/application_packages/#writing-your-own-app-package", 
            "text": "Please refer to the  Creating Apps  on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to  Operator Development Guide ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.", 
            "title": "Writing Your Own App Package"
        }, 
        {
            "location": "/application_packages/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId malhar-library /artifactId \n       version ${apex.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--\n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId apex-engine /artifactId \n       version ${apex.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies   By default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/application_packages/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/application_packages/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter dt.attr. attribute . The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/application_packages/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter dt.operator. operator-name .attr. attribute . The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10  property \n   name dt.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/application_packages/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter dt.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.     property \n     name dt.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.", 
            "title": "Operator properties"
        }, 
        {
            "location": "/application_packages/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter  dt.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and  dt.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.  property \n   name dt.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/application_packages/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter dt.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name dt.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/application_packages/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name dt.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/application_packages/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration   The name of an application-specific property takes the form of:  dt.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      dt.attr.APPLICATION_NAME  There are also other properties that can be set.  For details on\nproperties, refer to the  Operation and Installation Guide .  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/application_packages/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/application_packages/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/application_packages/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/application_packages/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties \n   apex.version 3.2.0-incubating /apex.version \n   apex.apppackage.classpath\\ lib*.jar /apex.apppackage.classpath  /properties   apex.version is the Apache Apex version that are to be used\nwith this Application Package.  apex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/application_packages/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n  The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-application-package", 
            "text": "Apache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.", 
            "title": "Zip Structure of Application Package"
        }, 
        {
            "location": "/application_packages/#managing-application-packages-through-dt-gateway", 
            "text": "The DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.", 
            "title": "Managing Application Packages Through DT Gateway"
        }, 
        {
            "location": "/application_packages/#storing-an-application-package", 
            "text": "You can store your Application Packages through DT Gateway using this\nREST call:   POST /ws/v2/appPackages  The payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:  $ curl -XPOST -T  app-package-file  http://localhost:9090/ws/v2/appPackages", 
            "title": "Storing an Application Package"
        }, 
        {
            "location": "/application_packages/#getting-meta-information-on-application-packages", 
            "text": "You can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}", 
            "title": "Getting Meta Information on Application Packages"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}  The parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this call.  GET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#getting-list-of-pre-set-configurations-in-application-package", 
            "text": "You can get a list of pre-set configurations within the Application\nPackage using this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs  You can also get the content of a specific pre-set configuration within\nthe Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Getting List of Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#changing-pre-set-configurations-in-application-package", 
            "text": "You can create or replace pre-set configurations within the Application\nPackage   PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  The payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.   DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Changing Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#retrieving-an-application-package", 
            "text": "You can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download", 
            "title": "Retrieving an Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  POST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}  The config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:   { property-name : property-value , ... }  Here is an example of launching an application through curl:   $ curl -XPOST -d'{ dt.operator.console.prop.stringFormat : xyz %s }'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app\n lications/MyFirstApplication/launch  Please refer to the  Gateway API reference  for the complete specification of the REST API.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/application_packages/#examining-and-launching-application-packages-through-apex-cli", 
            "text": "If you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (dtcli).  Please refer to the  Gateway API \nto see samples for these commands.", 
            "title": "Examining and Launching Application Packages Through Apex CLI"
        }, 
        {
            "location": "/application_packages/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis Apex CLI command.   dt  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package_1", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   dt  get-app-package-operators  app-package-file   package-prefix \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package_1", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  dt  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package_1", 
            "text": "You can launch an application within an Application Package.  dt  launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package]  app-package-file \n [matching-app-name]  Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/configuration_packages/", 
            "text": "Apache Apex Configuration Packages\n\n\nAn Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an\n\nApplication Package\n using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the Config Package)\n\n\nApex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)\n\n\n\n\nCreating Your First Configuration Package\n\n\nYou can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:\n\n\n$ mvn package                                                         \n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.2.0-incubating (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmydtconf\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy DataTorrent Application Configuration\n/name\n\n  \ndescription\nMy DataTorrent Application Configuration Description\n/description\n\n  \nproperties\n\n    \ndatatorrent.apppackage.name\nmydtapp\n/datatorrent.apppackage.name\n\n    \ndatatorrent.apppackage.minversion\n1.0.0\n/datatorrent.apppackage.minversion\n\n   \ndatatorrent.apppackage.maxversion\n1.9999.9999\n/datatorrent.apppackage.maxversion\n\n    \ndatatorrent.appconf.classpath\nclasspath/*\n/datatorrent.appconf.classpath\n\n    \ndatatorrent.appconf.files\nfiles/*\n/datatorrent.appconf.files\n\n  \n/properties\n \n\n\n\n\n\nIn pom.xml, you can change the following keys to your desired values\n\n\n\n\ngroupId\n\n\nversion\n\n\nartifactId\n\n\nname\n\n\ndescription\n\n\n\n\nYou can also change the values of \n\n\n\n\ndatatorrent.apppackage.name\n\n\ndatatorrent.apppackage.minversion\n\n\ndatatorrent.apppackage.maxversion\n\n\n\n\nto reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  \n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files} \n\n\n\n\nLaunching with CLI\n\n\n-conf\n option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:\n\n\ndt\\\n launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar\n\n\n\nThis command expects both the application package and the configuration package to be in the local file system.\n\n\nRelated REST API\n\n\nPOST /ws/v2/configPackages\n\n\nPayload: Raw content of configuration package zip\n\n\nFunction: Creates or replace a configuration package zip file in HDFS\n\n\nCurl example:\n\n\n$ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages\n\n\n\nGET /ws/v2/configPackages?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName. \n\n\nGET /ws/v2/configPackages/\nuser\n?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n\n\nFunction: Returns the information of the specified configuration package\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n/download\n\n\nFunction: Returns the raw config package file\n\n\nCurl example:\n\n\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \\\n DTConfig-xyz.jar\n$ unzip -t DTConfig-xyz.jar\n\n\n\n\nPOST /ws/v2/appPackages/\nuser\n/\napp-pkg-name\n/\napp-pkg-version\n/applications/{app-name}/launch?configPackage=\nuser\n/\nconfpkgname\n\n\nFunction: Launches the app package with the specified configuration package stored in HDFS.\n\n\nCurl example:\n\n\n$ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#apache-apex-configuration-packages", 
            "text": "An Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an Application Package  using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.", 
            "title": "Apache Apex Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the Config Package)  Apex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuration_packages/#creating-your-first-configuration-package", 
            "text": "You can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.", 
            "title": "Creating Your First Configuration Package"
        }, 
        {
            "location": "/configuration_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:  $ mvn package                                                           The \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/configuration_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.   Group ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.2.0-incubating (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/configuration_packages/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:  ./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/configuration_packages/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId mydtconf /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My DataTorrent Application Configuration /name \n   description My DataTorrent Application Configuration Description /description \n   properties \n     datatorrent.apppackage.name mydtapp /datatorrent.apppackage.name \n     datatorrent.apppackage.minversion 1.0.0 /datatorrent.apppackage.minversion \n    datatorrent.apppackage.maxversion 1.9999.9999 /datatorrent.apppackage.maxversion \n     datatorrent.appconf.classpath classpath/* /datatorrent.appconf.classpath \n     datatorrent.appconf.files files/* /datatorrent.appconf.files \n   /properties    In pom.xml, you can change the following keys to your desired values   groupId  version  artifactId  name  description   You can also change the values of    datatorrent.apppackage.name  datatorrent.apppackage.minversion  datatorrent.apppackage.maxversion   to reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/configuration_packages/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.    Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:   properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.   After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/configuration_packages/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/configuration_packages/#launching-with-cli", 
            "text": "-conf  option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:  dt\\  launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar  This command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/configuration_packages/#related-rest-api", 
            "text": "POST /ws/v2/configPackages  Payload: Raw content of configuration package zip  Function: Creates or replace a configuration package zip file in HDFS  Curl example:  $ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages  GET /ws/v2/configPackages?appPackageName=... appPackageVersion=...  All query parameters are optional  Function: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName.   GET /ws/v2/configPackages/ user ?appPackageName=... appPackageVersion=...  All query parameters are optional  Function: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.  GET /ws/v2/configPackages/ user / name  Function: Returns the information of the specified configuration package  GET /ws/v2/configPackages/ user / name /download  Function: Returns the raw config package file  Curl example:  $ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \\  DTConfig-xyz.jar\n$ unzip -t DTConfig-xyz.jar  POST /ws/v2/appPackages/ user / app-pkg-name / app-pkg-version /applications/{app-name}/launch?configPackage= user / confpkgname  Function: Launches the app package with the specified configuration package stored in HDFS.  Curl example:  $ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Related REST API"
        }, 
        {
            "location": "/operator_development/", 
            "text": "Operator Development Guide\n\n\nOperators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).\n\n\nIn this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes\n\n\n\n\nApache Apex Operators\n\u00a0- Introduction to operator terminology and concepts.\n\n\nWriting Custom Operators\n\u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.\n\n\nOperator Reference\n - Details of operator internals, lifecycle, and best practices and optimizations.\n\n\n\n\n\n\nApache Apex Operators \n\n\nOperators - \u201cWhat\u201d in a nutshell\n\n\nOperators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.\n\n\n\n\nOperators - \u201cHow\u201d in a nutshell\n\n\nAn Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.\n\n\nTypes of Operators\n\n\nAn operator works on one tuple at a time. These tuples may be supplied\nby some other operator in the application or by some external source,\nlike a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to some other operator, or may be\nstored into some external system. Based on the functions that the\nOperator performs, we have the following types of operators:\n\n\n\n\nInput Adapter\n - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.\n\n\nGeneric Operator\n - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.\n\n\nOutput Adapter\n - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.\n\n\n\n\nNote: There can be multiple operators of all types in an application\nDAG.\n\n\nPositioning of Operators in the DAG\n\n\nWe may refer to operators depending on their positioning with respect to\none another. For any operator opr, we have the following types of\noperators.\n\n\n\n\nUpstream operators\n - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.\n\n\nDownstream operators\n - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.\n\n\n\n\nNote that there are no cycles formed in the application\u00a0DAG.\n\n\n\n\nPorts\n\n\nThe operators in the DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. The ports again fall into two types.\n\n\n\n\nInput Port\n - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.\n\n\nOutput port\n - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.\n\n\n\n\nLooking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.\n\n\n\n\n\n\nWorking of an Operator\n\n\nAn operator passes through various stages during its lifetime. Each\nstage is an API call that the streaming application master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.\n\n\n\n\n\n\nThe \nsetup()\n call initializes the operator and prepares itself to\n    start processing tuples.\n\n\nThe \nbeginWindow()\n call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.\n\n\nThe \nprocess()\n call belongs to the \nInputPort\n and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.\n\n\nThe \nemitTuples()\n is the counterpart of \nprocess()\n call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.\n\n\nThe \nendWindow()\n call marks the end of the window and allows for any\n    processing to be done after the window ends.\n\n\nThe \nteardown()\n call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.\n\n\n\n\nWriting Custom Operators \n\n\nAbout this tutorial\n\n\nThis tutorial will guide the user towards developing a operator from\nscratch. It will include all aspects of writing an operator including\ndesign, code as well as unit tests.\n\n\nIntroduction\n\n\nIn this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.\n\n\nDesign\n\n\nDesign of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.\n\n\nFunctionality\n\n\nWe can define the scope of operator functionality using the following\ntasks:\n\n\n\n\nParse the input tuple to identify the words in the tuple\n\n\nIdentify the stop-words in the tuple by looking up the stop-word\n    file as configured\n\n\nFor each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts\n\n\n\n\nLet\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.\n\n\n\n\nHumpty dumpty sat on a wall\n\n\nHumpty dumpty had a great fall\n\n\n\n\nInitially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:\n\n\nhumpty - 1\ndumpty - 1\nsat - 1\nwall - 1\n\n\n\n\nNote that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.\n\n\nSimilarly, after the second tuple is processed, the counts that must be\nemitted are:\n\n\nhumpty - 2\ndumpty - 2\ngreat - 1\nfall - 1\n\n\n\n\nAgain, we ignore the words \n\u201chad\u201d\n and \n\u201ca\u201d\n since these are stop-words.\n\n\nNote that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.\n\n\nInputs\n\n\nAs seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:\n\n\n\n\nInput stream whose tuple type is String\n\n\nInput HDFS file path, pointing to a file containing stop-words\n\n\n\n\nOnly one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.\n\n\n\n\nOutputs\n\n\nWe can define the output for this operator in multiple ways.\n\n\n\n\nThe operator may send out the set of counts for which the counts\n    have changed after processing each tuple.\n\n\nSome applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.\n\n\n\n\nLet us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter\n\n\u201csendPerTuple\u201d\n. The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).\n\n\nThe type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a \n key, value \n\u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.\n\n\n\n\nConfiguration\n\n\nWe have the following configuration parameters:\n\n\n\n\nstopWordFilePath\n\u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.\n\n\nsendPerTuple\n\u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.\n\n\n\n\nCode\n\n\nThe source code for the tutorial can be found here:\n\n\nhttps://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial\n\n\nOperator Reference \n\n\nThe Operator Class\n\n\nThe operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:\n\n\n\n\nsetup(OperatorContext context)\n\n\nbeginWindow(long windowId)\n\n\nendWindow()\n\n\ntearDown()\n\n\n\n\nIn order to simplify the creation of an operator, the Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the \nApex Operators\n\u00a0section and the\n\nReference\n\u00a0section for details on these.\n\n\nWe extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.\n\n\npublic class WordCountOperator extends BaseOperator\n{\n}\n\n\n\n\nClass (Operator) properties\n\n\nWe define the following class variables:\n\n\n\n\nsendPerTuple\n\u00a0- Needed for configuring the frequency of output from\n    the operator\n\n\n\n\nprivate boolean sendPerTuple = true; // default\n\n\n\n\n\n\nstopWordFilePath\n\u00a0- Stores the path to the stop words file on HDFS\n\n\n\n\nprivate String stopWordFilePath;\u00a0// no default\n\n\n\n\n\n\nstopWords\n\u00a0- Stores the stop words read from the configured file\n\n\n\n\nprivate transient String[] stopWords;\n\n\n\n\n\n\nglobalCounts\n\u00a0- A Map which stores the counts for all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpointing state\n    and can be recovered in event of a crash.\n\n\n\n\nprivate Map\nString, Long\n globalCounts;\n\n\n\n\n\n\nupdatedCounts\n\u00a0- A map which stores the counts for only the most\n    recent tuple(s). Whether to store the most recent or the recent\n    window worth of tuples will be determined by the configuration\n    parameter sendPerTuple.\n\n\n\n\nprivate transient Map\nString, Long\n updatedCounts;\n\n\n\n\n\n\ninput\n - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.\n\n\n\n\npublic transient DefaultInputPort\nString\n input = new \u00a0 \u00a0\nDefaultInputPort\nString\n()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};\n\n\n\n\n\n\noutput - The output port for the operator. The type of this port is\n    Entry \n String, Long \n, which means the operator will emit \n word,\n    count \n pairs for the updated counts.\n\n\n\n\npublic transient DefaultOutputPort \nEntry\nString, Long\n output = new\nDefaultOutputPort\nEntry\nString,Long\n();\n\n\n\n\nThe Constructor\n\n\nThe constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. In case\nof Word Count\u00a0operator, we initialize the globalCounts variable in the\nconstructor.\n\n\nglobalCounts = Maps.newHashMap();\n\n\n\n\nSetup call\n\n\nThe setup method is called only once during the lifetime of an operator.\nThe purpose of the setup call is to allow the operator to set itself up\nfor processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence it is essential that such objects\nmust be initialized in the setup call. In case of operator failure, the\noperator will be redeployed, most likely on a different container. In\nthis case, it is the setup method which will be called by the Apache\nApex engine to allow the operator to prepare for execution in the new\ncontainer.\n\n\nWe perform the following tasks as part of the setup call:\n\n\n\n\nRead the stop-word list from HDFS and store it in the\n    stopWords\u00a0array\n\n\nInitialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    This is a transient variable, hence the value of this variable will\n    be lost in case of operator failure.\n\n\n\n\nBegin Window call\n\n\nThe begin window call signals the start of an application window. In\ncase of Word Count Operator, if the sendPerTuple\u00a0is set to false, it\nmeans that we are expecting updated counts for the most recent window of\ndata. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.\n\n\nProcess Tuple call\n\n\nThe processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.\n\n\nEnd Window call\n\n\nThis call signals the end of an application window. In case of Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.\n\n\nTeardown call\n\n\nThis method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. In case of our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.\n\n\nTesting your Operator\n\n\nTesting an operator after development is essential to ensure that he\nrequired functionality is indeed correctly implemented. As part of\ntesting our operator, we test the following two facets:\n\n\n\n\nTest output of the operator after processing a single tuple\n\n\nTest output of the operator after processing of a window of tuples\n\n\n\n\nThe unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.\n\n\n\n\nInvoke constructor; non-transients initialized.\n\n\nCopy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Operators"
        }, 
        {
            "location": "/operator_development/#operator-development-guide", 
            "text": "Operators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).  In this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes   Apache Apex Operators \u00a0- Introduction to operator terminology and concepts.  Writing Custom Operators \u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.  Operator Reference  - Details of operator internals, lifecycle, and best practices and optimizations.", 
            "title": "Operator Development Guide"
        }, 
        {
            "location": "/operator_development/#apache-apex-operators", 
            "text": "", 
            "title": "Apache Apex Operators "
        }, 
        {
            "location": "/operator_development/#operators-what-in-a-nutshell", 
            "text": "Operators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.", 
            "title": "Operators - \u201cWhat\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#operators-how-in-a-nutshell", 
            "text": "An Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.", 
            "title": "Operators - \u201cHow\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#types-of-operators", 
            "text": "An operator works on one tuple at a time. These tuples may be supplied\nby some other operator in the application or by some external source,\nlike a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to some other operator, or may be\nstored into some external system. Based on the functions that the\nOperator performs, we have the following types of operators:   Input Adapter  - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.  Generic Operator  - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.  Output Adapter  - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.   Note: There can be multiple operators of all types in an application\nDAG.", 
            "title": "Types of Operators"
        }, 
        {
            "location": "/operator_development/#positioning-of-operators-in-the-dag", 
            "text": "We may refer to operators depending on their positioning with respect to\none another. For any operator opr, we have the following types of\noperators.   Upstream operators  - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.  Downstream operators  - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.   Note that there are no cycles formed in the application\u00a0DAG.", 
            "title": "Positioning of Operators in the DAG"
        }, 
        {
            "location": "/operator_development/#ports", 
            "text": "The operators in the DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. The ports again fall into two types.   Input Port  - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.  Output port  - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.   Looking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development/#working-of-an-operator", 
            "text": "An operator passes through various stages during its lifetime. Each\nstage is an API call that the streaming application master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.    The  setup()  call initializes the operator and prepares itself to\n    start processing tuples.  The  beginWindow()  call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.  The  process()  call belongs to the  InputPort  and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.  The  emitTuples()  is the counterpart of  process()  call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.  The  endWindow()  call marks the end of the window and allows for any\n    processing to be done after the window ends.  The  teardown()  call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.", 
            "title": "Working of an Operator"
        }, 
        {
            "location": "/operator_development/#writing-custom-operators", 
            "text": "", 
            "title": "Writing Custom Operators "
        }, 
        {
            "location": "/operator_development/#about-this-tutorial", 
            "text": "This tutorial will guide the user towards developing a operator from\nscratch. It will include all aspects of writing an operator including\ndesign, code as well as unit tests.", 
            "title": "About this tutorial"
        }, 
        {
            "location": "/operator_development/#introduction", 
            "text": "In this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operator_development/#design", 
            "text": "Design of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.  Functionality  We can define the scope of operator functionality using the following\ntasks:   Parse the input tuple to identify the words in the tuple  Identify the stop-words in the tuple by looking up the stop-word\n    file as configured  For each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts   Let\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.   Humpty dumpty sat on a wall  Humpty dumpty had a great fall   Initially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:  humpty - 1\ndumpty - 1\nsat - 1\nwall - 1  Note that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.  Similarly, after the second tuple is processed, the counts that must be\nemitted are:  humpty - 2\ndumpty - 2\ngreat - 1\nfall - 1  Again, we ignore the words  \u201chad\u201d  and  \u201ca\u201d  since these are stop-words.  Note that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.  Inputs  As seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:   Input stream whose tuple type is String  Input HDFS file path, pointing to a file containing stop-words   Only one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.   Outputs  We can define the output for this operator in multiple ways.   The operator may send out the set of counts for which the counts\n    have changed after processing each tuple.  Some applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.   Let us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter \u201csendPerTuple\u201d . The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).  The type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a   key, value  \u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.", 
            "title": "Design"
        }, 
        {
            "location": "/operator_development/#configuration", 
            "text": "We have the following configuration parameters:   stopWordFilePath \u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.  sendPerTuple \u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operator_development/#code", 
            "text": "The source code for the tutorial can be found here:  https://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial", 
            "title": "Code"
        }, 
        {
            "location": "/operator_development/#operator-reference", 
            "text": "The Operator Class  The operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:   setup(OperatorContext context)  beginWindow(long windowId)  endWindow()  tearDown()   In order to simplify the creation of an operator, the Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the  Apex Operators \u00a0section and the Reference \u00a0section for details on these.  We extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.  public class WordCountOperator extends BaseOperator\n{\n}  Class (Operator) properties  We define the following class variables:   sendPerTuple \u00a0- Needed for configuring the frequency of output from\n    the operator   private boolean sendPerTuple = true; // default   stopWordFilePath \u00a0- Stores the path to the stop words file on HDFS   private String stopWordFilePath;\u00a0// no default   stopWords \u00a0- Stores the stop words read from the configured file   private transient String[] stopWords;   globalCounts \u00a0- A Map which stores the counts for all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpointing state\n    and can be recovered in event of a crash.   private Map String, Long  globalCounts;   updatedCounts \u00a0- A map which stores the counts for only the most\n    recent tuple(s). Whether to store the most recent or the recent\n    window worth of tuples will be determined by the configuration\n    parameter sendPerTuple.   private transient Map String, Long  updatedCounts;   input  - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.   public transient DefaultInputPort String  input = new \u00a0 \u00a0\nDefaultInputPort String ()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};   output - The output port for the operator. The type of this port is\n    Entry   String, Long  , which means the operator will emit   word,\n    count   pairs for the updated counts.   public transient DefaultOutputPort  Entry String, Long  output = new\nDefaultOutputPort Entry String,Long ();  The Constructor  The constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. In case\nof Word Count\u00a0operator, we initialize the globalCounts variable in the\nconstructor.  globalCounts = Maps.newHashMap();  Setup call  The setup method is called only once during the lifetime of an operator.\nThe purpose of the setup call is to allow the operator to set itself up\nfor processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence it is essential that such objects\nmust be initialized in the setup call. In case of operator failure, the\noperator will be redeployed, most likely on a different container. In\nthis case, it is the setup method which will be called by the Apache\nApex engine to allow the operator to prepare for execution in the new\ncontainer.  We perform the following tasks as part of the setup call:   Read the stop-word list from HDFS and store it in the\n    stopWords\u00a0array  Initialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    This is a transient variable, hence the value of this variable will\n    be lost in case of operator failure.   Begin Window call  The begin window call signals the start of an application window. In\ncase of Word Count Operator, if the sendPerTuple\u00a0is set to false, it\nmeans that we are expecting updated counts for the most recent window of\ndata. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.  Process Tuple call  The processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.  End Window call  This call signals the end of an application window. In case of Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.  Teardown call  This method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. In case of our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.", 
            "title": "Operator Reference "
        }, 
        {
            "location": "/operator_development/#testing-your-operator", 
            "text": "Testing an operator after development is essential to ensure that he\nrequired functionality is indeed correctly implemented. As part of\ntesting our operator, we test the following two facets:   Test output of the operator after processing a single tuple  Test output of the operator after processing of a window of tuples   The unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.   Invoke constructor; non-transients initialized.  Copy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Testing your Operator"
        }, 
        {
            "location": "/operators/file_splitter/", 
            "text": "File Splitter\n\n\nThis is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits. \n\n\nWhy is it needed?\n\n\nIt is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.\n\n\nClass Diagram\n\n\n\n\nAbstractFileSplitter\n\n\nThe abstract implementation defines the logic of processing \nFileInfo\n. This comprises the following tasks -  \n\n\n\n\n\n\nbuilding \nFileMetadata\n per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.\n\n\n\n\n\n\ncreating \nBlockMetadataIterator\n from \nFileMetadata\n. The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.\n\n\n\n\n\n\nretrieving \nBlockMetadata.FileBlockMetadata\n from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by \nblocksThreshold\n setting which by default is 1.  \n\n\n\n\n\n\nThe main utility method that performs all the above tasks is the \nprocess()\n method. Concrete implementations can invoke this method whenever they have data to process.\n\n\nPorts\n\n\nDeclares only output ports on which file metadata and block metadata are emitted.\n\n\n\n\nfilesMetadataOutput: metadata for each file is emitted on this port. \n\n\nblocksMetadataOutput: metadata for each block is emitted on this port. \n\n\n\n\nprocess()\n method\n\n\nWhen process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the \nblocksThreshold\n is reached or there are no more new files.\n\n\n  protected void process()\n  {\n    if (blockMetadataIterator != null \n blockCount \n blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount \n blocksThreshold \n (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }\n\n\n\n\nAbstract methods\n\n\n\n\n\n\nFileInfo getFileInfo()\n: called from within the \nprocess()\n and provides the next file to process.\n\n\n\n\n\n\nlong getDefaultBlockSize()\n: provides the block size which is used when user hasn't configured the size.\n\n\n\n\n\n\nFileStatus getFileStatus(Path path)\n: provides the \norg.apache.hadoop.fs.FileStatus\n instance for a path.   \n\n\n\n\n\n\nConfiguration\n\n\n\n\nblockSize\n: size of a block.\n\n\nblocksThreshold\n: threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.\n\n\n\n\nFileSplitterBase\n\n\nSimple operator that receives tuples of type \nFileInfo\n on its \ninput\n port. \nFileInfo\n contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.\n\n\n\nThe upstream operator emits tuples of type \nFileInfo\n on its output port which is connected to splitter input port. The downstream receives tuples of type \nBlockMetadata.FileBlockMetadata\n from the splitter's block metadata output port.\n\n\npublic class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator(\nInput\n, new JMSInput());\n    FileSplitterBase splitter = dag.addOperator(\nSplitter\n, new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator(\nBlockReader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nfile-info\n, input.output, splitter.input);\n    dag.addStream(\nblock-metadata\n, splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator\nAbstractFileSplitter.FileInfo\n\n  {\n\n    public final transient DefaultOutputPort\nAbstractFileSplitter.FileInfo\n output = new DefaultOutputPort\n();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}\n\n\n\n\nPorts\n\n\nDeclares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.\n\n\n\n\ninput: non optional port on which tuples of type \nFileInfo\n are received.\n\n\n\n\nConfiguration\n\n\n\n\nfile\n: path of the file from which the filesystem is inferred. FileSplitter creates an instance of \norg.apache.hadoop.fs.FileSystem\n which is why this path is needed.  \n\n\n\n\nFileSystem.newInstance(new Path(file).toUri(), new Configuration());\n\n\n\n\nThe fs instance is then used to fetch the default block size and \norg.apache.hadoop.fs.FileStatus\n for each file path.\n\n\nFileSplitterInput\n\n\nThis is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by \nTimeBasedDirectoryScanner\n. The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.\n\n\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.\n\n\n\n\nSplitter is the input operator here that sends block metadata to the downstream BlockReader.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nInput\n, new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator(\nBlock Reader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nblock-metadata\n, input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }\n\n\n\n\n\nPorts\n\n\nSince it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.\n\n\nConfiguration\n\n\n\n\nscanner\n: the component that scans directories asynchronously. It is of type \ncom.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner\n. The basic implementation of TimeBasedDirectoryScanner can be customized by users.  \n\n\n\n\na. \nfiles\n: comma separated list of directories to scan.  \n\n\nb. \nrecursive\n: flag that controls whether the directories should be scanned recursively.  \n\n\nc. \nscanIntervalMillis\n: interval specified in milliseconds after which another scan iteration is triggered.  \n\n\nd. \nfilePatternRegularExp\n: regular expression for accepted file names.  \n\n\ne. \ntrigger\n: a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2. \nidempotentStorageManager\n: by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of \ncom.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager\n.\n\n\nHandling of split records\n\n\nSplitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.\n\n\nWe have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers- \nAbstractFSLineReader\n and \nAbstractFSReadAheadLineReader\n can be found here \nAbstractFSBlockReader\n.", 
            "title": "File splitter"
        }, 
        {
            "location": "/operators/file_splitter/#file-splitter", 
            "text": "This is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/file_splitter/#why-is-it-needed", 
            "text": "It is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/file_splitter/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/file_splitter/#abstractfilesplitter", 
            "text": "The abstract implementation defines the logic of processing  FileInfo . This comprises the following tasks -      building  FileMetadata  per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.    creating  BlockMetadataIterator  from  FileMetadata . The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.    retrieving  BlockMetadata.FileBlockMetadata  from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by  blocksThreshold  setting which by default is 1.      The main utility method that performs all the above tasks is the  process()  method. Concrete implementations can invoke this method whenever they have data to process.  Ports  Declares only output ports on which file metadata and block metadata are emitted.   filesMetadataOutput: metadata for each file is emitted on this port.   blocksMetadataOutput: metadata for each block is emitted on this port.    process()  method  When process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the  blocksThreshold  is reached or there are no more new files.    protected void process()\n  {\n    if (blockMetadataIterator != null   blockCount   blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount   blocksThreshold   (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }  Abstract methods    FileInfo getFileInfo() : called from within the  process()  and provides the next file to process.    long getDefaultBlockSize() : provides the block size which is used when user hasn't configured the size.    FileStatus getFileStatus(Path path) : provides the  org.apache.hadoop.fs.FileStatus  instance for a path.       Configuration   blockSize : size of a block.  blocksThreshold : threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.", 
            "title": "AbstractFileSplitter"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterbase", 
            "text": "Simple operator that receives tuples of type  FileInfo  on its  input  port.  FileInfo  contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.  Example application  This is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.  The upstream operator emits tuples of type  FileInfo  on its output port which is connected to splitter input port. The downstream receives tuples of type  BlockMetadata.FileBlockMetadata  from the splitter's block metadata output port.  public class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator( Input , new JMSInput());\n    FileSplitterBase splitter = dag.addOperator( Splitter , new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator( BlockReader , new FSSliceReader());\n    ...\n    dag.addStream( file-info , input.output, splitter.input);\n    dag.addStream( block-metadata , splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator AbstractFileSplitter.FileInfo \n  {\n\n    public final transient DefaultOutputPort AbstractFileSplitter.FileInfo  output = new DefaultOutputPort ();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}  Ports  Declares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.   input: non optional port on which tuples of type  FileInfo  are received.   Configuration   file : path of the file from which the filesystem is inferred. FileSplitter creates an instance of  org.apache.hadoop.fs.FileSystem  which is why this path is needed.     FileSystem.newInstance(new Path(file).toUri(), new Configuration());  The fs instance is then used to fetch the default block size and  org.apache.hadoop.fs.FileStatus  for each file path.", 
            "title": "FileSplitterBase"
        }, 
        {
            "location": "/operators/file_splitter/#filesplitterinput", 
            "text": "This is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by  TimeBasedDirectoryScanner . The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.   Example application  This is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.   Splitter is the input operator here that sends block metadata to the downstream BlockReader.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( Input , new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator( Block Reader , new FSSliceReader());\n    ...\n    dag.addStream( block-metadata , input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }  Ports  Since it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.  Configuration   scanner : the component that scans directories asynchronously. It is of type  com.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner . The basic implementation of TimeBasedDirectoryScanner can be customized by users.     a.  files : comma separated list of directories to scan.    b.  recursive : flag that controls whether the directories should be scanned recursively.    c.  scanIntervalMillis : interval specified in milliseconds after which another scan iteration is triggered.    d.  filePatternRegularExp : regular expression for accepted file names.    e.  trigger : a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2.  idempotentStorageManager : by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of  com.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager .", 
            "title": "FileSplitterInput"
        }, 
        {
            "location": "/operators/file_splitter/#handling-of-split-records", 
            "text": "Splitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.  We have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers-  AbstractFSLineReader  and  AbstractFSReadAheadLineReader  can be found here  AbstractFSBlockReader .", 
            "title": "Handling of split records"
        }, 
        {
            "location": "/operators/block_reader/", 
            "text": "Block Reader\n\n\nThis is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block. \n\n\nWhy is it needed?\n\n\nA Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see \nAbstractFileInputOperator\n) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.\n\n\nClass Diagram\n\n\n\n\nAbstractBlockReader\n\n\nThis is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.\n\n\n\n\nPorts\n\n\n\n\n\n\nblocksMetadataInput: input port on which block metadata are received.\n\n\n\n\n\n\nblocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.\n\n\n\n\n\n\nmessages: output port on which tuples of type \ncom.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord\n are emitted. This class encapsulates a \nrecord\n and the \nblockId\n of the corresponding block.\n\n\n\n\n\n\nreaderContext\n\n\nThis is one of the most important fields in the block reader. It is of type \ncom.datatorrent.lib.io.block.ReaderContext\n and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.\n\n\nOnce the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking \nreaderContext.initialize(stream, blockMetadata, consecutiveBlock);\n. Initialize method is where any implementation of \nReaderContext\n can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.\n\n\nOnce the initialization is done, \nreaderContext.next()\n is called repeatedly until it returns \nnull\n. It is left to the \nReaderContext\n implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples- \nLineReaderContext\n and \nReadAheadLineReaderContext\n). In other cases when there isn't a possibility of split record (example- \nFixedBytesReaderContext\n), it returns \nnull\n immediately when the block boundary is reached. The return type of \nreaderContext.next()\n is of type \ncom.datatorrent.lib.io.block.ReaderContext.Entity\n which is just a wrapper for a \nbyte[]\n that represents the record and total bytes used in fetching the record.\n\n\nAbstract methods\n\n\n\n\n\n\nSTREAM setupStream(B block)\n: creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.\n\n\n\n\n\n\nR convertToRecord(byte[] bytes)\n: this converts the array of bytes into the actual instance of record type.\n\n\n\n\n\n\nAuto-scalability\n\n\nBlock reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the \nblocksMetadataInput\n port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the \npartitioner and stats-listener\n.\n\n\nConfiguration\n\n\n\n\nmaxReaders\n: when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.\n\n\nminReaders\n: when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.\n\n\ncollectStats\n: this enables or disables auto-scaling. When it is set to \ntrue\n the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.\n\n\nintervalMillis\n: when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.\n\n\n\n\n AbstractFSBlockReader\n\n\nThis abstract implementation deals with files. Different types of file systems that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The user can override \ngetFSInstance()\n method to create an instance of a specific \nFileSystem\n. By default, filesystem instance is created from the filesytem URI that comes from the default hadoop configuration.\n\n\nprotected FileSystem getFSInstance() throws IOException\n{\n  return FileSystem.newInstance(configuration);\n}\n\n\n\n\nIt uses this filesystem instance to setup a stream of type \norg.apache.hadoop.fs.FSDataInputStream\n to read the block.\n\n\n@Override\nprotected FSDataInputStream setupStream(BlockMetadata.FileBlockMetadata block) throws IOException\n{\n  return fs.open(new Path(block.getFilePath()));\n}\n\n\n\n\nAll the ports and configurations are derived from the super class. It doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n method which is delegated to concrete sub-classes.\n\n\nExample Application\n\n\nThis simple dag demonstrates how any concrete implementation of \nAbstractFSBlockReader\n can be plugged into an application. \n\n\n\n\nIn the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.\n\n\npublic class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nFile-splitter\n, new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator(\nBlock-reader\n, new LineReader());\n    Filter filter = dag.addOperator(\nFilter\n, new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator(\nRecord-writer\n, new RecordOutputOperator());\n\n    dag.addStream(\nfile-block metadata\n, input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream(\nrecords\n, blockReader.messages, filter.input);\n    dag.addStream(\nfiltered-records\n, filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader\nString\n\n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort\nAbstractBlockReader.ReaderRecord\nString\n output = new DefaultOutputPort\n();\n    public final transient DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n input = new DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord\nString\n stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(), \n.\n)) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator\nAbstractBlockReader.ReaderRecord\nString\n\n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}\n\n\n\n\nConfiguration to parallel partition block reader with its downstream operators.\n\n\n  \nproperty\n\n    \nname\ndt.operator.Filter.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n\n\n\n\nAbstractFSReadAheadLineReader\n\n\nThis extension of \nAbstractFSBlockReader\n parses lines from a block and binds the \nreaderContext\n field to an instance of \nReaderContext.ReadAheadLineReaderContext\n.\n\n\nIt is abstract because it doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n since the user may want to convert the bytes that make a line into some other type. \n\n\nReadAheadLineReaderContext\n\n\nIn order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.\n\n\nThis is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.\n\n\nAbstractFSLineReader\n\n\nSimilar to \nAbstractFSReadAheadLineReader\n, even this parses lines from a block. However, it binds the \nreaderContext\n field to an instance of \nReaderContext.LineReaderContext\n.\n\n\nLineReaderContext\n\n\nThis handles the line split differently from \nReadAheadLineReaderContext\n. It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.\n\n\nWhen the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.\n\n\nIf the validations of completeness fails for a line then \nconvertToRecord(byte[] bytes)\n should return null.\n\n\nFSSliceReader\n\n\nA concrete extension of \nAbstractFSBlockReader\n that reads fixed-size \nbyte[]\n from a block and emits the byte array wrapped in \ncom.datatorrent.netlet.util.Slice\n.\n\n\nThis operator binds the \nreaderContext\n to an instance of \nReaderContext.FixedBytesReaderContext\n.\n\n\nFixedBytesReaderContext\n\n\nThis implementation of \nReaderContext\n never reads beyond a block boundary which can result in the last \nbyte[]\n of a block to be of a shorter length than the rest of the records.\n\n\nConfiguration\n\n\nreaderContext.length\n: length of each record. By default, this is initialized to the default hdfs block size.\n\n\nPartitioner and StatsListener\n\n\nThe logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute - \nPARTITIONER\n) as well as a StatsListener. This is because the \n\nAbstractBlockReader\n implements both the \ncom.datatorrent.api.Partitioner\n and \ncom.datatorrent.api.StatsListener\n interfaces and provides an implementation of \ndefinePartitions(...)\n and \nprocessStats(...)\n which make it auto-scalable.\n\n\nprocessStats \n\n\nThe application master invokes \nResponse processStats(BatchedOperatorStats stats)\n method on the logical instance with the stats (\ntuplesProcessedPSMA\n, \ntuplesEmittedPSMA\n, \nlatencyMA\n, etc.) of each partition. The data which this operator is interested in is the \nqueueSize\n of the input port \nblocksMetadataInput\n.\n\n\nUsually the \nqueueSize\n of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with \n@DataQueueSize\n. In this case \nAbstractBlockReader\n itself is the \nStatsListener\n which is why it is annotated with \n@DataQueueSize\n.\n\n\nThe logical instance caches the queue size per partition and at regular intervals (configured by \nintervalMillis\n) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.\n\n\n\n\nThe goal of this logic is to create as many partitions within bounds (see \nmaxReaders\n and \nminReaders\n above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.\n\n\ndefinePartitions\n\n\nBased on the \nrepartitionRequired\n field of the \nResponse\n object which is returned by \nprocessStats\n method, the application master invokes \n\n\nCollection\nPartition\nAbstractBlockReader\n...\n definePartitions(Collection\nPartition\nAbstractBlockReader\n...\n partitions, PartitioningContext context)\n\n\n\n\non the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created. \n\n\nPlease note auto-scaling can be disabled by setting \ncollectStats\n to \nfalse\n. If the use-case requires only static partitioning, then that can be achieved by setting \nStatelessPartitioner\n as the operator attribute- \nPARTITIONER\n on the block reader.", 
            "title": "Block reader"
        }, 
        {
            "location": "/operators/block_reader/#block-reader", 
            "text": "This is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/block_reader/#why-is-it-needed", 
            "text": "A Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see  AbstractFileInputOperator ) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/block_reader/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/block_reader/#abstractblockreader", 
            "text": "This is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.   Ports    blocksMetadataInput: input port on which block metadata are received.    blocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.    messages: output port on which tuples of type  com.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord  are emitted. This class encapsulates a  record  and the  blockId  of the corresponding block.    readerContext  This is one of the most important fields in the block reader. It is of type  com.datatorrent.lib.io.block.ReaderContext  and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.  Once the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking  readerContext.initialize(stream, blockMetadata, consecutiveBlock); . Initialize method is where any implementation of  ReaderContext  can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.  Once the initialization is done,  readerContext.next()  is called repeatedly until it returns  null . It is left to the  ReaderContext  implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples-  LineReaderContext  and  ReadAheadLineReaderContext ). In other cases when there isn't a possibility of split record (example-  FixedBytesReaderContext ), it returns  null  immediately when the block boundary is reached. The return type of  readerContext.next()  is of type  com.datatorrent.lib.io.block.ReaderContext.Entity  which is just a wrapper for a  byte[]  that represents the record and total bytes used in fetching the record.  Abstract methods    STREAM setupStream(B block) : creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.    R convertToRecord(byte[] bytes) : this converts the array of bytes into the actual instance of record type.    Auto-scalability  Block reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the  blocksMetadataInput  port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the  partitioner and stats-listener .  Configuration   maxReaders : when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.  minReaders : when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.  collectStats : this enables or disables auto-scaling. When it is set to  true  the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.  intervalMillis : when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.", 
            "title": "AbstractBlockReader"
        }, 
        {
            "location": "/operators/block_reader/#abstractfsreadaheadlinereader", 
            "text": "This extension of  AbstractFSBlockReader  parses lines from a block and binds the  readerContext  field to an instance of  ReaderContext.ReadAheadLineReaderContext .  It is abstract because it doesn't provide an implementation of  convertToRecord(byte[] bytes)  since the user may want to convert the bytes that make a line into some other type.   ReadAheadLineReaderContext  In order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.  This is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.", 
            "title": "AbstractFSReadAheadLineReader"
        }, 
        {
            "location": "/operators/block_reader/#abstractfslinereader", 
            "text": "Similar to  AbstractFSReadAheadLineReader , even this parses lines from a block. However, it binds the  readerContext  field to an instance of  ReaderContext.LineReaderContext .  LineReaderContext  This handles the line split differently from  ReadAheadLineReaderContext . It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.  When the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.  If the validations of completeness fails for a line then  convertToRecord(byte[] bytes)  should return null.", 
            "title": "AbstractFSLineReader"
        }, 
        {
            "location": "/operators/block_reader/#fsslicereader", 
            "text": "A concrete extension of  AbstractFSBlockReader  that reads fixed-size  byte[]  from a block and emits the byte array wrapped in  com.datatorrent.netlet.util.Slice .  This operator binds the  readerContext  to an instance of  ReaderContext.FixedBytesReaderContext .  FixedBytesReaderContext  This implementation of  ReaderContext  never reads beyond a block boundary which can result in the last  byte[]  of a block to be of a shorter length than the rest of the records.  Configuration  readerContext.length : length of each record. By default, this is initialized to the default hdfs block size.", 
            "title": "FSSliceReader"
        }, 
        {
            "location": "/operators/block_reader/#partitioner-and-statslistener", 
            "text": "The logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute -  PARTITIONER ) as well as a StatsListener. This is because the  AbstractBlockReader  implements both the  com.datatorrent.api.Partitioner  and  com.datatorrent.api.StatsListener  interfaces and provides an implementation of  definePartitions(...)  and  processStats(...)  which make it auto-scalable.  processStats   The application master invokes  Response processStats(BatchedOperatorStats stats)  method on the logical instance with the stats ( tuplesProcessedPSMA ,  tuplesEmittedPSMA ,  latencyMA , etc.) of each partition. The data which this operator is interested in is the  queueSize  of the input port  blocksMetadataInput .  Usually the  queueSize  of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with  @DataQueueSize . In this case  AbstractBlockReader  itself is the  StatsListener  which is why it is annotated with  @DataQueueSize .  The logical instance caches the queue size per partition and at regular intervals (configured by  intervalMillis ) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.   The goal of this logic is to create as many partitions within bounds (see  maxReaders  and  minReaders  above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.  definePartitions  Based on the  repartitionRequired  field of the  Response  object which is returned by  processStats  method, the application master invokes   Collection Partition AbstractBlockReader ...  definePartitions(Collection Partition AbstractBlockReader ...  partitions, PartitioningContext context)  on the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created.   Please note auto-scaling can be disabled by setting  collectStats  to  false . If the use-case requires only static partitioning, then that can be achieved by setting  StatelessPartitioner  as the operator attribute-  PARTITIONER  on the block reader.", 
            "title": "Partitioner and StatsListener"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "File Input, S3, NFS"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/operators/file_output/", 
            "text": "AbstractFileOutputOperator\n\n\nThe abstract file output operator in Apache Apex Malhar library \n \nAbstractFileOutputOperator\n writes streaming data to files. The main features of this operator are:\n\n\n\n\nPersisting data to files.\n\n\nAutomatic rotation of files based on:\n\n  a. maximum length of a file.\n\n  b. time-based rotation where time is specified using a count of application windows.\n\n\nFault-tolerance.\n\n\nCompression and encryption of data before it is persisted.\n\n\n\n\nIn this tutorial we will cover the details of the basic structure and implementation of all the above features in \nAbstractFileOutputOperator\n. Configuration items related to each feature are discussed as they are introduced in the section of that feature.\n\n\nPersisting data to files\n\n\nThe principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:\n\n\nfilePath\n: path specifying the directory where files are written.\n\n\nDifferent types of file system that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The file system instance which is used for creating streams is constructed from the \nfilePath\n URI.\n\n\nFileSystem.newInstance(new Path(filePath).toUri(), new Configuration())\n\n\n\n\nTuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.\n\n\nPorts\n\n\n\n\ninput\n: the input port on which tuples to be persisted are received.\n\n\n\n\nstreamsCache\n\n\nThis transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.\n\n\nstreamsCache\n is of type \ncom.google.common.cache.LoadingCache\n. A \nLoadingCache\n has an attached \nCacheLoader\n which is responsible to load value of a key when the key is not present in the cache. Details are explained here- \nCachesExplained\n.\n\n\nThe operator constructs this cache in \nsetup(...)\n. It is built with the following configuration items:\n\n\n\n\nmaxOpenFiles\n: maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit. \nDefault\n: 100\n\n\nexpireStreamAfterAcessMillis\n: expires streams after the specified duration has passed since the stream was last accessed. \nDefault\n: value of attribute- \nOperatorContext.SPIN_MILLIS\n.\n\n\n\n\nAn important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.\n\n\nCacheLoader\n\n\nstreamsCache\n is created with a \nCacheLoader\n that opens an \nFSDataOutputStream\n for a file which is not in the cache. The output stream is opened in either \nappend\n or \ncreate\n mode and the basic logic to determine this is explained by the simple diagram below.\n\n\n\n\nThis process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.\n\n\nFollowing are few configuration items used for opening the streams:\n\n\n\n\nreplication\n: specifies the replication factor of the output files. \nDefault\n: \nfs.getDefaultReplication(new Path(filePath))\n\n\nfilePermission\n: specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command. \nDefault\n: 0777\n\n\n\n\nRemovalListener\n\n\nA \nGuava\n cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since \nstreamsCache\n is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to \nstreamsCache\n which closes the stream when it is evicted.\n\n\nsetup(OperatorContext context)\n\n\nDuring setup the following main tasks are performed:\n\n\n\n\nFileSystem instance is created.\n\n\nThe cache of streams is created.\n\n\nFiles are recovered (see Fault-tolerance section).\n\n\nStray part files are cleaned (see Automatic rotation section).\n\n\n\n\nprocessTuple(INPUT tuple)\n\n\nThe code snippet below highlights the basic steps of processing a tuple.\n\n\nprotected void processTuple(INPUT tuple)\n{  \n  //which file to write to is derived from the tuple.\n  String fileName = getFileName(tuple);  \n\n  //streamsCache is queried for the output stream. If the stream is already opened then it is returned immediately otherwise the cache loader creates one.\n  FilterOutputStream fsOutput = streamsCache.get(fileName).getFilterStream();\n\n  byte[] tupleBytes = getBytesForTuple(tuple);\n\n  fsOutput.write(tupleBytes);\n}\n\n\n\n\nendWindow()\n\n\nIt should be noted that while processing a tuple we do not flush the stream after every write. Since flushing is expensive it is done periodically for all the open streams in the operator's \nendWindow()\n.\n\n\nMap\nString, FSFilterStreamContext\n openStreams = streamsCache.asMap();\nfor (FSFilterStreamContext streamContext: openStreams.values()) {\n  ...\n  //this flushes the stream\n  streamContext.finalizeContext();\n  ...\n}\n\n\n\n\nFSFilterStreamContext\n will be explained with compression and encryption.\n\n\nteardown()\n\n\nWhen any operator in a DAG fails then the application master invokes \nteardown()\n for that operator and its downstream operators. In \nAbstractFileOutputOperator\n we have a bunch of open streams in the cache and the operator (acting as HDFS client) holds leases for all the corresponding files. It is important to release these leases for clean re-deployment. Therefore, we try to close all the open streams in \nteardown()\n.\n\n\nAutomatic rotation\n\n\nIn a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.\n\n\nTo help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.\n\n\nPart filename\n\n\nThe filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,\n\n\norigfile.partnum\n\n\nThis naming scheme can be changed by the user. It can be done so by overriding the following method\n\n\nprotected String getPartFileName(String fileName, int part)\n\n\n\n\nThis method is passed the original filename and part number as arguments and should return the part filename.\n\n\nMechanisms\n\n\nThe user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.\n\n\nSize Based\n\n\nWith size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property\n\n\nmaxLength\n\n\nLike any other property this can be set in Java application code or in the property file.\n\n\nTime Based\n\n\nIn time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property\n\n\nrotationWindows\n\n\nsetup(OperatorContext context)\n\n\nWhen an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process\n\n\n\n\nFault-tolerance\n\n\nThere are two issues that should be addressed in order to make the operator fault-tolerant:\n\n\n\n\n\n\nThe operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.\n\n\n\n\n\n\nWhile writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:  \n\n\n\n\nalwaysWriteToTmp\n: enables/disables writing to a temporary file. \nDefault\n: true.\n\n\n\n\nMost of the complexity in the code comes from making this operator fault-tolerant.\n\n\nCheckpointed states needed for fault-tolerance\n\n\n\n\n\n\nendOffsets\n: contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator \nsetup(...)\n and is also used while loading a stream to find out if the operator has seen a file before.\n\n\n\n\n\n\nfileNameToTmpName\n: contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.\n\n\n\n\n\n\nfinalizedFiles\n: contains set of files which were requested to be finalized per window id.\n\n\n\n\n\n\nfinalizedPart\n: contains the latest \npart\n of each file which was requested to be finalized.\n\n\n\n\n\n\nThe use of \nfinalizedFiles\n and \nfinalizedPart\n are explained in detail under \nrequestFinalize(...)\n method.\n\n\nRecovering files\n\n\nWhen the operator is re-deployed, it checks in its \nsetup(...)\n method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the \nendOffsets\n. When it doesn't the operator truncates the file.\n\n\nFor example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.\n\n\nrequestFinalize(String fileName)\n\n\nWhen the operator is always writing to temporary files (in order to avoid HDFS Lease exceptions), then it is necessary to rename the temporary files to the actual files once it has been determined that the files are closed. This is refered to as \nfinalization\n of files and the method allows the user code to specify when a file is ready for finalization.\n\n\nIn this method, the requested file (or in the case of rotation \n all the file parts including the latest open part which have not yet been requested for finalization) are registered for finalization. Registration is basically adding the file names to \nfinalizedFiles\n state and updating \nfinalizedPart\n.\n\n\nThe process of \nfinalization\n of all the files which were requested till the window \nw\n is deferred till window \nw\n is committed. This is because until a window is committed it can be replayed after a failure which means that a file can be open for writing even after it was requested for finalization.\n\n\nWhen rotation is enabled, part files as and when they get completed are requested for finalization. However, when rotation is not enabled user code needs to invoke this method as the knowledge that when a file is closed is unknown to this abstract operator.", 
            "title": "File Output"
        }, 
        {
            "location": "/operators/file_output/#abstractfileoutputoperator", 
            "text": "The abstract file output operator in Apache Apex Malhar library    AbstractFileOutputOperator  writes streaming data to files. The main features of this operator are:   Persisting data to files.  Automatic rotation of files based on: \n  a. maximum length of a file. \n  b. time-based rotation where time is specified using a count of application windows.  Fault-tolerance.  Compression and encryption of data before it is persisted.   In this tutorial we will cover the details of the basic structure and implementation of all the above features in  AbstractFileOutputOperator . Configuration items related to each feature are discussed as they are introduced in the section of that feature.", 
            "title": "AbstractFileOutputOperator"
        }, 
        {
            "location": "/operators/file_output/#persisting-data-to-files", 
            "text": "The principal function of this operator is to persist tuples to files efficiently. These files are created under a specific directory on the file system. The relevant configuration item is:  filePath : path specifying the directory where files are written.  Different types of file system that are implementations of  org.apache.hadoop.fs.FileSystem  are supported. The file system instance which is used for creating streams is constructed from the  filePath  URI.  FileSystem.newInstance(new Path(filePath).toUri(), new Configuration())  Tuples may belong to different files therefore expensive IO operations like creating multiple output streams, flushing of data to disk, and closing streams are handled carefully.  Ports   input : the input port on which tuples to be persisted are received.   streamsCache  This transient state caches output streams per file in memory. The file to which the data is appended may change with incoming tuples. It will be highly inefficient to keep re-opening streams for a file just because tuples for that file are interleaved with tuples for another file. Therefore, the operator maintains a cache of limited size with open output streams.  streamsCache  is of type  com.google.common.cache.LoadingCache . A  LoadingCache  has an attached  CacheLoader  which is responsible to load value of a key when the key is not present in the cache. Details are explained here-  CachesExplained .  The operator constructs this cache in  setup(...) . It is built with the following configuration items:   maxOpenFiles : maximum size of the cache. The cache evicts entries that haven't been used recently when the cache size is approaching this limit.  Default : 100  expireStreamAfterAcessMillis : expires streams after the specified duration has passed since the stream was last accessed.  Default : value of attribute-  OperatorContext.SPIN_MILLIS .   An important point to note here is that the guava cache does not perform cleanup and evict values asynchronously, that is, instantly after a value expires. Instead, it performs small amounts of maintenance during write operations, or during occasional read operations if writes are rare.  CacheLoader  streamsCache  is created with a  CacheLoader  that opens an  FSDataOutputStream  for a file which is not in the cache. The output stream is opened in either  append  or  create  mode and the basic logic to determine this is explained by the simple diagram below.   This process gets complicated when fault-tolerance (writing to temporary files)  and rotation is added.  Following are few configuration items used for opening the streams:   replication : specifies the replication factor of the output files.  Default :  fs.getDefaultReplication(new Path(filePath))  filePermission : specifies the permission of the output files. The permission is an octal number similar to that used by the Unix chmod command.  Default : 0777   RemovalListener  A  Guava  cache also allows specification of removal listener which can perform some operation when an entry is removed from the cache. Since  streamsCache  is of limited size and also has time-based expiry enabled, it is imperative that when a stream is evicted from the cache it is closed properly. Therefore, we attach a removal listener to  streamsCache  which closes the stream when it is evicted.  setup(OperatorContext context)  During setup the following main tasks are performed:   FileSystem instance is created.  The cache of streams is created.  Files are recovered (see Fault-tolerance section).  Stray part files are cleaned (see Automatic rotation section).   processTuple(INPUT tuple)  The code snippet below highlights the basic steps of processing a tuple.  protected void processTuple(INPUT tuple)\n{  \n  //which file to write to is derived from the tuple.\n  String fileName = getFileName(tuple);  \n\n  //streamsCache is queried for the output stream. If the stream is already opened then it is returned immediately otherwise the cache loader creates one.\n  FilterOutputStream fsOutput = streamsCache.get(fileName).getFilterStream();\n\n  byte[] tupleBytes = getBytesForTuple(tuple);\n\n  fsOutput.write(tupleBytes);\n}  endWindow()  It should be noted that while processing a tuple we do not flush the stream after every write. Since flushing is expensive it is done periodically for all the open streams in the operator's  endWindow() .  Map String, FSFilterStreamContext  openStreams = streamsCache.asMap();\nfor (FSFilterStreamContext streamContext: openStreams.values()) {\n  ...\n  //this flushes the stream\n  streamContext.finalizeContext();\n  ...\n}  FSFilterStreamContext  will be explained with compression and encryption.  teardown()  When any operator in a DAG fails then the application master invokes  teardown()  for that operator and its downstream operators. In  AbstractFileOutputOperator  we have a bunch of open streams in the cache and the operator (acting as HDFS client) holds leases for all the corresponding files. It is important to release these leases for clean re-deployment. Therefore, we try to close all the open streams in  teardown() .", 
            "title": "Persisting data to files"
        }, 
        {
            "location": "/operators/file_output/#automatic-rotation", 
            "text": "In a streaming application where data is being continuously processed, when this output operator is used, data will be continuously written to an output file. The users may want to be able to take the data from time to time to use it, copy it out of Hadoop or do some other processing. Having all the data in a single file makes it difficult as the user needs to keep track of how much data has been read from the file each time so that the same data is not read again. Also users may already have processes and scripts in place that work with full files and not partial data from a file.  To help solve these problems the operator supports creating many smaller files instead of writing to just one big file. Data is written to a file and when some condition is met the file is finalized and data is written to a new file. This is called file rotation. The user can determine when the file gets rotated. Each of these files is called a part file as they contain portion of the data.  Part filename  The filename for a part file is formed by using the original file name and the part number. The part number starts from 0 and is incremented each time a new part file created. The default filename has the format, assuming origfile represents the original filename and partnum represents the part number,  origfile.partnum  This naming scheme can be changed by the user. It can be done so by overriding the following method  protected String getPartFileName(String fileName, int part)  This method is passed the original filename and part number as arguments and should return the part filename.  Mechanisms  The user has a couple of ways to specify when a file gets rotated. First is based on size and second on time. In the first case the files are limited by size and in the second they are rotated by time.  Size Based  With size based rotation the user specifies a size limit. Once the size of the currently file reaches this limit the file is rotated. The size limit can be specified by setting the following property  maxLength  Like any other property this can be set in Java application code or in the property file.  Time Based  In time based rotation user specifies a time interval. This interval is specified as number of application windows. The files are rotated periodically once the specified number of application windows have elapsed. Since the interval is application window based it is not always exactly constant time. The interval can be specified using the following property  rotationWindows  setup(OperatorContext context)  When an operator is being started there may be stray part files and they need to be cleaned up. One common scenario, when these could be present, is in the case of failure, where a node running the operator failed and a previous instance of the operator was killed. This cleanup and other initial processing for the part files happens in the operator setup. The following diagram describes this process", 
            "title": "Automatic rotation"
        }, 
        {
            "location": "/operators/file_output/#fault-tolerance", 
            "text": "There are two issues that should be addressed in order to make the operator fault-tolerant:    The operator flushes data to the filesystem every application window. This implies that after a failure when the operator is re-deployed and tuples of a window are replayed, then duplicate data will be saved to the files. This is handled by recording how much the operator has written to each file every window in a state that is checkpointed and truncating files back to the recovery checkpoint after re-deployment.    While writing to HDFS, if the operator gets killed and didn't have the opportunity to close a file, then later when it is redeployed it will attempt to truncate/restore that file. Restoring a file may fail because the lease that the previous process (operator instance before failure) had acquired from namenode to write to a file may still linger and therefore there can be exceptions in acquiring the lease again by the new process (operator instance after failure). This is handled by always writing data to temporary files and renaming these files to actual files when a file is finalized (closed) for writing, that is, we are sure that no more data will be written to it. The relevant configuration item is:     alwaysWriteToTmp : enables/disables writing to a temporary file.  Default : true.   Most of the complexity in the code comes from making this operator fault-tolerant.  Checkpointed states needed for fault-tolerance    endOffsets : contains the size of each file as it is being updated by the operator. It helps the operator to restore a file during recovery in operator  setup(...)  and is also used while loading a stream to find out if the operator has seen a file before.    fileNameToTmpName : contains the name of the temporary file per actual file. It is needed because the name of a temporary file is random. They are named based on the timestamp when the stream is created. During recovery the operator needs to know the temp file which it was writing to and if it needs restoration then it creates a new temp file and updates this mapping.    finalizedFiles : contains set of files which were requested to be finalized per window id.    finalizedPart : contains the latest  part  of each file which was requested to be finalized.    The use of  finalizedFiles  and  finalizedPart  are explained in detail under  requestFinalize(...)  method.  Recovering files  When the operator is re-deployed, it checks in its  setup(...)  method if the state of a file which it has seen before the failure is consistent with the file's state on the file system, that is, the size of the file on the file system should match the size in the  endOffsets . When it doesn't the operator truncates the file.  For example, let's say the operator wrote 100 bytes to test1.txt by the end of window 10. It wrote another 20 bytes by the end of window 12 but failed in window 13. When the operator gets re-deployed it is restored with window 10 (recovery checkpoint) state. In the previous run, by the end of window 10, the size of file on the filesystem was 100 bytes but now it is 120 bytes. Tuples for windows 11 and 12 are going to be replayed. Therefore, in order to avoid writing duplicates to test1.txt, the operator truncates the file to 100 bytes (size at the end of window 10) discarding the last 20 bytes.  requestFinalize(String fileName)  When the operator is always writing to temporary files (in order to avoid HDFS Lease exceptions), then it is necessary to rename the temporary files to the actual files once it has been determined that the files are closed. This is refered to as  finalization  of files and the method allows the user code to specify when a file is ready for finalization.  In this method, the requested file (or in the case of rotation   all the file parts including the latest open part which have not yet been requested for finalization) are registered for finalization. Registration is basically adding the file names to  finalizedFiles  state and updating  finalizedPart .  The process of  finalization  of all the files which were requested till the window  w  is deferred till window  w  is committed. This is because until a window is committed it can be replayed after a failure which means that a file can be open for writing even after it was requested for finalization.  When rotation is enabled, part files as and when they get completed are requested for finalization. However, when rotation is not enabled user code needs to invoke this method as the knowledge that when a file is closed is unknown to this abstract operator.", 
            "title": "Fault-tolerance"
        }, 
        {
            "location": "/operators/deduper/", 
            "text": "Dedup Operator\n\n\nThis document is intended as a guide for understanding and using the\nDedup operator/module.\n\n\nDedup - \u201cWhat\u201d in a Nutshell\n\n\nDedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.\n\n\n\n\nDedup - \u201cHow\u201d in a Nutshell\n\n\nIn order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.\n\n\n\n\nFollowing are the different components of the Deduper Operator\n\n\n\n\nDedup Operator\n - This is responsible for the overall functionality\n    of the operator. This in turn makes use of other components to\n    establish the end goal of deciding whether a tuple is a duplicate of\n    some earlier tuple, or is a unique tuple.\n\n\nBucket Store\n - This is responsible for storing the unique tuples as\n    supplied by the Deduper and storing them into Buckets in HDFS.\n\n\nBucket Manager\n - Since, all of the data cannot be stored in memory,\n    this component is responsible for loading and unloading of the\n    buckets to and from the memory as requested by the Deduper.\n\n\n\n\nThis was a very small introduction to the functioning of the Deduper.\nFollowing sections will go into more detail on each of the components.\n\n\nUse case - Basic Dedup\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.\n\n\nConsider an example schema and two sample tuples\n\n\n{Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}\n\n\n\n\nLet us assume that the Dedup Key is\n\n\n{Name, Phone}\n\n\n\n\nIn this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as\n\n\n{Phone, Email}\n\n\n\n\nthen in this case, the two are unique tuples as the keys of both tuples\ndo not match.\n\n\nUse case Details\n\n\nConsider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.\n\n\n{Name, Phone, Email, Date, City, Zip, Country}\n\n\n\n\nAlso consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.\n\n\nAs part of configuring the operator for this use case, we need to set\nthe following parameters:\n\n\nDedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to\n\n\n{Name,Email}\n\n\n\n\nThe above configuration is sufficient to resolve the use case.\n\n\n\n\nUse case - Dedup with Expiry\n\n\nMotivation\n\n\nThe Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.\n\n\nExpiry Key\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.\n\n\nUse case Details\n\n\nConsider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.\n\n\nThe expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.\n\n\nConfiguring the below parameters will solve the problem for this use\ncase:\n\n\n\n\nDedup Key\n - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.\n\n\nExpiry Key\n - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.\n\n\nExpiry Period\n - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.\n\n\n\n\nConfiguration of these parameters would resolve this use case.\n\n\nUse cases - Summary\n\n\n\n\nBasic Dedup\n - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.\n\n\nDedup with Expiry\n - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:\n\n\nTime based\n - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time\n\n\nWith respect to system time\n - Time progresses with system time. Any expiry criterions are executed with this notion of system time.\n\n\nWith respect to tuple time\n - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.\n\n\nAny Ordered Key\n - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.\n\n\nCategorical Key\n - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.\n\n\n\n\n\n\nTechnical Architecture\n\n\nBlock Diagram\n\n\n\n\nThe deduper has a single input port and multiple output ports.\n\n\n\n\ninput\n - This is the input port through which the tuples arrive at\n    the Deduper.\n\n\nunique\n\u00a0- This is the output port on which unique tuples are sent out\n    by the Deduper.\n\n\nduplicate\n\u00a0- This is the output port on which duplicate tuples are\n    sent out by the Deduper.\n\n\nexpired\n\u00a0- This is the output port on which expired tuples are sent\n    out by the Deduper.\n\n\nerror\n\u00a0- This is the output port on which the error tuples are sent\n    out by the Deduper.\n\n\n\n\n\n\nConcepts\n\n\nDedup Key\n\n\nA dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.\n\n\nExpiry Key\n\n\nA tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.\n\n\nAt the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.\n\n\nIn such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.\n\n\nIn order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.\n\n\nThe easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.\n\n\n\n\nThe domain of the key must be ordered. Example - Timestamp field\n\n\nThe domain of the key must be categorical and sorted. Example - City\n    names grouped together\n\n\n\n\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.\n\n\nExpiry Period\n\n\nThe Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.\n\n\nTime Points\n\n\nFor every dataset that the deduper processes, it maintains a set of time\npoints:\n\n\n\n\nLatest Point\n\u00a0- This is the maximum time point observed in all the\n    processed tuples.\n\n\nExpiry Point\n\u00a0- This is given by: \nExpiry Point = Latest Point -\n    Expiry Period\n\n\n\n\nThese points help the deduper to make decisions related to expiry of a\ntuple.\n\n\nExample - Expiry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTuple Id\n\n\nExpiry Key\n\n\n(Expiry Period = 10)\n\n\nLatest Point\n\n\nExpiry Point\n\n\nDecision for Tuple\n\n\n\n\n\n\n1\n\n\n10\n\n\n10\n\n\n1\n\n\nNot Expired\n\n\n\n\n\n\n2\n\n\n20\n\n\n20\n\n\n11\n\n\nNot Expired\n\n\n\n\n\n\n3\n\n\n25\n\n\n25\n\n\n16\n\n\nNot Expired\n\n\n\n\n\n\n4\n\n\n40\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n5\n\n\n21\n\n\n40\n\n\n31\n\n\nExpired\n\n\n\n\n\n\n6\n\n\n35\n\n\n40\n\n\n31\n\n\nNot Expired\n\n\n\n\n\n\n7\n\n\n45\n\n\n45\n\n\n36\n\n\nNot Expired\n\n\n\n\n\n\n8\n\n\n57\n\n\n57\n\n\n48\n\n\nNot Expired\n\n\n\n\n\n\n\n\n\nBuckets\n\n\nOne of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.\n\n\nBucket Span\n\n\nBucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.\n\n\nBucket Key\n\n\nBucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.\n\n\nWe define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.\n\n\nIn case of Basic Dedup:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with Expiry:\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nNumber of Buckets\n\n\nThe number of buckets can be given by\n\n\nNum Buckets = Expiry Period / Bucket Span.\n\n\n\n\nThis is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.\n\n\nBucket Index\n\n\nA Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).\n\n\nExample - Buckets\n\n\n\n\nAssumptions\n\n\nAssumption 1\n\n\nThis assumption is only applicable in case of Dedup with Expiry.\n\n\nFor any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2\n\n\n\n\nIn other words, there may never be\u00a0two tuples t1 and t2 such that\n\n\nTuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2\n\n\n\n\nThis assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.\n\n\nThe reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.\n\n\nFlow of a Tuple through Dedup Operator\n\n\nTuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.\n\n\nDeduper View\n\n\nA tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.\n\n\nIdentify Bucket Key\n\n\nIdentify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:\n\n\nBucket Key = Hash(Dedup Key) % Number of Buckets\n\n\n\n\nIn case of Dedup with expiry, we calculate the Bucket key as\n\n\nBucket Key = Expiry Key / Bucket Span\n\n\n\n\nCheck if tuple is Expired\n\n\nThis is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.\n\n\nif ( Latest Point - Expiry Key \n Expiry Point ) then Expired\n\n\n\n\nIf the tuple is expired, then put it out to the expired port.\n\n\nCheck if tuple is a Duplicate or Unique\n\n\nOnce a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed \nhere\n.\n\n\nDuplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the \nfirst\nstep\n. The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.\n\n\nCase I - Bucket available in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.\n\n\nCase II - Bucket not in memory\n\n\nIn case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed\n\nhere\n. Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.\n\n\nHandling tuples after Buckets are loaded\n\n\nThe Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:\n\n\n\n\nThe bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.\n\n\nThe bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.\n\n\n\n\nAfter processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section \nCase II\n- Bucket not in memory\n. For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for \nBuckets available in memory\n.\n\n\nBucket Manager\n\n\nBucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.\n\n\nIdentify Bucket Index\n\n\nBucket index is discussed \nhere\n. Bucket index can be\ncalculated as follows:\n\n\nBucket Index = Requested Bucket Key % Number of Buckets,\n\n\n\n\nwhere Number of Buckets is as defined \nhere\n.\n\n\nRequest Bucket Load from Store\n\n\nOnce we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned \nhere\n. We detail the process of fetching the\nbucket data from the store in \nthis\n\u00a0section.\n\n\nBucket Eviction\n\n\nIt may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.\n\n\nBucket Store\n\n\nThe Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.\n\n\nData Format\n\n\nBucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.\n\n\n\n\nAll the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the \nData Structures\n\u00a0section.\n\n\nData Structures\n\n\nHDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.\n\n\n\n\nBucket Positions\n - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.\n\n\nWindow To Buckets\n - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.\n\n\nWindow to Timestamp\n - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.\n\n\n\n\nBucket Fetch\n\n\nFor fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Deduper"
        }, 
        {
            "location": "/operators/deduper/#dedup-operator", 
            "text": "This document is intended as a guide for understanding and using the\nDedup operator/module.", 
            "title": "Dedup Operator"
        }, 
        {
            "location": "/operators/deduper/#dedup-what-in-a-nutshell", 
            "text": "Dedup is actually a shortened form of Deduplication. Duplicates are\nomnipresent and can be found in almost any kind of data. Most of the\ntimes it is essential to discard, or at the very least separate out the\ndata into unique\u00a0and duplicate\u00a0components. The entire purpose of this\noperator is to de-duplicate data. In other words, when data passes\nthrough this operator, it will be segregated into two different data\nsets, one containing all unique tuples, and the other containing duplicates.", 
            "title": "Dedup - \u201cWhat\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#dedup-how-in-a-nutshell", 
            "text": "In order to quickly decide whether an incoming tuple is duplicate or\nunique, it has to store each incoming tuple (or a signature, like key\nfor example) to be used for comparison later. A plain storage for such a\nhuge data is hardly scalable. Deduper employs a large scale distributed\nhashing mechanism (known as the Bucket Store) which allows it to\nidentify if a particular tuple is duplicate or unique. Each time it\nidentifies a tuple as a unique tuple, it also stores it into a\npersistent store called the Bucket Store for lookup in the future.", 
            "title": "Dedup - \u201cHow\u201d in a Nutshell"
        }, 
        {
            "location": "/operators/deduper/#use-case-basic-dedup", 
            "text": "", 
            "title": "Use case - Basic Dedup"
        }, 
        {
            "location": "/operators/deduper/#dedup-key", 
            "text": "A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates.  Consider an example schema and two sample tuples  {Name, Phone, Email, Date, State, Zip, Country}\n\nTuple 1:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  ausaunders@semperegestasurna.com,\n  2015-11-09 13:38:38,\n  Texas,\n  73301,\n  United States\n}\n\nTuple 2:\n{\n  Austin U. Saunders,\n  +91-319-340-59385,\n  austin@semperegestasurna.com,\n  2015-11-09 13:39:38,\n  Texas,\n  73301,\n  United States\n}  Let us assume that the Dedup Key is  {Name, Phone}  In this case, the two\ntuples are duplicates because the key fields are same in both the\ntuples. However, if we plan to make the Dedup Key as  {Phone, Email}  then in this case, the two are unique tuples as the keys of both tuples\ndo not match.", 
            "title": "Dedup Key"
        }, 
        {
            "location": "/operators/deduper/#use-case-details", 
            "text": "Consider the case of de-duplicating a master data set which is stored in\na file. Further also consider the following schema for tuples in the\ndata set.  {Name, Phone, Email, Date, City, Zip, Country}  Also consider that we need to identify unique customers from the master\ndata set. So, ultimately the output needed for the use case is two data\nsets - Unique Records\u00a0and Duplicate Records.  As part of configuring the operator for this use case, we need to set\nthe following parameters:  Dedup Key - This can be set as the primary key which can be used to uniquely identify a Customer. For example, we can set it to  {Name,Email}  The above configuration is sufficient to resolve the use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-case-dedup-with-expiry", 
            "text": "", 
            "title": "Use case - Dedup with Expiry"
        }, 
        {
            "location": "/operators/deduper/#motivation", 
            "text": "The Basic Dedup use case is the most straightforward and is usually\napplied when the amount of data to be processed is not huge. However, if\nthe incoming data is huge, or even never-ending, it is usually not\nnecessary to keep storing all the data. This is because in most real\nworld use cases, the duplicates occur only a short distance apart.\nHence, after a while, it is usually okay to forget the part of the\nhistory and consider only limited history for identifying duplicates, in\nthe interest of efficiency. In other words, we expire some tuples which\nare (or were supposed to be) delivered long back. Doing so, reduces the\nload on the Bucket Store which effectively deletes part of the history,\nthus making the whole process more efficient. We call this use case,\nDedup with expiry.", 
            "title": "Motivation"
        }, 
        {
            "location": "/operators/deduper/#expiry-key", 
            "text": "The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a natural expiry key and is in line\nwith the concept of expiry. Formally, an expiry field is a field in the\ninput tuple which can be used to discard incoming tuples as expired.\nThis expiry key usually works with another parameter called Expiry\nPeriod defined next.", 
            "title": "Expiry Key"
        }, 
        {
            "location": "/operators/deduper/#expiry-period", 
            "text": "The expiry period is the value supplied by the user to define the extent\nof history which should be considered while expiring tuples.", 
            "title": "Expiry Period"
        }, 
        {
            "location": "/operators/deduper/#use-case-details_1", 
            "text": "Consider an incoming stream of system logs. The use case requires us to\nidentify duplicate log messages and pass on only the unique ones.\nAnother relaxation in the use case is that the log messages which are\nolder than a day, may not be considered and must be filtered out as\nexpired. The expiry must be measured with respect to the time stamp in\nthe logs. For example, if the timestamp in the incoming message is\n\u201c30-12-2014 00:00:00\u201d and the latest message that the system has\nencountered had the time stamp \u201c31-12-2014 00:00:00\u201d, then the incoming\nmessage must be considered as expired. However, if the incoming message\nhad any timestamp like \u201c30-12-2014 00:11:00\u201d, it must be accepted into\nthe system and check for a possible duplicate.  The expiry facet in the use case above gives us an advantage in that we\ndo not have to compare the incoming record with all the data to check if\nit is a duplicate. At the same time, all the data need not be stored.\nJust a day worth of data needs to be stored in order to address the\nabove use case.  Configuring the below parameters will solve the problem for this use\ncase:   Dedup Key  - This is the dedup key for the incoming tuples (similar\n    to the Basic Dedup use case). This can be any key which can uniquely\n    identify a record. For log messages this can be a serial number\n    attached in the log.  Expiry Key  - This is the key which can help identify the expired\n    records, as explained above. In this particular use case, it can be\n    a timestamp field which indicates when the log message was\n    generated.  Expiry Period  - This is the period of expiry as explained above. In\n    our particular use case this will be 24 hours.   Configuration of these parameters would resolve this use case.", 
            "title": "Use case Details"
        }, 
        {
            "location": "/operators/deduper/#use-cases-summary", 
            "text": "Basic Dedup  - Deduplication of bounded datasets. Data is assumed to be bounded. This use case is not meant for never ending streams of data. For example: Deduplication of master data like customer records, product catalogs etc.  Dedup with Expiry  - Deduplication of unlimited streams of data. This use case handles unbounded streams of data and can run forever. An expiry key and criterion is expected as part of the input which helps avoid storing all the unique data. This helps speed up performance. Following expiry keys are supported:  Time based  - Timestamp fields, system date, creation date, load date etc. are examples of the fields that can be used as a time based expiry key. Additionally an option can be provided so as to maintain time with respect to System time, or Tuple time  With respect to system time  - Time progresses with system time. Any expiry criterions are executed with this notion of system time.  With respect to tuple time  - Time progresses based on the time in the incoming tuples. Expiry criterions are executed with the notion of time indicated by the incoming tuple.  Any Ordered Key  - Similar to time, any non-time field can also be used as an expiry key, provided the key is also ordered (analogous to the time field). Examples include Transaction ids, Sequence Ids etc. The expiry criterion must also be in the domain of the key.  Categorical Key  - Any categorical key can be used for expiry, provided that data is grouped by the key. Examples include City name, Circle Id etc. In case of City name, for example, the records tend to appear for City 1 first, followed by City 2, then City 3 and so on. Any out of order cities may be considered as expired based on the configuration of the expiry criterion.", 
            "title": "Use cases - Summary"
        }, 
        {
            "location": "/operators/deduper/#technical-architecture", 
            "text": "", 
            "title": "Technical Architecture"
        }, 
        {
            "location": "/operators/deduper/#block-diagram", 
            "text": "", 
            "title": "Block Diagram"
        }, 
        {
            "location": "/operators/deduper/#concepts", 
            "text": "Dedup Key  A dedup key is a set of one or more fields in the data tuple which acts\nas the key\u00a0for the tuples. This is used by the deduper to compare tuples\nto arrive at the conclusion on whether two tuples are duplicates. If\nDedup Key of two tuples match, then they are duplicates, else they are\nunique.  Expiry Key  A tuple may or may not have an Expiry Key. Dedup operator cannot keep\nstoring all the data that is flowing into the operator. At some point it\nbecomes essential to discard some of the historical tuples in interest\nof memory and efficiency.  At the same time, tuples are expected to arrive at the Dedup operator\nwithin some time after they are generated. After this time, the tuples\nmay be considered as stale or obsolete.  In such cases, the Deduper chooses to consider these tuples expired\u00a0and\ntakes no action but to separate out these tuples on a different port in\norder to be processed by some other operator or offline analysis.  In order to create a criterion for discarding such tuples, we introduce\nan Expiry Key. Looking at the value of the Expiry Key in each tuple, we\ncan decide whether or not to discard this tuple as expired.  The easiest way to understand this use case is to consider time\u00a0as the\ncriteria of expiring tuples. Time\u00a0is a very good and general example of\nan expiry key and is in line with the concept of expiry. Formally, an\nexpiry field is a field in the input tuple which can be used to discard\nincoming tuples as expired. There are some criteria for a field to be\nconsidered an Expiry Field. At-least one\u00a0of the following must hold for\nan Expiry Key.   The domain of the key must be ordered. Example - Timestamp field  The domain of the key must be categorical and sorted. Example - City\n    names grouped together   This expiry key usually works with another parameter called Expiry\nPeriod defined next.  Expiry Period  The Expiry Period is the value supplied by the user which decides when a\nparticular tuple expires.  Time Points  For every dataset that the deduper processes, it maintains a set of time\npoints:   Latest Point \u00a0- This is the maximum time point observed in all the\n    processed tuples.  Expiry Point \u00a0- This is given by:  Expiry Point = Latest Point -\n    Expiry Period   These points help the deduper to make decisions related to expiry of a\ntuple.  Example - Expiry           Tuple Id  Expiry Key  (Expiry Period = 10)  Latest Point  Expiry Point  Decision for Tuple    1  10  10  1  Not Expired    2  20  20  11  Not Expired    3  25  25  16  Not Expired    4  40  40  31  Not Expired    5  21  40  31  Expired    6  35  40  31  Not Expired    7  45  45  36  Not Expired    8  57  57  48  Not Expired     Buckets  One of the requirements of the Deduper is to store all the unique tuples\n(actually, just the dedup keys of tuples). Keeping an ever growing cache\nin memory is not scalable. So what we need was a limited cache backed by\na persistent store. Now to reduce cache misses, we load a chunk of data\n(called Buckets), together into memory. Buckets help narrow down the\nsearch of duplicates for incoming tuples. A Bucket is an abstraction for\na collection of tuples all of which share a common hash value based on\nsome hash function. A bucket is identified using a Bucket Key, defined\nbelow. A Bucket\u00a0has a span called Bucket Span.  Bucket Span  Bucket span is simply the range of the domain that is covered by the\nBucket. This span is specified in the domain of the Expiry key. If the\nExpiry Key is time, \u00a0then the Bucket span will be specified in seconds.\nIt is only defined in case tuples have an Expiry Key.  Bucket Key  Bucket Key acts as the identifier for a Bucket. It is derived using the\nDedup Key or Expiry Key of the tuple along with the Bucket Span.  We define Bucket Key differently in case of Basic Dedup\u00a0and Dedup with\nExpiry.  In case of Basic Dedup:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with Expiry:  Bucket Key = Expiry Key / Bucket Span  Number of Buckets  The number of buckets can be given by  Num Buckets = Expiry Period / Bucket Span.  This is because at any point of time, we need only store Expiry Period\nworth of data. As soon as we get a new tuple, we can forget about the\nleast recent tuple in our store, since this tuple will be expired due to\nthe most recent tuple.  Bucket Index  A Bucket Index iterates over number of buckets. In contrast to Bucket\nKey, which continuously keeps on increasing, Bucket Index will loop\naround to 0, once it has reached value (Number of Buckets - 1).  Example - Buckets", 
            "title": "Concepts"
        }, 
        {
            "location": "/operators/deduper/#assumptions", 
            "text": "Assumption 1  This assumption is only applicable in case of Dedup with Expiry.  For any two tuples, t1 and t2 having dedup keys d1 and d2, and expiry\nkeys e1 and e2, respectively, the following holds:\nIf d1 = d2, then e1 = e2  In other words, there may never be\u00a0two tuples t1 and t2 such that  Tuple 1: d1, e1\nTuple 2: d2, e2\nd1 = d2 and e1 != e2  This assumption was made with respect to certain use cases. These use\ncases follow this assumption in that the records which are duplicates\nare exactly identical. An example use case is when log messages are\nreplayed erroneously, and we want to identify the duplicate log\nmessages. In such cases, we need not worry about two different log\nmessages having the same identifier but different timestamps. Since its\na replay of the same data, the duplicate records are assumed to be\nexactly identical.  The reason for making this assumption was to simplify and architect the\noperator to suit only such use cases. The backend architecture for use\ncases where this assumption does not hold, is very different from the\none where this assumption holds. Hence handling the generic case could\nhave been much more complicated and inefficient.", 
            "title": "Assumptions"
        }, 
        {
            "location": "/operators/deduper/#flow-of-a-tuple-through-dedup-operator", 
            "text": "Tuples flow through the Dedup operator one by one. Deduper may choose to\nprocess a tuple immediately, or store it in some data structure for\nlater processing. We break down the processing of the Deduper by various\nstages as follows.  Deduper View  A tuple always arrives at the input port\u00a0of the Dedup operator. Once\narrived, the Deduper does the following tasks.  Identify Bucket Key  Identify the Bucket Key\u00a0of the tuple. Bucket key identifies the Bucket\nto which this tuple belongs. In case of the basic dedup use case, the\nBucket Key will be calculated as follows:  Bucket Key = Hash(Dedup Key) % Number of Buckets  In case of Dedup with expiry, we calculate the Bucket key as  Bucket Key = Expiry Key / Bucket Span  Check if tuple is Expired  This is only applicable in case of Dedup with expiry. The following\ncondition can be used to check if the tuple is expired.  if ( Latest Point - Expiry Key   Expiry Point ) then Expired  If the tuple is expired, then put it out to the expired port.  Check if tuple is a Duplicate or Unique  Once a tuple passes the check of expiry, we proceed to check if the\ntuple already has a duplicate tuple which is not expired. Note that if\nthe tuple in question is not expired, the duplicate will also not have\nexpired due to the assumption listed  here .  Duplicates of the tuple being processed, if any, will be available only\nin the bucket identified by the Bucket Key identified in the  first\nstep . The amount of physical memory available with the\nDedup operator may not be sufficient to hold all the buckets in memory.\nHence at any given point in time, only a configured maximum number of\nbuckets can be kept in memory. The Deduper follows different paths\ndepending on the availability of the required bucket in memory.  Case I - Bucket available in memory  In case the bucket with key Bucket Key\u00a0is available in memory, the\nDeduper simply checks if there is a tuple in the bucket with the same\nDedup Key as the one currently being processed. If, so, then the tuple\nbeing processed is deemed to be a duplicate and put out on the\nDuplicate\u00a0port. If not, then the tuple being processed is deemed to be\nunique and put out on the Unique\u00a0port. If the tuple is unique,\nadditionally it is also added to the bucket for future references.  Case II - Bucket not in memory  In case the bucket with key Bucket Key\u00a0is not available in memory, the\nDeduper requests the Bucket Manager to load the bucket with key Bucket\nKey\u00a0from the Bucket Store. This request is processed by the Bucket\nManager in a separate asynchronous thread as detailed here . Additionally the Deduper also inserts the tuple\nbeing processed into a waiting events\u00a0queue for later processing. After\nthis, the Deduper cannot proceed until the bucket is loaded by the\nBucket Manager and hence proceeds to process another tuple. Next section\ndetails the process after the bucket is loaded by the Bucket Manager.  Handling tuples after Buckets are loaded  The Bucket Manager would load all the buckets requested by the Deduper\nand add them to a fetched buckets\u00a0queue. During the span of one\napplication window of the Deduper, it will process all the tuples on its\ninput port. Processing here could mean one of the below:   The bucket for a tuple was already available in memory and hence the\n    deduper could conclude whether a tuple is a duplicate or unique.  The bucket for a tuple was not available in memory and a request was\n    made to the Bucket Manager for asynchronously loading that\n    particular bucket in memory.   After processing all the tuples as above, the Deduper starts processing\nthe tuples in the waiting events\u00a0queue as mentioned in section  Case II\n- Bucket not in memory . For each of these waiting\ntuples, a corresponding bucket is loaded by the Bucket Manager in the\nfetched buckets\u00a0queue. Using these fetched buckets, the Deduper can\nresolve the pending tuples as duplicate or unique. This is done in the\nsame way as for  Buckets available in memory .  Bucket Manager  Bucket manager is responsible for loading and unloading of buckets to\nand from memory. Bucket manager maintains a requested buckets\u00a0queue\nwhich holds the requests (in form of bucket keys) from the Deduper,\nindicating which buckets need to be loaded from the Bucket Store. The\nrequests are processed by the Bucket Manager one by one. The first step\nis to identify the Bucket Index for bucket key.  Identify Bucket Index  Bucket index is discussed  here . Bucket index can be\ncalculated as follows:  Bucket Index = Requested Bucket Key % Number of Buckets,  where Number of Buckets is as defined  here .  Request Bucket Load from Store  Once we have the Bucket Index, the Bucket Store is requested to fetch\nthe corresponding bucket \u00a0and load it into memory. This is a blocking\ncall and the Bucket Manager waits while the Bucket Store fetches the\ndata from the store. Once the data is available, the Bucket Manager\nbundles the data into a bucket and adds it to the fetched buckets\u00a0queue\nmentioned  here . We detail the process of fetching the\nbucket data from the store in  this \u00a0section.  Bucket Eviction  It may not be efficient or even possible in some cases to keep all the\nbuckets into memory. This is the reason the buckets are persisted to the\nHDFS store every window. This makes it essential to off load some of the\nbuckets from memory so that new buckets can be loaded. The policy\nfollowed by the Bucket Manager is the least recently used policy.\nWhenever the Bucket Manager needs to load a particular bucket into\nmemory, it identifies a bucket in memory which has been accessed least\nrecently and unloads it from memory. No other processing has to be done\nin this case. Upon unloading, it informs the listeners (the Deduper\nthread) that the particular bucket has been off loaded from memory and\nis no longer available.  Bucket Store  The Bucket Store is responsible for fetching and storing the data from a\npersistent store. In this case, HDFS is used as the persistent store and\nHDFSBucketStore is responsible for interacting with the store.  Data Format  Bucket store persists the buckets onto the HDFS. This is typically done\nevery window, although this can be configured to be done every\ncheckpoint window. This data is stored as files on HDFS. Every write\ni.e. new unique records generated per window (or per checkpoint window)\nis written into a new file on HDFS. The format of the file is given\nbelow.   All the unique records (actually, just keys, since dedup requires just\nstorage of keys) that are received in a window, are collected in a set\nof buckets and written one after the other serially in a file on HDFS.\nThis data is indexed in case it needs to be read back. Index structures\nare described in the  Data Structures \u00a0section.  Data Structures  HDFS Bucket Store keeps the information about the buckets and their\nlocations in various data structures.   Bucket Positions  - This data structure stores for each bucket index,\n    the files and the offset within those files where the data for the\n    bucket is stored. Since the data for a single bucket may be spread\n    across multiple files, the number of files and their offsets may be\n    multiple.  Window To Buckets  - This data structure keeps track of what buckets\n    were modified in which window. This is essentially a multi map of\n    window id to the set of bucket indexes that were written in that\n    window.  Window to Timestamp  - This data structure keeps track of the maximum\n    timestamp within a particular window file. This gives an indication\n    as to how old is the data in the window file and helps in\n    identifying window files that can be deleted entirely.   Bucket Fetch  For fetching a particular bucket, a bucket index is passed to the HDFS\nbucket store. Using the bucket index, a list of window files is\nidentified which contains the data for this index. The bucket positions\ndata structure is used for this purpose. For each such window file, the\nBucket Store forks off a thread to fetch that particular window file\nfrom HDFS. Effectively all the window files which contain data for a\nparticular bucket index, are fetched in parallel from the HDFS. Once\nfetched, the data is bundled together and returned to the calling\nfunction i.e. the Bucket Manager.", 
            "title": "Flow of a Tuple through Dedup Operator"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "HDHT"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/operators/dimensions_computation/", 
            "text": "Dimensions Computation\n\n\nBig data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends.\n\nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as \nDimensions Computation\n.\n\n\nThis document provides an overview of \nDimensions Computation\n, as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.\n\n\nOverview\n\n\nWhat is Dimensions Computation?\n\n\nDimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.\n\n\nDimensions Computation\n provides a way for businesses to perform aggregations on configured numeric data. The \nDimensionsComputation\n operator works along with the \nDimensionStore\n operator, which provides the capability for applications to display historical data and trends.\n\n\nKey Concepts\n\n\nKey set\n\n\nA key set is a set of fields in the incoming tuple that is used to combine data for aggregation.\n\n\nValue set\n\n\nA value set is the set of fields in the incoming tuple on which \nAggregator\n(s)  are applied.\n\n\nAggregator\n\n\nAn aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.\n\n\nAggregates\n\n\nAggregates are objects containing the aggregated values for a configured value set and key combination.\n\n\nTime buckets\n\n\nTime buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12:\n01:34\n PM * and * 12:\n01:59\n PM * will be * 12:\n01:00\n PM\n. Similarly, for an hourly time bucket, floor time-value for * \n15\n:02:34 PM\n and * \n15\n:34:00 PM\n will be * \n15:00:00\n PM\n.\n\n\nAfter calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.\n\n\nCombinations\n\n\nCombinations indicate a group of the keys that are used for aggregate computations.\n\n\nIncremental \nAggregators\n\n\nIncremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,\n\n\nSUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value} \n {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}\n\n\n\n\nOn-the-fly \nAggregators\n (OTF Aggregators)\n\n\nOn-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,\n\n\nAVERAGE = {Current_SUM} / {Current_COUNT}\n\n\n\n\nDimensionsComputation operator\n\n\nThe DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.\n\n\nDimensionsStore operator\n\n\nThe DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.\n\n\nDimensions Computation use cases\n\n\nConsider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.\n\n\nArchitecture\n\n\nDimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given \nDAG\n is for a complete Dimensions Computation application.\n\n\nThe operators within Dimensions Computation are described in detail.\n\n\nDimensionsComputation\n\n\nThe DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the \nconfiguration\n, the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in \nDAG\n) for calculating cumulative aggregates.\n\n\nDimensionsStore\n\n\nThe DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:\n\n\nSUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2\n\n\n\n\nThe DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.\n\n\nQuery\n\n\nThe Query operator interfaces with the \npubsub server\n of \nDataTorrent Gateway\n. The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.\n\n\nQueryResult\n\n\nThe QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the \npubsub server\n for publishing to UI widgets.\n\n\nDAG \n\n\n\n\nDimensions Computation Configuration \n\n\nConfiguration Definitions\n\n\nThe configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:\n\n\nDimensions Computation Schema Configuration\n\n\nThe Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:\n\n\n{\nkeys\n:[{\nname\n:\npublisher\n,\ntype\n:\nstring\n,\nenumValues\n:[\ntwitter\n,\nfacebook\n,\nyahoo\n]},\n         {\nname\n:\nadvertiser\n,\ntype\n:\nstring\n,\nenumValues\n:[\nstarbucks\n,\nsafeway\n,\nmcdonalds\n]},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n,\nenumValues\n:[\nN\n,\nLREC\n,\nSKY\n,\nAL\n,\nAK\n]}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nCOUNT\n,\nAVG\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\npublisher\n], \nadditionalValues\n:[\nimpressions:MIN\n, \nclicks:MIN\n, \ncost:MIN\n, \nrevenue:MIN\n, \nimpressions:MAX\n, \nclicks:MAX\n, \ncost:MAX\n, \nrevenue:MAX\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nlocation\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n]},\n   {\ncombination\n:[\npublisher\n,\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nThe schema configuration is a JSON string that contains the following information:\n\n\n\n\n\n\nkeys:\n - This contains the set of keys derived from an input tuple. The \nname\n field stands for the name of the field from input tuple. The \ntype\n can be defined for individual keys. The probable values for individual keys can be provided using \nenumValues\n.\n\n\n\n\n\n\nvalues:\n - This contains the set of fields from an input tuple on which aggregates are calculated. The \nname\n field stands for the name of the field from an input tuple. The \ntype\n can be defined for individual keys. The \naggregators\n can be defined separately for individual values. Only configured aggregator functions are executed on values.\n\n\n\n\n\n\ntimeBuckets:\n - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are \n\"1m\", \"1h\", \"1d\"\n\n\n\n\n\n\ndimensions:\n - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in \ncombination\n with the JSON key. \nadditionalValues\n can be used for mentioning additional aggregators for any \nvalue\n. For example, \nimpressions:MIN\n indicates that for a given combination, calculate \"\nMIN\n\" for value \"\nimpression\n\" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.\n\n\n\n\n\n\nOperator Configurations\n\n\nOperator configurations is another set of configuration that can be used to configure individual operators.\n\n\nProperties\n\n\n\n\ndt.operator.QueryResult.topic:\n - This is the name of the topic on which UI widgets listen for results.\n\n\ndt.operator.Query.topic:\n - his is the name of the topic on which Query operator listen for queries.\n\n\ndt.operator.QueryResult.numRetries\n - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.\n\n\n\n\nAttributes\n\n\n\n\ndt.operator.DimensionsComputation.attr.PARTITIONER:\n - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the \nPartitioning\n section for details on partitioning.\n\n\ndt.operator.DimensionsComputation.attr.MEMORY_MB:\n - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.\n\n\ndt.operator.Store.attr.MEMORY_MB:\n - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.\n\n\ndt.port.*.attr.QUEUE_CAPACITY\n - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.\n\n\n\n\nVisualizing Dimensions Computation\n\n\nWhen Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.\n\n\nAnd example of a Dashboard UI Widget is as follows:\n\n\n\n\nCreating Dimensions Computation Application\n\n\nConsider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.\n\n\nSample publisher event\n\n\nAn event might look like this:\n\n\npublic class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}\n\n\n\n\nCreating an Application using out-of-the-box operators\n\n\nDimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:\n\n\n@ApplicationAnnotation(name=\nAdEventDemo\n)\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA = \nadsGenericEventSchema.json\n;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString(\neventSchema.json\n);\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver oject.\n    AdEventReceiver receiver = dag.addOperator(\nEvent Receiver\n, AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map\nString, String\n keyToExpression = Maps.newHashMap();\n    keyToExpression.put(\nadvertiser\n, \ngetAdvertiser()\n);\n    keyToExpression.put(\nlocation\n, \ngetLocation()\n);\n    keyToExpression.put(\ntime\n, \ngetTime()\n);\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map\nString, String\n valueToExpression = Maps.newHashMap();\n    valueToExpression.put(\ncost\n, \ngetCost()\n);\n    valueToExpression.put(\nrevenue\n, \ngetRevenue()\n);\n    valueToExpression.put(\nimpressions\n, \ngetImpressions()\n);\n    valueToExpression.put(\nclicks\n, \ngetClicks()\n);\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator(\nStore\n, AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath(\ndataStorePath\n);\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create(\nws://\n + gatewayAddress + \n/pubsub\n);\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator(\nQuery\n, PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator(\nQueryResult\n, PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream(\nQuery\n, wsIn.outputPort, store.query);\n    dag.addStream(\nQueryResult\n, store.queryResult, wsOut.input);\n    dag.addStream(\nInputStream\n, receiver.output, dimensions.input);\n    dag.addStream(\nDimensionalData\n, dimensions.output, store.input);\n  }\n}\n\n\n\n\nConfiguration for Sample Predefined Use Cases\n\n\nThe following configuration can be used for the predefined use case of the advertiser-publisher.\n\n\nDimensions Schema Configuration\n\n\n{\nkeys\n:[{\nname\n:\nadvertiser\n,\ntype\n:\nstring\n},\n         {\nname\n:\nlocation\n,\ntype\n:\nstring\n}],\n \ntimeBuckets\n:[\n1m\n,\n1h\n,\n1d\n],\n \nvalues\n:\n  [{\nname\n:\nimpressions\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nclicks\n,\ntype\n:\nlong\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\ncost\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]},\n   {\nname\n:\nrevenue\n,\ntype\n:\ndouble\n,\naggregators\n:[\nSUM\n,\nMAX\n,\nMIN\n]}],\n \ndimensions\n:\n  [{\ncombination\n:[]},\n   {\ncombination\n:[\nlocation\n]},\n   {\ncombination\n:[\nadvertiser\n]},\n   {\ncombination\n:[\nadvertiser\n,\nlocation\n]}]\n}\n\n\n\n\nOperator Configuration\n\n\nNote:\n This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.\n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nno\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.PARTITIONER\n/name\n\n    \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.DimensionsComputation.attr.MEMORY_MB\n/name\n\n    \nvalue\n16384\n/value\n\n  \n/property\n\n  \nproperty\n\n     \nname\ndt.port.*.attr.QUEUE_CAPACITY\n/name\n\n     \nvalue\n32000\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Query.topic\n/name\n\n    \nvalue\nAdsEventQuery\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.QueryResult.topic\n/name\n\n    \nvalue\nAdsEventQueryResult\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.\n\n\nAdvanced Concepts\n\n\nPartitioning \n\n\nThe Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the \nproperties.xml\n file, or in the \ndt-site.xml\n file.\n\n\nproperty\n\n  \nname\ndt.operator.DimensionsComputations.attr.PARTITIONER\n/name\n\n  \nvalue\ncom.datatorrent.common.partitioner.StatelessPartitioner:8\n/value\n\n\n/property\n\n\n\n\n\nThis adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of \n8\n is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.\n\n\nAlong with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:\n\n\nDimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator(\nDimensionsComputation\n, DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl\nInputEvent, Aggregate\n());\n\n\n\n\nHere the unifier used is \nDimensionsComputationUnifierImpl\n which is an out-of-the-box operator present in the DataTorrent distribution.\n\n\nConclusion\n\n\nAggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "DimenstionComputation"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation", 
            "text": "Big data scenarios often have use-case where huge volumes of data flowing through the big data systems need to be observed for historical trends. \nMany applications addressing such scenarios can greatly benefit if they are equipped with the functionality of viewing historical data aggregated across time buckets. The process of receiving individual events, aggregating them over a duration, and using parameters to observe trends is referred to as  Dimensions Computation .  This document provides an overview of  Dimensions Computation , as well as instructions for using DataTorrent's operators to easily add dimensions computation to an Apache Apex application.", 
            "title": "Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#overview", 
            "text": "What is Dimensions Computation?  Dimensions Computation is a powerful mechanism that allows for spotting trends in streaming data in real-time. This tutorial will cover the concepts behind Dimensions Computation and provide details on the process of performing Dimensions Computation. We will also show you how to use Data Torrent's out of the box operators to easily add Dimensions Computation to an application.  Dimensions Computation  provides a way for businesses to perform aggregations on configured numeric data. The  DimensionsComputation  operator works along with the  DimensionStore  operator, which provides the capability for applications to display historical data and trends.", 
            "title": "Overview"
        }, 
        {
            "location": "/operators/dimensions_computation/#key-concepts", 
            "text": "Key set  A key set is a set of fields in the incoming tuple that is used to combine data for aggregation.  Value set  A value set is the set of fields in the incoming tuple on which  Aggregator (s)  are applied.  Aggregator  An aggregator is a mathematical function that is  applied on value fields in an incoming tuple. Examples of aggregators are SUM, COUNT, MAX, MIN, and AVERAGE.  Aggregates  Aggregates are objects containing the aggregated values for a configured value set and key combination.  Time buckets  Time buckets indicate the duration for which the floor time is calculated. For example, a for a time bucket of 1 minute, the floor time-value for both * 12: 01:34  PM * and * 12: 01:59  PM * will be * 12: 01:00  PM . Similarly, for an hourly time bucket, floor time-value for *  15 :02:34 PM  and *  15 :34:00 PM  will be *  15:00:00  PM .  After calculating the floor value for a duration, the time-value becomes a key. Currently supported time buckets are:  1 second, 1 minute, 1 hour, and 1 day.  Combinations  Combinations indicate a group of the keys that are used for aggregate computations.  Incremental  Aggregators  Incremental aggregators are aggregate functions for which computations are possible only by using previous aggregate value and the new value. For example,  SUM = {Previous_SUM} + {Current_Value}\nCOUNT = {Previous_COUNT}  + 1\nMIN = ( {Current_Value}   {Previous_MIN} ) ? {Current_Value} : {Previous_MIN}  On-the-fly  Aggregators  (OTF Aggregators)  On-the-fly aggregators are the aggregate functions that use the result of multiple incremental aggregators, and can be calculated on-the-fly if necessary. For example,  AVERAGE = {Current_SUM} / {Current_COUNT}  DimensionsComputation operator  The DimensionsComputation operator  is an Operator that performs intermediate aggregations using incremental aggregators.  DimensionsStore operator  The DimensionsStore operator  is an Operator that performs transient and final aggregations on the data generated by the Dimensions Computations operator. It maintains the historical data for aggregates to ensure a meaningful  historical view.", 
            "title": "Key Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-use-cases", 
            "text": "Consider the case of a digital advertising publisher who receives thousands of click events every second. The history of individual clicks and impressions doesn't divulge details about users and the advertisements. A technique for deriving meaning out of such data is to observe the total number of clicks and impressions every second, minute, hour, and day. Such a technique might be  helpful for determining global trends in the advertising system, but may not provide enough granularity for localized trends. For example, the total clicks and impressions over a duration might lack in usefulness, however, the total clicks and impressions for a particular advertiser, a particular geographical area, or a combination of the two can provide actionable insight.", 
            "title": "Dimensions Computation use cases"
        }, 
        {
            "location": "/operators/dimensions_computation/#architecture", 
            "text": "Dimensions Computation requires 4 operators working in sync: DimensionsComputation, DimensionsStore, Query, and QueryResult. Given  DAG  is for a complete Dimensions Computation application.  The operators within Dimensions Computation are described in detail.  DimensionsComputation  The DimensionsComputation operator works only with incremental aggregates. The incoming data stream contains tuples that are Plain Old Java Objects (POJO), which contain data required for aggregations.\nDepending on the  configuration , the DimensionsComputation operator applies incremental aggregators on the value set of the tuple data within a boundary of an application window. At the end of the application window, the aggregate value is reset to calculate the new aggregates. Thus, discrete aggregates are generated by DimensionsComputation operator for every application window. This output is used by the DimensionStore operator (labeled as Store in  DAG ) for calculating cumulative aggregates.  DimensionsStore  The DimensionsStore operator uses the discrete aggregates generated by the DimensionsComputation operator to generate cumulative aggregates in turn. Because the aggregates generated by the DimensionsComputation operator are incremental aggregates, the sum of multiple such aggregates provides cumulative aggregates as follows:  SUM1 = SUM(Value11, Value12, ...)\nSUM2 = SUM(Value21, Value22, ...)\n\n{Cumulative_SUM} = SUM1 + SUM2  The DimensionsStore operator also stores transient aggregates in a persistent proprietary store called HDHT. The DimensionsStore operator uses HDHT to present the requested historical data.  Query  The Query operator interfaces with the  pubsub server  of  DataTorrent Gateway . The browser creates a websocket connection with the  pubsub server hosted by DataTorrent Gateway. The Dashboard UI Widgets send queries to the pubsub server via this connection. The Query operator subscribes to the configured pubsub topic for receiving queries. These queries are parsed by the Query operator and passed onto DimensionsStore to fetch data from HDHT Store.  QueryResult  The QueryResult operator gets the result from the DimensionsStore operator for a given query. The results are reconstructed into a format that is understood by a widget. After the results are reconstructed into the required format, they are sent to the  pubsub server  for publishing to UI widgets.  DAG", 
            "title": "Architecture"
        }, 
        {
            "location": "/operators/dimensions_computation/#dimensions-computation-configuration", 
            "text": "Configuration Definitions  The configuration of Dimensions Computation is divided into: Dimensions Computation Schema Configuration and Operator Configurations as follows:  Dimensions Computation Schema Configuration  The Dimensions Computation Schema provides the Dimensions Computation operator with information about the aggregations. The schema looks like this:  { keys :[{ name : publisher , type : string , enumValues :[ twitter , facebook , yahoo ]},\n         { name : advertiser , type : string , enumValues :[ starbucks , safeway , mcdonalds ]},\n         { name : location , type : string , enumValues :[ N , LREC , SKY , AL , AK ]}],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : clicks , type : long , aggregators :[ SUM , COUNT , AVG ]},\n   { name : cost , type : double , aggregators :[ SUM , COUNT , AVG ]},\n   { name : revenue , type : double , aggregators :[ SUM , COUNT , AVG ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ publisher ],  additionalValues :[ impressions:MIN ,  clicks:MIN ,  cost:MIN ,  revenue:MIN ,  impressions:MAX ,  clicks:MAX ,  cost:MAX ,  revenue:MAX ]},\n   { combination :[ advertiser , location ]},\n   { combination :[ publisher , location ]},\n   { combination :[ publisher , advertiser ]},\n   { combination :[ publisher , advertiser , location ]}]\n}  The schema configuration is a JSON string that contains the following information:    keys:  - This contains the set of keys derived from an input tuple. The  name  field stands for the name of the field from input tuple. The  type  can be defined for individual keys. The probable values for individual keys can be provided using  enumValues .    values:  - This contains the set of fields from an input tuple on which aggregates are calculated. The  name  field stands for the name of the field from an input tuple. The  type  can be defined for individual keys. The  aggregators  can be defined separately for individual values. Only configured aggregator functions are executed on values.    timeBuckets:  - This can be used to specify the time bucket over which aggregations occur. Possible values for timeBuckets are  \"1m\", \"1h\", \"1d\"    dimensions:  - This defines the combinations of keys that are used for grouping data for aggregate calculations. This can be mentioned in  combination  with the JSON key.  additionalValues  can be used for mentioning additional aggregators for any  value . For example,  impressions:MIN  indicates that for a given combination, calculate \" MIN \" for value \" impression \" as well.\nBy default, the down time rounded off to the next value as per time bucket is always considered as one of the keys.    Operator Configurations  Operator configurations is another set of configuration that can be used to configure individual operators.  Properties   dt.operator.QueryResult.topic:  - This is the name of the topic on which UI widgets listen for results.  dt.operator.Query.topic:  - his is the name of the topic on which Query operator listen for queries.  dt.operator.QueryResult.numRetries  - This property indicates the maximum number of times the QueryResult operator should retry sending data. This value is usually high.   Attributes   dt.operator.DimensionsComputation.attr.PARTITIONER:  - This attribute determines the number of  partitions for DimensionsComputation. Adding more partitions means data is  processed in parallel. If this attribute is not provided, a single partition is created. Refer to the  Partitioning  section for details on partitioning.  dt.operator.DimensionsComputation.attr.MEMORY_MB:  - This attribute determines the  memory that should be assigned to the DimensionsComputations operator. If this attribute is not provided, the default value of  1 GB is used.  dt.operator.Store.attr.MEMORY_MB:  - This attribute determines the memory that should be assigned to DimensionsStore operator. If this attribute is not provided, the default value of 1 GB is used.  dt.port.*.attr.QUEUE_CAPACITY  - This attribute determines the number of tuples the buffer server can cache without blocking the input stream to the port. For peak activity, we  recommend increasing QUEUE_CAPACITY to a higher value such as 32000. If this attribute is not provided, the default value of 1024 is used.", 
            "title": "Dimensions Computation Configuration "
        }, 
        {
            "location": "/operators/dimensions_computation/#visualizing-dimensions-computation", 
            "text": "When Dimension Computation  is launched, the visualization of aggregates over a duration can be seen by adding a widget to a dtDashboard. dtDashboard is the self-service real-time and historical data visualization interface. Rapidly gaining insight and reducing time to action provides the greatest value to an organization. DataTorrent RTS provides self-service data visualization for the business user enabling them to not only see dashboards and reports an order of magnitude faster, but to also create and share customer reports.To derive more value out of dashboards, you can add widgets to the dashboards. Widgets are charts in addition to the default charts that you can see on the dashboard.  And example of a Dashboard UI Widget is as follows:", 
            "title": "Visualizing Dimensions Computation"
        }, 
        {
            "location": "/operators/dimensions_computation/#creating-dimensions-computation-application", 
            "text": "Consider an example of the advertising publisher. Typically, an advertising publisher receives a packet of information for every event related to their  advertisements.  Sample publisher event  An event might look like this:  public class AdEvent\n{\n  //The name of the company that is advertising\n  public String advertiser;\n  //The geographical location of the person initiating the event\n  public String location;\n  //How much the advertiser was charged for the event\n  public double cost;\n  //How much revenue was generated for the advertiser\n  public double revenue;\n  //The number of impressions the advertiser received from this event\n  public long impressions;\n  //The number of clicks the advertiser received from this event\n  public long clicks;\n  //The timestamp of the event in milliseconds\n  public long time;\n\n  public AdEvent() {}\n\n  public AdEvent(String advertiser, String location, double cost, double revenue,\n                 long impressions, long clicks, long time)\n  {\n    this.advertiser = advertiser;\n    this.location = location;\n    this.cost = cost;\n    this.revenue = revenue;\n    this.impressions = impressions;\n    this.clicks = clicks;\n    this.time = time;\n  }\n\n  /* Getters and setters go here */\n}  Creating an Application using out-of-the-box operators  Dimensions Computation can be created using out-of-the-box operators from the Megh and Malhar library. A sample is given below:  @ApplicationAnnotation(name= AdEventDemo )\npublic class AdEventDemo implements StreamingApplication\n{\n  public static final String EVENT_SCHEMA =  adsGenericEventSchema.json ;\n\n  @Override\n  public void populateDAG(DAG dag, Configuration conf)\n  {\n    //This loads the eventSchema.json file which is a jar resource file.\n    // eventSchema.json contains the Dimensions Schema using which aggregations is configured.\n    String eventSchema = SchemaUtils.jarResourceFileToString( eventSchema.json );\n\n    // Operator that receives Ad Events\n    // This can be coming from any source as long as the operator can convert the data into AdEventReceiver oject.\n    AdEventReceiver receiver = dag.addOperator( Event Receiver , AdEventReceiver.class);\n\n    //Adding Dimensions Computation operator into DAG.\n    DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\n\n    // This defines the name present in input tuple to the name of the getter method to be used to get the value of the field.\n    Map String, String  keyToExpression = Maps.newHashMap();\n    keyToExpression.put( advertiser ,  getAdvertiser() );\n    keyToExpression.put( location ,  getLocation() );\n    keyToExpression.put( time ,  getTime() );\n\n    // This defines value to expression mapping for value field name to the name of the getter method to get the value of the field.\n    Map String, String  valueToExpression = Maps.newHashMap();\n    valueToExpression.put( cost ,  getCost() );\n    valueToExpression.put( revenue ,  getRevenue() );\n    valueToExpression.put( impressions ,  getImpressions() );\n    valueToExpression.put( clicks ,  getClicks() );\n\n    dimensions.setKeyToExpression(keyToExpression);\n    dimensions.setAggregateToExpression(aggregateToExpression);\n    dimensions.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the unifier. The purpose of this unifier is to combine the partial aggregates from different partitions of DimensionsComputation operator into single stream.\n    dimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());\n\n    // Add Dimension Store operator to DAG.\n    AppDataSingleSchemaDimensionStoreHDHT store = dag.addOperator( Store , AppDataSingleSchemaDimensionStoreHDHT.class);\n\n    // This configure the Backend HDHT store of DimensionStore operator. This backend will be used to persist the Historical Aggregates Data.\n    TFileImpl hdsFile = new TFileImpl.DTFileImpl();\n    hdsFile.setBasePath( dataStorePath );\n    store.setFileStore(hdsFile);\n    store.setConfigurationSchemaJSON(eventSchema);\n\n    // This configures the Query and QueryResult operators to the gateway address. This is needs for pubsub communication of queries/results between operators and pubsub server.\n    String gatewayAddress = dag.getValue(DAG.GATEWAY_CONNECT_ADDRESS);\n    URI uri = URI.create( ws://  + gatewayAddress +  /pubsub );\n    PubSubWebSocketAppDataQuery wsIn = dag.addOperator( Query , PubSubWebSocketAppDataQuery.class);\n    wsIn.setUri(uri);\n    PubSubWebSocketAppDataResult wsOut = dag.addOperator( QueryResult , PubSubWebSocketAppDataResult.class);\n    wsOut.setUri(uri);\n\n    // Connecting all together.\n    dag.addStream( Query , wsIn.outputPort, store.query);\n    dag.addStream( QueryResult , store.queryResult, wsOut.input);\n    dag.addStream( InputStream , receiver.output, dimensions.input);\n    dag.addStream( DimensionalData , dimensions.output, store.input);\n  }\n}  Configuration for Sample Predefined Use Cases  The following configuration can be used for the predefined use case of the advertiser-publisher.  Dimensions Schema Configuration  { keys :[{ name : advertiser , type : string },\n         { name : location , type : string }],\n  timeBuckets :[ 1m , 1h , 1d ],\n  values :\n  [{ name : impressions , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : clicks , type : long , aggregators :[ SUM , MAX , MIN ]},\n   { name : cost , type : double , aggregators :[ SUM , MAX , MIN ]},\n   { name : revenue , type : double , aggregators :[ SUM , MAX , MIN ]}],\n  dimensions :\n  [{ combination :[]},\n   { combination :[ location ]},\n   { combination :[ advertiser ]},\n   { combination :[ advertiser , location ]}]\n}  Operator Configuration  Note:  This operator configuration is used for an application where the input data rate is high. To sustain the load, the Dimensions Computation operator is partitioned 8 times, and the queue capacity is increased.  ?xml version= 1.0  encoding= UTF-8  standalone= no ?  configuration \n   property \n     name dt.operator.DimensionsComputation.attr.PARTITIONER /name \n     value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value \n   /property \n   property \n     name dt.operator.DimensionsComputation.attr.MEMORY_MB /name \n     value 16384 /value \n   /property \n   property \n      name dt.port.*.attr.QUEUE_CAPACITY /name \n      value 32000 /value \n   /property \n   property \n     name dt.operator.Query.topic /name \n     value AdsEventQuery /value \n   /property \n   property \n     name dt.operator.QueryResult.topic /name \n     value AdsEventQueryResult /value \n   /property  /configuration   The above operator configuration is to be used for a highly loaded application where the input rate is quite high. To sustain the load, the Dimensions Computation operator is partitioned 8 times and the queue capacity is also increased.", 
            "title": "Creating Dimensions Computation Application"
        }, 
        {
            "location": "/operators/dimensions_computation/#advanced-concepts", 
            "text": "Partitioning   The Dimensions Computation operator can be statically partitioned for higher processing throughput. This can be done by adding the following attributes in the  properties.xml  file, or in the  dt-site.xml  file.  property \n   name dt.operator.DimensionsComputations.attr.PARTITIONER /name \n   value com.datatorrent.common.partitioner.StatelessPartitioner:8 /value  /property   This adds the PARTITIONER attribute. This attribute creates a StatelessPartitioner for the DimensionsComputation operator. The parameter of  8  is going to partition the operator 8 times.\nThe StatelessPartitioner ensures that the operators are clones of each other. The tuples passed to individual clones of operators are decided based on the hashCode of the tuple.  Along with partitioned DimensionsComputation, there also comes a unifier which combines all the intermediate results from individual DimensionsComputation operators into a single stream. This stream is then passed to Dimensions Store.\nFollowing code needs to be added in populateDAG for adding an Unifier:  DimensionsComputationFlexibleSingleSchemaPOJO dimensions = dag.addOperator( DimensionsComputation , DimensionsComputationFlexibleSingleSchemaPOJO.class);\ndimensions.setUnifier(new DimensionsComputationUnifierImpl InputEvent, Aggregate ());  Here the unifier used is  DimensionsComputationUnifierImpl  which is an out-of-the-box operator present in the DataTorrent distribution.", 
            "title": "Advanced Concepts"
        }, 
        {
            "location": "/operators/dimensions_computation/#conclusion", 
            "text": "Aggregating huge amounts of data in real time is a major challenge that many enterprises face today. Dimension Computation provides a valuable way in which to think about the problem of aggregating data, and Data Torrent provides an implementation of of Dimension Computation that allows users to integrate data aggregation with their applications with minimal effort.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "KafkaInput"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "KafkaOutput"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Solace"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "JMS"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "HBase"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtgateway_api/", 
            "text": "DataTorrent dtGateway API v2 Specification\n\n\nREST API\n\n\nReturn codes\n\n\n\n\n200\n: OK\n\n\n400\n: The request is not in the format that the server expects\n\n\n404\n: The resource is not found\n\n\n500\n: Something is wrong on the server side\n\n\n\n\nREST URI Specification\n\n\nGET /ws/v2/about\n\n\nFunction:\n\n\nReturn:\n\n\n{\n    \nbuildVersion\n: \n{buildVersion}\n,\n    \nbuildDate\n: \n{date and time}\n,\n    \nbuildRevision\n: \n{revision}\n,\n    \nbuildUser\n: \n{user}\n,\n    \nversion\n: \n{version}\n,\n    \ngatewayUser\n: \n{user}\n,\n    \njavaVersion\n: \n{java_version}\n,\n    \nhadoopLocation\n: \n{hadoop_location}\n,\n    \njvmName\n: \n{pid}@{hostname}\n,\n    \nconfigDirectory\n: \n{configDir}\n,\n    \nhostname\n: \n{hostname}\n,\n    \nhadoopIsSecurityEnabled\n: \n{true/false}\n\n}\n\n\n\n\nGET /ws/v2/cluster/metrics\n\n\nFunction: List metrics that are relevant to the entire cluster\n\n\nReturn:\n\n\n{\n    \naverageAge\n: \n{average running application age in milliseconds}\n,\n    \ncpuPercentage\n: \n{cpuPercentage}\n,\n    \ncurrentMemoryAllocatedMB\n: \n{currentMemoryAllocatedMB}\n,\n    \nmaxMemoryAllocatedMB\n: \n{maxMemoryAllocatedMB}\n,\n    \nnumAppsFailed\n: \n{numAppsFailed}\n,\n    \nnumAppsFinished\n: \n{numAppsFinished}\n,\n    \nnumAppsKilled\n: \n{numAppsKilled}\n,\n    \nnumAppsPending\n: \n{numAppsPending}\n,\n    \nnumAppsRunning\n: \n{numAppsRunning}\n,\n    \nnumAppsSubmitted\n: \n{numAppsSubmitted}\n,\n    \nnumContainers\n: \n{numContainers}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications[?states={STATE_FILTER}\nname={NAME_FILTER}\nuser={USER_FILTER]\n\n\nFunction: List IDs of all streaming applications\n\n\nReturn:\n\n\n{\n    \napps\n: [\n        {\n            \ndiagnostics\n: \n{diagnostics}\n,\n            \nelapsedTime\n: \n{elapsedTime}\n,\n            \nfinalStatus\n: \n{finalStatus}\n,\n            \nfinishedTime\n: \n{finishedTime}\n,\n            \nid\n: \n{appId}\n,\n            \nname\n: \n{name}\n,\n            \nqueue\n: \n{queue}\n,\n            \nstartedTime\n: \n{startedTime}\n,\n            \nstate\n: \n{state}\n,\n            \ntrackingUrl\n: \n{trackingUrl}\n,\n            \nuser\n: \n{user}\n\n        },  \n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}\n\n\nFunction: Get the information for the specified application\n\n\nReturn:\n\n\n{\n    \nid\n: \n{appid}\n,\n    \nname\n: \n{name}\n,\n    \nstate\n: \n{state}\n,\n    \ntrackingUrl\n: \n{tracking url}\n,\n    \nfinalStatus\n: {finalStatus},\n    \nappPath\n: \n{appPath}\n,\n    \ngatewayAddress\n: \n{gatewayAddress}\n,\n    \nelapsedTime\n: \n{elapsedTime}\n,\n    \nstartedTime\n: \n{startTime}\n,\n    \nuser\n: \n{user}\n,\n    \nversion\n: \n{stram version}\n,\n    \nremainingLicensedMB\n: \n{remainingLicensedMB}\n,\n    \nallocatedMB\n: \n{allocatedMB}\n,\n    \ngatewayConnected\n: \ntrue/false\n,\n    \nconnectedToThisGateway\n: \ntrue/false\n,\n    \nattributes\n: {\n           \n{attributeName}\n: \n{attributeValue}\n, \n           \n{attributeName-n}\n: \n{attributeValue-n}\n, \n    },\n    \nstats\n: {\n        \nallocatedContainers\n: \n{allocatedContainer}\n,\n        \ntotalMemoryAllocated\n: \n{totalMemoryAllocated}\n,\n        \nlatency\n: \n{overall latency}\n,\n        \ncriticalPath\n: \n{list of operator id that represents the critical path}\n,\n        \nfailedContainers\n: \n{failedContainers}\n,\n        \nnumOperators\n: \n{numOperators}\n,\n        \nplannedContainers\n: \n{plannedContainers}\n,\n        \ncurrentWindowId\n: \n{min of operators:currentWindowId}\n,\n        \nrecoveryWindowId\n: \n{min of operators:recoveryWindowId}\n,\n        \ntuplesProcessedPSMA\n: \n{sum of operators:tuplesProcessedPSMA}\n,\n        \ntotalTuplesProcessed\n:\n{sum of operators:totalTuplesProcessed}\n,\n        \ntuplesEmittedPSMA\n:\n{sum of operators:tuplesEmittedPSMA}\n,\n        \ntotalTuplesEmitted\n:\n{sum of operators:totalTuplesEmitted}\n,\n        \ntotalBufferServerReadBytesPSMA\n: \n{totalBufferServerReadBytesPSMA}\n,\n        \ntotalBufferServerWriteBytesPSMA\n: \n{totalBufferServerWriteBytesPSMA}\n\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan\n\n\nFunction: Return the physical plan for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nisUnifier\n: true/false\n        },\n         \u2026\n     ],\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators\n\n\nFunction: Return list of operators for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncounters\n: {\n                \n{counterName}\n: \n{counterValue}\n, \n                ...\n             },\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nunifierClass\n: \n{unifierClass}\n\n        },\n         \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/streams\n\n\nFunction: Return physical streams\n\n\nReturn:\n\n\n{\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}\n\n\nFunction: Return information of the given operator for the given application\n\n\nReturn:\n\n\n{\n    \nclassName\n: \n{className}\n,\n    \ncontainer\n: \n{containerId}\n,\n    \ncounters\n: {\n      \n{counterName}: \n{counterValue}\n, ...            \n    }\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \ncurrentWindowId\n: \n{currentWindowId}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \nports\n: [\n       {\n          \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n          \nname\n: \n{name}\n,\n          \ntotalTuples\n: \n{totalTuples}\n,\n          \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n          \ntype\n: \ninput/output\n\n       }, ...\n    ],\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,\n    \nname\n: \n{name}\n,\n    \nrecordingStartTime\n: \n{recordingStartTime}\n,\n    \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n    \nstatus\n: \n{status}\n,\n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory\n\n\nFunction: Return container deploy history of this operator\nSince: 1.0.6\n\n\nReturn:\n\n\n{\n   \ncontainers\n: [  \n        {  \n            \ncontainer\n: \n{containerId}\n,   \n            \nstartTime\n: \n{startTime}\n  \n        }, ...  \n    ],   \n    \nname\n: \n{operatorName}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports\n\n\nFunction: Get the information of all ports of the given operator of the\ngiven application\n\n\nReturn:\n\n\n{  \n    \nports\n: [\n        {  \n            \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,  \n            \ntotalTuples\n: \n{totalTuples}\n,   \n            \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n            \ntype\n: \noutput\n  \n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}\n\n\nFunction: Get the information of a specified port\n\n\nReturn:\n\n\n{  \n    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n    \nname\n: \n{name}\n,   \n    \ntotalTuples\n: \n{totalTuples}\n,   \n    \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n    \ntype\n: \n{type}\n  \n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}\nq={searchTerm}\npackagePrefixes={comma-separated-package-prefixes}]\n\n\nFunction: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        { \nname\n:\n{className}\n },\n       \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}\n\n\nFunction: Get the description of the given operator class\n\n\nReturn:\n\n\n{\n    \ninputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n          ...\n    ],\n    \noutputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n        \u2026\n    ],\n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/shutdown\n\n\nFunction: Shut down the application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/kill\n\n\nFunction: Kill the given application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start\n\n\nFunction: Start recording on operator\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop\n\n\nFunction: Stop recording on operator\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start\n\n\nFunction: Start recording on port\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop\n\n\nFunction: Stop recording on port\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]\n\n\nFunction: Return the list of containers for this application\n\n\nReturn:\n\n\n{\n    \ncontainers\n: [\n        {\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n            \nmemoryMBFree\n: \n{memoryMBFree}\n,\n            \nnumOperators\n: \n{numOperators}\n,\n            \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n            \nstate\n: \n{state}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}\n\n\nFunction: Return the information of the specified container\n\n\nReturn:\n\n\n{\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \njvmName\n: \n{jvmName}\n,\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n    \nmemoryMBFree\n: \n{memoryMBFree}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n    \nstate\n: \n{state}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs\n\n\nFunction: Return the container log list\n\n\nReturn:\n\n\n{\n    \nlogs\n: [\n        {\n            \nlength\n: \n{log length}\n,\n            \nname\n: \n{logName}\n\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}\nend={endPos}\ngrep={regexp}\nincludeOffset={true/false}]\n\n\nFunction: Return the raw log\n\n\nReturn: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):\n\n\n{\n    \nlines\n: [\n        { \nbyteOffset\n:\n{byteOffset}\n, \nline\n: \n{line}\n }, \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill\n\n\nFunction: Kill this container\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Return the logical plan of this application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n      {\n        \nname\n: \n{name}\n,\n        \nattributes\n: {attributeMap},\n        \nclass\n: \n{class}\n,\n        \nports\n: {\n           [\n            {\n                \nname\n: \n{name}\n,\n                \nattributes\n: {attributeMap},\n                \ntype\n: \ninput/output\n\n            }, ...\n           ]\n         },\n         \nproperties\n: {\n            \nclass\n: \n{class}\n\n         }\n      }, ...\n    ],\n    \nstreams\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/attributes\n\n\nFunction: Return the application attributes\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators\n\n\nFunction: Return the list of info of the logical operator\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n\n\nFunction: Return the info of the logical operator\n\n\nReturn:\n\n\n{\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Return the properties of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Set the properties of the logical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Return the properties of the physical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Set the properties of the physical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes\n\n\nFunction: Get the attributes of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes\n\n\nFunction:  Get the attributes of the port\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Change logical plan of this application\nPayload:\n\n\n{\n    \nrequests\n: [\n        {\n            \nrequestType\n: \nAddStreamSinkRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nCreateOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \noperatorFQCN\n: \n{operatorFQCN}\n,\n        },\n        {\n            \nrequestType\n: \nCreateStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsourceOperatorName\n: \n{sourceOperatorName}\n,\n            \nsourceOperatorPortName\n: \n{sourceOperatorPortName}\n\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nRemoveOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n        },\n        {\n            \nrequestType\n: \nRemoveStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n        },\n        {\n            \nrequestType\n: \nSetOperatorPropertyRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \npropertyName\n: \n{propertyName}\n,\n            \npropertyValue\n: \n{propertyValue}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta\n\n\nFunction: Return the meta information about the statistics stored for\nthis operator\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \noperatorName\n: \n{operatorName}\n,\n    \noperatorIds\n: [ {opid}, \u2026 ],\n    \nstartTime\n: \n{startTime}\n,\n    \nendTime\n: \n{endTime}\n,\n    \ncount\n: \n{count}\n,\n    \nended\n: \n{boolean}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the statistics stored for this logical operator\n\n\n{\n    \noperatorStats\n: [\n        {\n            \noperatorId\n: \n{operatorId}\n,\n            \ntimestamp\n: \n{timestamp}\n,\n            \nstats\n: {\n                \ncontainer\n: \ncontainerId\n,\n                \nhost\n: \nhost\n,\n                \ntotalTuplesProcessed\n, \n{totalTuplesProcessed}\n,\n                \ntotalTuplesEmitted\n, \n{totalTuplesEmitted}\n,\n                \ntuplesProcessedPSMA\n, \n{tuplesProcessedPSMA}\n,\n                \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n                \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n                \nlatencyMA\n: \n{latencyMA}\n,\n                \nports\n: [ {\n                    \nname\n: \n{name}\n,\n                    \ntype\n:\n{input/output}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n, \n{tuplesPSMA}\n,\n                    \nbufferServerBytesPSMA\n, \n{bufferServerBytesPSMA}\n\n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta\n\n\nFunction: Return the meta information about the container statistics\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \ncontainers\n: {\n        \n{containerId}\n: {\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nhost\n: \n{host}\n,\n            \nmemoryMBAllocated\n, \n{memoryMBAllocated}\n\n        },\n        \u2026\n    },\n    \nstartTime\n: \n{startTime}\n\n    \nendTime\n: \n{endTime}\n\n    \ncount\n: \n{count}\n\n    \nended\n: {boolean}\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the container statistics stored for this application\n\n\n{\n    \ncontainerStats\n: [\n        {\n            \ncontainerId\n: \n{containerId}\n\n            \ntimestamp\n: \n{timestamp}\n\n            \nstats\n: {\n                \nnumOperators\n: \n{numOperators}\n,\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/recordings\n\n\nFunction: Get the list of all recordings for this application\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [{\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings\n\n\nFunction: Get the list of recordings on this operator\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [ {\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Get the information about the recording\n\n\nReturn:\n\n\n{\n    \nid\n: \n{id}\n,\n    \nstartTime\n: \n{startTime}\n,\n    \nappId\n: \n{appId}\n,\n    \noperatorId\n: \n{operatorId}\n,\n    \ncontainerId\n: \n{containerId}\n,\n    \ntotalTuples\n: \n{totalTuples}\n,\n    \nports\n: [ {\n       \nname\n: \n{portName}\n,\n       \nstreamName\n: \n{streamName}\n,\n       \ntype\n: \n{type}\n,\n       \nid\n: \n{index}\n,\n       \ntupleCount\n: \n{tupleCount}\n\n     } \u2026 ],\n    \nended\n: {boolean},\n    \nwindowIdRanges\n: [ {\n       \nlow\n: \n{lowId}\n,\n       \nhigh\n: \n{highId}\n\n     } \u2026 ],\n    \nproperties\n: {\n       \nname\n: \nvalue\n, ...\n     }\n}\n\n\n\n\nDELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Deletes the specified recording\n\n\nSince: 1.0.4\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples\n\n\nQuery Parameters:\n\n\noffset\nstartWindow\nlimit\nports\nexecuteEmptyWindow\n\n\n\nFunction: Get the tuples\n\n\nReturn:\n\n\n{\n    \nstartOffset\n: \n{startOffset}\n,\n    \ntuples\n: [ {\n        \nwindowId\n: \n{windowId}\n,\n        \ntuples\n: [ {\n            \nportId\n: \n{portId}\n,\n            \ndata\n: \n{tupleData}\n\n        }, \u2026 ]\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/events?from={fromTime}\nto={toTime}\noffset={offset}\nlimit={limit}\n\n\nFunction: Get the events\n\n\nReturn:\n\n\n{\n    \nevents\n: [ {\n           \nid\n: \n{id}\n,\n        \ntimestamp\n: \n{timestamp}\n,\n        \ntype\n: \n{type}\n,\n        \ndata\n: {\n            \nname\n: \nvalue\n, \u2026\n        }\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/user\n\n\nFunction: Get the user profile information, list of roles and list of\npermissions given the user\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/settings\n\n\nFunction: Get the current user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}\n\n\nFunction: Get the specified user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Get the specified user's setting key\n\n\nReturn:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nPUT /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Set the specified user's setting key\nPayload:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nGET /ws/v2/auth/roles\n\n\nFunction: Get the list of roles the system has\n\n\nReturn:\n\n\n{\n    \nroles\n: [\n       {\n         \nname\n: \n{role1}\n,\n         \npermissions\n: [ \n{permission1}\n, \u2026 ]\n       }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/auth/roles/{role}\n\n\nFunction: Get the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/auth/roles/{role}\n\n\nFunction: create or edit the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/auth/restoreDefaultRoles\n\n\nFunction: Restores default roles\n\n\nDELETE /ws/v2/auth/roles/{role}\n\n\nFunction: delete the given role\n\n\nGET /ws/v2/auth/permissions\n\n\nFunction: Get the list of possible permissions\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ {\n       \nname\n: \n{permissionName}\n,\n       \nadminOnly\n: true/false\n    }, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/applications/{appid}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPOST /ws/v2/licenses\n\n\nFunction: Add a license to the registry or generate an eval license\n\n\nPayload: The license file content, if payload is empty, it will try to generate an eval license and return the info\n\n\nReturn:\n\n\n{\n  \nid\n: \n{licenseId}\n,\n  \nexpireTime\n: {unixTimeMillis},\n  \nnodesAllowed\n: {nodesAllowed},\n  \nmemoryMBAllowed\n: {memoryMBAllowed},\n  \ncontextType\n: \n{contextType}\n,\n  \ntype\n: \n{type}\n,\n  \nfeatures\n: [ \n{feature1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/licenses/current\n\n\nFunction: Get info on the current license\n\n\n{\n      \nid\n: \n{licenseId}\n,\n      \nexpireTime\n: {unixTimeMillis},\n      \nnodesAllowed\n: {nodesAllowed},\n      \nnodesUsed\n: {nodesUsed},\n      \nmemoryMBAllowed\n: {memoryMBAllowed},\n      \nmemoryMBUsed\n: {memoryMBUsed},\n      \ncontextType\n: \n{community|standard|enterprise}\n,\n      \ntype\n: \n{evaluation|non_production|production}\n\n      \nfeatures\n: [ \n{feature1}\n, \u2026 ], // for community, empty array\n      \ncurrent\n: true/false\n}\n\n\n\n\nGET /ws/v2/config/installMode\n\n\nFunction: returns the install mode\n\n\n{\n  \ninstallMode\n: \n{evaluation|community|app}\n,\n  \nappPackageName\n: \n{optionalAppPackageName}\n,\n  \nappPackageVersion\n: \n{optionalAppPackageVersion}\n\n}\n\n\n\n\nGET /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction: returns the download type\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nPUT /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction:\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nFeature List:  \n\n\n\n\nSYSTEM_APPS\n\n\nSYSTEM_ALERTS\n\n\nAPP_DATA_DASHBOARDS\n\n\nRUNTIME_DAG_CHANGE\n\n\nRUNTIME_PROPERTY_CHANGE\n\n\nAPP_CONTAINER_LOGS\n\n\nLOGGING_LEVELS\n\n\nAPP_DATA_TRACKER\n\n\nJAAS_LDAP_AUTH\n\n\nAPP_BUILDER\n\n\n\n\nGET /ws/v2/config/properties\n\n\nFunction: Returns list of properties from dt-site.xml.\n\n\nReturn:\n\n\n{\n    \n{name}\n: {\n        \nvalue\n: \n{PROPERTY_VALUE}\n,\n        \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n    }\n\n}\n\n\n\n\nGET /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Returns single property from dt-site.xml, specify by name\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nPOST /ws/v2/config/properties\n\n\nFunction: Overwrites all specified properties in dt-site.xml\n\n\nPayload:\n\n\n{\n    \nproperties\n: [\n        {\n            \nname\n: \n{name}\n\n            \nvalue\n: \n{PROPERTY_VALUE}\n,\n            \nlocal\n: true/false,\n                    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nPUT /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Overwrites or creates new property in dt-site.xml\nPayload:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \nlocal\n: true/false,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nDELETE /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Deletes a property from dt-site.xml. This may have to be\nrestricted to custom properties?\n\n\nGET /ws/v2/config/hadoopExecutable\n\n\nFunction: Returns the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nPUT /ws/v2/config/hadoopExecutable\n\n\nFunction: Sets the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nGET /ws/v2/config/issues\n\n\nFunction: Returns list of potential issues with environment\n\n\nReturn:\n\n\n{\n    \nissues\n: [\n        {\n            \nkey\n: \n{issueKey}\n,\n            \npropertyName\n: \n{PROPERTY_NAME}\n,\n            \ndescription\n: \n{ISSUE_DESCRIPTION}\n,\n            \nseverity\n: \nerror\n|\nwarning\n\n        },\n        {...},\n        {...}\n    ]    \n}\n\n\n\n\nGET /ws/v2/config/ipAddresses\n\n\nFunction: Returns list of ip addresses the gateway can listen to\n\n\nReturn:\n\n\n{\n    \nipAddresses\n: [\n      \n1.2.3.4\n, ...\n    ]    \n}\n\n\n\n\nPOST /ws/v2/config/restart\n\n\nFunction: Restarts the gateway\n\n\nPayload: none\n\n\nGET /proxy/rm/v1/\u2026\n\n\nPOST /proxy/rm/v1/\u2026\n\n\nFunction: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.\n\n\nGET /proxy/stram/v2/...\n\n\nPOST /proxy/stram/v2/\u2026\n\n\nPUT /proxy/stram/v2/\u2026\n\n\nDELETE /proxy/stram/v2/\u2026\n\n\nFunction: Proxy calls to Stram Web Services.\n\n\nPOST /ws/v2/applications/{appid}/loggers\n\n\nFunction: Set the logger levels of packages/classes.\n\n\nPayload:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers\n\n\nFunction: Gets the logger levels of packages/classes.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"\n\n\nFunction: searches for all classes that match the pattern.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nname\n : \n{fully qualified class name}\n,\n            \nlevel\n: \n{logger level}\n\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of appPackages the user can view in the system\n\n\n{\n    \nappPackages\n: [\n        {\n                 \nappPackageName\n: \n{appPackageName}\n,\n                 \nappPackageVersion\n: \n{appPackageVersion}\n,\n            \nmodificationTime\n: \n{modificationTime}\n,\n            \nowner\n: \n{owner}\n,\n        }, ...\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages?merge={replace|fail|ours|theirs}\n\n\nSince: 1.0.4\n\n\nFunction: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\n\n\nPayload: the raw zip file\n\n\nReturn: The information of the app package\n\n\nGET /ws/v2/appPackages/{owner}/{name}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of versions of appPackages with the given name in the system owned by the specified user\n\n\n{\n    \nversions\n: [\n        \n1.0-SNAPSHOT\n\n    ]\n}\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the appPackage\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download\n\n\nSince: 1.0.4\n\n\nFunction: Downloads the appPackage zip file\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta information of the app package\n\n\nReturns:\n\n\n{\n    \nappPackageName\n: \n{appPackageName}\n,\n    \nappPackageVersion\n: \n{appPackageVersion}\n,\n    \nmodificationTime\n:  \n{modificationTime}\n,\n    \nowner\n: \n{owner}\n,\n    ...\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs\n\n\nSince: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:\n\n\n{\n    \nconfigs\n: [\n        \nmy-app-conf1.xml\n\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the properties XML of the specified config\n\n\nReturns:\n\n\nconfiguration\n\n        \nproperty\n\n                \nname\n...\n/name\n\n                \nvalue\n...\n/value\n\n        \n/property\n\n        \u2026\n\n/configuration\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Creates or replaces the specified config with the property parameters specified payload\n\n\nPayload: configuration in XML\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the specified config\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of applications in the appPackage\n\n\nReturns:\n\n\n{\n    \napplications\n: [\n        {\n            \ndag\n: {dag in json format},\n            \nfile\n: \n{fileName}\n,\n            \nname\n: \n{name}\n,\n            \ntype\n: \n{type}\n,\n            \nerror\n: \n{error}\n,\n            \nfileContent\n: {originalFileContentForJSONTypeApp}\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta data for that application\n\n\nReturns:\n\n\n{\n    \nfile\n: \n{fileName}\n,\n    \nname\n: \n{name}\n,\n    \ntype\n: \n{json/class/properties}\n,\n    \nerror\n: \n{error}\n\n    \ndag\n: {\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n         }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n    },\n    \nfileContent\n: {originalFileContentForJSONTypeApp}\n}\n\n\n\n\nPOST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge\n\n\nFunction: Merge the configuration, json apps, and resources files from the version specified from the payload to the specified app package in the url, without overwriting any existing file in the specified app package\n\n\nPayload:\n\n\n{\n \nversion\n: \n{versionToMergeFrom}\n\n}\n\n\n\n\nPOST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}\noriginalAppId={originalAppId}\nqueue={queueName}]\n\n\nSince: 1.0.4\n\n\nFunction: Launches the application with the given configuration specified in the POST payload\n\n\nPayload:\n\n\n{\n    \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}\n\n\nSince: 1.0.4\n\n\nFunction: Get the properties of the operator given the classname in the jar\n\n\n{  \n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]\n\n\nFunction: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app ackage\n\n\nPayload:\n\n\n{\n        \ndisplayName\n: \n{displayName}\n,\n        \ndescription\n: \n{description}\n,\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n          }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n}\n\n\n\n\nReturn:\n\n\n{\n        \nerror\n: \n{error}\n\n}\n\n\n\n\nAvailable port attributes to set: \n\n\n\n\nAUTO_RECORD\n\n\nIS_OUTPUT_UNIFIED\n\n\nPARTITION_PARALLEL\n\n\nQUEUE_CAPACITY\n\n\nSPIN_MILLIS\n\n\nSTREAM_CODEC\n\n\nUNIFIER_LIMIT\n\n\n\n\nAvailable locality options to set: \n\n\n\n\nTHREAD_LOCAL\n\n\nCONTAINER_LOCAL\n\n\nNODE_LOCAL\n\n\nRACK_LOCAL\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}\n\n\nSince: 1.0.5\n\n\nFunction: Deletes non-jar based application in the app package\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators\n\n\nSince: 1.0.5\n\n\nFunction: Get the classes of operators from specified app package.\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        {\n            \nname\n:\n{fullyQualifiedClassName}\n, \n            \ntitle\n: \n{title}\n,\n            \nshortDesc\n: \n{description}\n,\n            \nlongDesc\n: \n{description}\n,\n            \ncategory\n: \n{categoryName}\n,\n            \ndoclink\n: \n{doc url}\n,\n            \ntags\n: [ \n{tag}\n, \n{tag}\n, \u2026 ],\n            \ninputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ]\n            \noutputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ],\n            \nproperties\n: [  \n                {\n                    \nname\n:\n{propertyName}\n,\n                    \ncanGet\n: {canGet},\n                    \ncanSet\n: {canSet},\n                    \ntype\n:\n{type}\n,\n                    \ndescription\n:\n{description}\n,\n                    \nproperties\n: ...\n                }, \u2026\n            ],\n            \ndefaultValue\n: {\n                \n{propertyName}\n: [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/import\n\n\nFunction: List the importable app packages on Gateway's local file\nsystem\n\n\nReturn:\n\n\n{\n    \nappPackages: [\n        {\n            \nfile\n: \n{file}\n,\n            \nname\n: \n{name}\n,\n            \ndisplayName\n: \n{displayName}\n,\n            \nversion\n: \n{version}\n,\n            \ndescription\n: \n{description}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages/import\n\n\nFunction: Import app package from Gateway's local file system\n\n\nPayload:\n\n\n{\n        \nfiles\n: [\n{file}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Creates or replaces the specified system alert. The condition has access to an object in its scope called \n_topic\n. An example alert might take the form of the following:\n\n\n_topic[\"applications.application_1400294100000_0001\"].allocatedContainers \n 5\n\n\n\nPayload:\n\n\n{\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n        \ntimeThresholdMillis\n:\n{time}\n\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Deletes the specified system alert\n\n\nGET /ws/v2/systemAlerts/alerts?inAlert={true/false}\n\n\nFunction: Gets the created alerts\n\n\nReturn:\n\n\n{\n    \nalerts\n: [{\n        \nname\n: \n{alertName}\n,\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n        \ntimeThresholdMillis\n:\n{time}\n,\n        \nalertStatus\n: {\n            \nisInAlert\n:{true/false}\n            \ninTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }\n    }, \u2026  ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Gets the specified system alert\n\n\nReturn:\n\n\n{\n    \nname\n: \n{alertName}\n,\n    \ncondition\n:\n{condition in javascript}\n,        \n    \nemail\n:\n{email}\n,\n    \ntimeThresholdMillis\n:\n{time}\n,\n    \nalertStatus\n: {\n        \nisInAlert\n:{true/false}\n        \ninTime\n: \n{time}\n,\n        \nmessage\n: \n{message}\n,\n        \nemailSent\n: {true/false}\n    }\n}\n\n\n\n\nGET /ws/v2/systemAlerts/history\n\n\nFunction: Gets the history of alerts\n\n\nReturn:\n\n\n{\n    \nhistory\n: [\n        {\n            \nname\n:\n{alertName}\n,\n            \ninTime\n:\n{time}\n,\n            \noutTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\nFunction: Gets the topic data that is used for evaluating alert\ncondition\n\n\nReturn:\n\n\n{\n     \n{topicName}\n: {json object data}, ...\n}\n\n\n\n\nGET /ws/v2/auth/users/{user}\n\n\nFunction: Gets the info of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPOST /ws/v2/auth/users/{user}\n\n\nFunction: Changes password and/or roles of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \noldPassword\n: \n{oldPassword}\n,\n    \nnewPassword\n: \n{newPassword}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPUT /ws/v2/auth/users/{user}\n\n\nFunction: Creates new user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nDELETE /ws/v2/auth/users/{user}\n\n\nFunction: Deletes the specified user\n\n\nGET /ws/v2/auth/users\n\n\nFunction: Gets the list of users\n\n\nReturn:\n\n\n{\n    \nusers\n: [ {\n       \nuserName\n: \n{username1}\n,\n       \nroles\n: [ \n{role1}\n, \u2026 ],\n       \npermissions\n: [ \n{permission1}\n, \u2026 ]\n    }\n}\n\n\n\n\nPOST /ws/v2/login\n\n\nFunction: Login\nPayload:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n\n}\n\n\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/logout\n\n\nFunction: Log out the current user\n\n\nReturn:\n\n\n{\n}\n\n\n\n\nPublisher-Subscriber WebSocket Protocol\n\n\nInput\n\n\nPublishing\n\n\n{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nSubscribing\n\n\n{\"type\":\"subscribe\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing\n\n\n{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nOutput\n\n\nNormal Published Data\n\n\n{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nNumber of Subscribers:\n\n\n{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}\n\n\n\nAuto publish topics\n\n\ndata that gets published every one second:\n\n\n\n\napplications\n - list of streaming applications running in the cluster\n\n\napplications.[appid]\n - information about a particular application\n\n\napplications.[appid].containers\n - information about containers of a particular application\n\n\napplications.[appid].physicalOperators\n - information about operators of a particular application\n\n\napplications.[appid].logicalOperators\n - information about logical operators of a particular application\n\n\napplications.[appid].events\n - events from the AM of a particularapplication\n\n\n\n\ndata that gets published every five seconds:\n\n\n\n\ncluster.metrics\n - metrics of the cluster", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#datatorrent-dtgateway-api-v2-specification", 
            "text": "", 
            "title": "DataTorrent dtGateway API v2 Specification"
        }, 
        {
            "location": "/dtgateway_api/#rest-api", 
            "text": "", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#return-codes", 
            "text": "200 : OK  400 : The request is not in the format that the server expects  404 : The resource is not found  500 : Something is wrong on the server side", 
            "title": "Return codes"
        }, 
        {
            "location": "/dtgateway_api/#rest-uri-specification", 
            "text": "GET /ws/v2/about  Function:  Return:  {\n     buildVersion :  {buildVersion} ,\n     buildDate :  {date and time} ,\n     buildRevision :  {revision} ,\n     buildUser :  {user} ,\n     version :  {version} ,\n     gatewayUser :  {user} ,\n     javaVersion :  {java_version} ,\n     hadoopLocation :  {hadoop_location} ,\n     jvmName :  {pid}@{hostname} ,\n     configDirectory :  {configDir} ,\n     hostname :  {hostname} ,\n     hadoopIsSecurityEnabled :  {true/false} \n}  GET /ws/v2/cluster/metrics  Function: List metrics that are relevant to the entire cluster  Return:  {\n     averageAge :  {average running application age in milliseconds} ,\n     cpuPercentage :  {cpuPercentage} ,\n     currentMemoryAllocatedMB :  {currentMemoryAllocatedMB} ,\n     maxMemoryAllocatedMB :  {maxMemoryAllocatedMB} ,\n     numAppsFailed :  {numAppsFailed} ,\n     numAppsFinished :  {numAppsFinished} ,\n     numAppsKilled :  {numAppsKilled} ,\n     numAppsPending :  {numAppsPending} ,\n     numAppsRunning :  {numAppsRunning} ,\n     numAppsSubmitted :  {numAppsSubmitted} ,\n     numContainers :  {numContainers} ,\n     numOperators :  {numOperators} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}  GET /ws/v2/applications[?states={STATE_FILTER} name={NAME_FILTER} user={USER_FILTER]  Function: List IDs of all streaming applications  Return:  {\n     apps : [\n        {\n             diagnostics :  {diagnostics} ,\n             elapsedTime :  {elapsedTime} ,\n             finalStatus :  {finalStatus} ,\n             finishedTime :  {finishedTime} ,\n             id :  {appId} ,\n             name :  {name} ,\n             queue :  {queue} ,\n             startedTime :  {startedTime} ,\n             state :  {state} ,\n             trackingUrl :  {trackingUrl} ,\n             user :  {user} \n        },  \n        \u2026\n    ]\n}  GET /ws/v2/applications/{appid}  Function: Get the information for the specified application  Return:  {\n     id :  {appid} ,\n     name :  {name} ,\n     state :  {state} ,\n     trackingUrl :  {tracking url} ,\n     finalStatus : {finalStatus},\n     appPath :  {appPath} ,\n     gatewayAddress :  {gatewayAddress} ,\n     elapsedTime :  {elapsedTime} ,\n     startedTime :  {startTime} ,\n     user :  {user} ,\n     version :  {stram version} ,\n     remainingLicensedMB :  {remainingLicensedMB} ,\n     allocatedMB :  {allocatedMB} ,\n     gatewayConnected :  true/false ,\n     connectedToThisGateway :  true/false ,\n     attributes : {\n            {attributeName} :  {attributeValue} , \n            {attributeName-n} :  {attributeValue-n} , \n    },\n     stats : {\n         allocatedContainers :  {allocatedContainer} ,\n         totalMemoryAllocated :  {totalMemoryAllocated} ,\n         latency :  {overall latency} ,\n         criticalPath :  {list of operator id that represents the critical path} ,\n         failedContainers :  {failedContainers} ,\n         numOperators :  {numOperators} ,\n         plannedContainers :  {plannedContainers} ,\n         currentWindowId :  {min of operators:currentWindowId} ,\n         recoveryWindowId :  {min of operators:recoveryWindowId} ,\n         tuplesProcessedPSMA :  {sum of operators:tuplesProcessedPSMA} ,\n         totalTuplesProcessed : {sum of operators:totalTuplesProcessed} ,\n         tuplesEmittedPSMA : {sum of operators:tuplesEmittedPSMA} ,\n         totalTuplesEmitted : {sum of operators:totalTuplesEmitted} ,\n         totalBufferServerReadBytesPSMA :  {totalBufferServerReadBytesPSMA} ,\n         totalBufferServerWriteBytesPSMA :  {totalBufferServerWriteBytesPSMA} \n    }\n}  GET /ws/v2/applications/{appid}/physicalPlan  Function: Return the physical plan for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             isUnifier : true/false\n        },\n         \u2026\n     ],\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators  Function: Return list of operators for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             counters : {\n                 {counterName} :  {counterValue} , \n                ...\n             },\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             unifierClass :  {unifierClass} \n        },\n         \u2026\n     ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/streams  Function: Return physical streams  Return:  {\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}  Function: Return information of the given operator for the given application  Return:  {\n     className :  {className} ,\n     container :  {containerId} ,\n     counters : {\n       {counterName}:  {counterValue} , ...            \n    }\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     currentWindowId :  {currentWindowId} ,\n     failureCount :  {failureCount} ,\n     host :  {host} ,\n     id :  {id} ,\n     ports : [\n       {\n           bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n           name :  {name} ,\n           totalTuples :  {totalTuples} ,\n           tuplesPSMA :  {tuplesPSMA} ,\n           type :  input/output \n       }, ...\n    ],\n     lastHeartbeat :  {lastHeartbeat} ,\n     latencyMA :  {latencyMA} ,\n     name :  {name} ,\n     recordingStartTime :  {recordingStartTime} ,\n     recoveryWindowId :  {recoveryWindowId} ,\n     status :  {status} ,\n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory  Function: Return container deploy history of this operator\nSince: 1.0.6  Return:  {\n    containers : [  \n        {  \n             container :  {containerId} ,   \n             startTime :  {startTime}   \n        }, ...  \n    ],   \n     name :  {operatorName} \n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports  Function: Get the information of all ports of the given operator of the\ngiven application  Return:  {  \n     ports : [\n        {  \n             bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,  \n             totalTuples :  {totalTuples} ,   \n             tuplesPSMA :  {tuplesPSMA} ,   \n             type :  output   \n        }, \u2026\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}  Function: Get the information of a specified port  Return:  {  \n     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n     name :  {name} ,   \n     totalTuples :  {totalTuples} ,   \n     tuplesPSMA :  {tuplesPSMA} ,   \n     type :  {type}   \n}  GET /ws/v2/applications/{appid}/operatorClasses[?parent={parent} q={searchTerm} packagePrefixes={comma-separated-package-prefixes}]  Function: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent  Return:  {  \n     operatorClasses : [  \n        {  name : {className}  },\n       \u2026\n     ]\n}  GET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}  Function: Get the description of the given operator class  Return:  {\n     inputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n          ...\n    ],\n     outputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n        \u2026\n    ],\n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}  POST /ws/v2/applications/{appid}/shutdown  Function: Shut down the application  Payload: none  POST /ws/v2/applications/{appid}/kill  Function: Kill the given application  Payload: none  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start  Function: Start recording on operator  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop  Function: Stop recording on operator  Payload: none  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start  Function: Start recording on port  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop  Function: Stop recording on port  Payload: none  GET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]  Function: Return the list of containers for this application  Return:  {\n     containers : [\n        {\n             host :  {host} ,\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             lastHeartbeat :  {lastHeartbeat} ,\n             memoryMBAllocated :  {memoryMBAllocated} ,\n             memoryMBFree :  {memoryMBFree} ,\n             numOperators :  {numOperators} ,\n             containerLogsUrl :  {containerLogsUrl} ,\n             state :  {state} \n        }, \u2026\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}  Function: Return the information of the specified container  Return:  {\n     host :  {host} ,\n     id :  {id} ,\n     jvmName :  {jvmName} ,\n     lastHeartbeat :  {lastHeartbeat} ,\n     memoryMBAllocated :  {memoryMBAllocated} ,\n     memoryMBFree :  {memoryMBFree} ,\n     numOperators :  {numOperators} ,\n     containerLogsUrl :  {containerLogsUrl} ,\n     state :  {state} \n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs  Function: Return the container log list  Return:  {\n     logs : [\n        {\n             length :  {log length} ,\n             name :  {logName} \n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos} end={endPos} grep={regexp} includeOffset={true/false}]  Function: Return the raw log  Return: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):  {\n     lines : [\n        {  byteOffset : {byteOffset} ,  line :  {line}  }, \u2026\n     ]\n}  POST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill  Function: Kill this container  Payload: none  GET /ws/v2/applications/{appid}/logicalPlan  Function: Return the logical plan of this application  Return:  {\n     operators : [\n      {\n         name :  {name} ,\n         attributes : {attributeMap},\n         class :  {class} ,\n         ports : {\n           [\n            {\n                 name :  {name} ,\n                 attributes : {attributeMap},\n                 type :  input/output \n            }, ...\n           ]\n         },\n          properties : {\n             class :  {class} \n         }\n      }, ...\n    ],\n     streams : [\n        {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/logicalPlan/attributes  Function: Return the application attributes  Return:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators  Function: Return the list of info of the logical operator  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}  Function: Return the info of the logical operator  Return:  {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties  Function: Return the properties of the logical operator  Return:  {\n     {name} : value, ...\n}  POST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties  Function: Set the properties of the logical operator\nPayload:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties  Function: Return the properties of the physical operator  Return:  {\n     {name} : value, ...\n}  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties  Function: Set the properties of the physical operator\nPayload:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes  Function: Get the attributes of the logical operator  Return:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes  Function:  Get the attributes of the port  Return:  {\n     {name} : value, ...\n}  POST /ws/v2/applications/{appid}/logicalPlan  Function: Change logical plan of this application\nPayload:  {\n     requests : [\n        {\n             requestType :  AddStreamSinkRequest ,\n             streamName :  {streamName} ,\n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  CreateOperatorRequest ,\n             operatorName :  {operatorName} ,\n             operatorFQCN :  {operatorFQCN} ,\n        },\n        {\n             requestType :  CreateStreamRequest ,\n             streamName :  {streamName} ,\n             sourceOperatorName :  {sourceOperatorName} ,\n             sourceOperatorPortName :  {sourceOperatorPortName} \n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  RemoveOperatorRequest ,\n             operatorName :  {operatorName} ,\n        },\n        {\n             requestType :  RemoveStreamRequest ,\n             streamName :  {streamName} ,\n        },\n        {\n             requestType :  SetOperatorPropertyRequest ,\n             operatorName :  {operatorName} ,\n             propertyName :  {propertyName} ,\n             propertyValue :  {propertyValue} \n        },\n        ...\n    ]\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta  Function: Return the meta information about the statistics stored for\nthis operator  Return:  {\n     appId :  {appId} ,\n     operatorName :  {operatorName} ,\n     operatorIds : [ {opid}, \u2026 ],\n     startTime :  {startTime} ,\n     endTime :  {endTime} ,\n     count :  {count} ,\n     ended :  {boolean} \n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime} endTime={endTime}  Function: Return the statistics stored for this logical operator  {\n     operatorStats : [\n        {\n             operatorId :  {operatorId} ,\n             timestamp :  {timestamp} ,\n             stats : {\n                 container :  containerId ,\n                 host :  host ,\n                 totalTuplesProcessed ,  {totalTuplesProcessed} ,\n                 totalTuplesEmitted ,  {totalTuplesEmitted} ,\n                 tuplesProcessedPSMA ,  {tuplesProcessedPSMA} ,\n                 tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n                 cpuPercentageMA :  {cpuPercentageMA} ,\n                 latencyMA :  {latencyMA} ,\n                 ports : [ {\n                     name :  {name} ,\n                     type : {input/output} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA ,  {tuplesPSMA} ,\n                     bufferServerBytesPSMA ,  {bufferServerBytesPSMA} \n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta  Function: Return the meta information about the container statistics  {\n     appId :  {appId} ,\n     containers : {\n         {containerId} : {\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             host :  {host} ,\n             memoryMBAllocated ,  {memoryMBAllocated} \n        },\n        \u2026\n    },\n     startTime :  {startTime} \n     endTime :  {endTime} \n     count :  {count} \n     ended : {boolean}\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime} endTime={endTime}  Function: Return the container statistics stored for this application  {\n     containerStats : [\n        {\n             containerId :  {containerId} \n             timestamp :  {timestamp} \n             stats : {\n                 numOperators :  {numOperators} ,\n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/recordings  Function: Get the list of all recordings for this application  Return:  {\n     recordings : [{\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings  Function: Get the list of recordings on this operator  Return:  {\n     recordings : [ {\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}  Function: Get the information about the recording  Return:  {\n     id :  {id} ,\n     startTime :  {startTime} ,\n     appId :  {appId} ,\n     operatorId :  {operatorId} ,\n     containerId :  {containerId} ,\n     totalTuples :  {totalTuples} ,\n     ports : [ {\n        name :  {portName} ,\n        streamName :  {streamName} ,\n        type :  {type} ,\n        id :  {index} ,\n        tupleCount :  {tupleCount} \n     } \u2026 ],\n     ended : {boolean},\n     windowIdRanges : [ {\n        low :  {lowId} ,\n        high :  {highId} \n     } \u2026 ],\n     properties : {\n        name :  value , ...\n     }\n}  DELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}  Function: Deletes the specified recording  Since: 1.0.4  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples  Query Parameters:  offset\nstartWindow\nlimit\nports\nexecuteEmptyWindow  Function: Get the tuples  Return:  {\n     startOffset :  {startOffset} ,\n     tuples : [ {\n         windowId :  {windowId} ,\n         tuples : [ {\n             portId :  {portId} ,\n             data :  {tupleData} \n        }, \u2026 ]\n    }, \u2026 ]\n}  GET /ws/v2/applications/{appid}/events?from={fromTime} to={toTime} offset={offset} limit={limit}  Function: Get the events  Return:  {\n     events : [ {\n            id :  {id} ,\n         timestamp :  {timestamp} ,\n         type :  {type} ,\n         data : {\n             name :  value , \u2026\n        }\n    }, \u2026 ]\n}  GET /ws/v2/profile/user  Function: Get the user profile information, list of roles and list of\npermissions given the user  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}  GET /ws/v2/profile/settings  Function: Get the current user's settings  Return:  {\n     {key} : {value}, ...\n}  GET /ws/v2/profile/settings/{user}  Function: Get the specified user's settings  Return:  {\n     {key} : {value}, ...\n}  GET /ws/v2/profile/settings/{user}/{key}  Function: Get the specified user's setting key  Return:  {\n     value : {value}\n}  PUT /ws/v2/profile/settings/{user}/{key}  Function: Set the specified user's setting key\nPayload:  {\n     value : {value}\n}  GET /ws/v2/auth/roles  Function: Get the list of roles the system has  Return:  {\n     roles : [\n       {\n          name :  {role1} ,\n          permissions : [  {permission1} , \u2026 ]\n       }, \u2026\n    ]\n}  GET /ws/v2/auth/roles/{role}  Function: Get the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}  PUT /ws/v2/auth/roles/{role}  Function: create or edit the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}  POST /ws/v2/auth/restoreDefaultRoles  Function: Restores default roles  DELETE /ws/v2/auth/roles/{role}  Function: delete the given role  GET /ws/v2/auth/permissions  Function: Get the list of possible permissions  Return:  {\n     permissions : [ {\n        name :  {permissionName} ,\n        adminOnly : true/false\n    }, \u2026 ]\n}  PUT /ws/v2/applications/{appid}/permissions  Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  GET /ws/v2/applications/{appid}/permissions  Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  PUT /ws/v2/appPackages/{owner}/{name}/permissions  Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  GET /ws/v2/appPackages/{owner}/{name}/permissions  Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  POST /ws/v2/licenses  Function: Add a license to the registry or generate an eval license  Payload: The license file content, if payload is empty, it will try to generate an eval license and return the info  Return:  {\n   id :  {licenseId} ,\n   expireTime : {unixTimeMillis},\n   nodesAllowed : {nodesAllowed},\n   memoryMBAllowed : {memoryMBAllowed},\n   contextType :  {contextType} ,\n   type :  {type} ,\n   features : [  {feature1} , \u2026 ]\n}  GET /ws/v2/licenses/current  Function: Get info on the current license  {\n       id :  {licenseId} ,\n       expireTime : {unixTimeMillis},\n       nodesAllowed : {nodesAllowed},\n       nodesUsed : {nodesUsed},\n       memoryMBAllowed : {memoryMBAllowed},\n       memoryMBUsed : {memoryMBUsed},\n       contextType :  {community|standard|enterprise} ,\n       type :  {evaluation|non_production|production} \n       features : [  {feature1} , \u2026 ], // for community, empty array\n       current : true/false\n}  GET /ws/v2/config/installMode  Function: returns the install mode  {\n   installMode :  {evaluation|community|app} ,\n   appPackageName :  {optionalAppPackageName} ,\n   appPackageVersion :  {optionalAppPackageVersion} \n}  GET /ws/v2/config/properties/dt.phoneHome.enable  Function: returns the download type  {\n   value :  true/false \n}  PUT /ws/v2/config/properties/dt.phoneHome.enable  Function:  {\n   value :  true/false \n}  Feature List:     SYSTEM_APPS  SYSTEM_ALERTS  APP_DATA_DASHBOARDS  RUNTIME_DAG_CHANGE  RUNTIME_PROPERTY_CHANGE  APP_CONTAINER_LOGS  LOGGING_LEVELS  APP_DATA_TRACKER  JAAS_LDAP_AUTH  APP_BUILDER   GET /ws/v2/config/properties  Function: Returns list of properties from dt-site.xml.  Return:  {\n     {name} : {\n         value :  {PROPERTY_VALUE} ,\n         description :  {PROPERTY_DESCRIPTION} \n    }\n\n}  GET /ws/v2/config/properties/{PROPERTY_NAME}  Function: Returns single property from dt-site.xml, specify by name  Return:  {\n     value :  {PROPERTY_VALUE} ,\n     description :  {PROPERTY_DESCRIPTION} \n}  POST /ws/v2/config/properties  Function: Overwrites all specified properties in dt-site.xml  Payload:  {\n     properties : [\n        {\n             name :  {name} \n             value :  {PROPERTY_VALUE} ,\n             local : true/false,\n                     description :  {PROPERTY_DESCRIPTION} \n        }, \u2026\n    ]\n}  PUT /ws/v2/config/properties/{PROPERTY_NAME}  Function: Overwrites or creates new property in dt-site.xml\nPayload:  {\n     value :  {PROPERTY_VALUE} ,\n     local : true/false,\n     description :  {PROPERTY_DESCRIPTION} \n}  DELETE /ws/v2/config/properties/{PROPERTY_NAME}  Function: Deletes a property from dt-site.xml. This may have to be\nrestricted to custom properties?  GET /ws/v2/config/hadoopExecutable  Function: Returns the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}  PUT /ws/v2/config/hadoopExecutable  Function: Sets the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}  GET /ws/v2/config/issues  Function: Returns list of potential issues with environment  Return:  {\n     issues : [\n        {\n             key :  {issueKey} ,\n             propertyName :  {PROPERTY_NAME} ,\n             description :  {ISSUE_DESCRIPTION} ,\n             severity :  error | warning \n        },\n        {...},\n        {...}\n    ]    \n}  GET /ws/v2/config/ipAddresses  Function: Returns list of ip addresses the gateway can listen to  Return:  {\n     ipAddresses : [\n       1.2.3.4 , ...\n    ]    \n}  POST /ws/v2/config/restart  Function: Restarts the gateway  Payload: none  GET /proxy/rm/v1/\u2026  POST /proxy/rm/v1/\u2026  Function: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.  GET /proxy/stram/v2/...  POST /proxy/stram/v2/\u2026  PUT /proxy/stram/v2/\u2026  DELETE /proxy/stram/v2/\u2026  Function: Proxy calls to Stram Web Services.  POST /ws/v2/applications/{appid}/loggers  Function: Set the logger levels of packages/classes.  Payload:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}  GET /ws/v2/applications/{appid}/loggers  Function: Gets the logger levels of packages/classes.  Return:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}  GET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"  Function: searches for all classes that match the pattern.  Return:  {\n     loggers  : [\n        {\n             name  :  {fully qualified class name} ,\n             level :  {logger level} \n        }\n    ]\n}  GET /ws/v2/appPackages  Since: 1.0.4  Function: Gets the list of appPackages the user can view in the system  {\n     appPackages : [\n        {\n                  appPackageName :  {appPackageName} ,\n                  appPackageVersion :  {appPackageVersion} ,\n             modificationTime :  {modificationTime} ,\n             owner :  {owner} ,\n        }, ...\n    ]\n}  POST /ws/v2/appPackages?merge={replace|fail|ours|theirs}  Since: 1.0.4  Function: Uploads an appPackage file, merge with existing app package if exists. Default is replace.  Payload: the raw zip file  Return: The information of the app package  GET /ws/v2/appPackages/{owner}/{name}  Since: 1.0.4  Function: Gets the list of versions of appPackages with the given name in the system owned by the specified user  {\n     versions : [\n         1.0-SNAPSHOT \n    ]\n}  DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}  Since: 1.0.4  Function: Deletes the appPackage  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download  Since: 1.0.4  Function: Downloads the appPackage zip file  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}  Since: 1.0.4  Function: Gets the meta information of the app package  Returns:  {\n     appPackageName :  {appPackageName} ,\n     appPackageVersion :  {appPackageVersion} ,\n     modificationTime :   {modificationTime} ,\n     owner :  {owner} ,\n    ...\n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs  Since: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:  {\n     configs : [\n         my-app-conf1.xml \n    ]\n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}  Since: 1.0.4  Function: Gets the properties XML of the specified config  Returns:  configuration \n         property \n                 name ... /name \n                 value ... /value \n         /property \n        \u2026 /configuration   PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}  Since: 1.0.4  Function: Creates or replaces the specified config with the property parameters specified payload  Payload: configuration in XML  DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}  Since: 1.0.4  Function: Deletes the specified config  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications  Since: 1.0.4  Function: Gets the list of applications in the appPackage  Returns:  {\n     applications : [\n        {\n             dag : {dag in json format},\n             file :  {fileName} ,\n             name :  {name} ,\n             type :  {type} ,\n             error :  {error} ,\n             fileContent : {originalFileContentForJSONTypeApp}\n        }\n    ]\n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}  Since: 1.0.4  Function: Gets the meta data for that application  Returns:  {\n     file :  {fileName} ,\n     name :  {name} ,\n     type :  {json/class/properties} ,\n     error :  {error} \n     dag : {\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n         }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n    },\n     fileContent : {originalFileContentForJSONTypeApp}\n}  POST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge  Function: Merge the configuration, json apps, and resources files from the version specified from the payload to the specified app package in the url, without overwriting any existing file in the specified app package  Payload:  {\n  version :  {versionToMergeFrom} \n}  POST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName} originalAppId={originalAppId} queue={queueName}]  Since: 1.0.4  Function: Launches the application with the given configuration specified in the POST payload  Payload:  {\n     {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n     appId :  {appId} \n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}  Since: 1.0.4  Function: Get the properties of the operator given the classname in the jar  {  \n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}  PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]  Function: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app ackage  Payload:  {\n         displayName :  {displayName} ,\n         description :  {description} ,\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n          }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n}  Return:  {\n         error :  {error} \n}  Available port attributes to set:    AUTO_RECORD  IS_OUTPUT_UNIFIED  PARTITION_PARALLEL  QUEUE_CAPACITY  SPIN_MILLIS  STREAM_CODEC  UNIFIER_LIMIT   Available locality options to set:    THREAD_LOCAL  CONTAINER_LOCAL  NODE_LOCAL  RACK_LOCAL   DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}  Since: 1.0.5  Function: Deletes non-jar based application in the app package  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators  Since: 1.0.5  Function: Get the classes of operators from specified app package.  Return:  {  \n     operatorClasses : [  \n        {\n             name : {fullyQualifiedClassName} , \n             title :  {title} ,\n             shortDesc :  {description} ,\n             longDesc :  {description} ,\n             category :  {categoryName} ,\n             doclink :  {doc url} ,\n             tags : [  {tag} ,  {tag} , \u2026 ],\n             inputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ]\n             outputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ],\n             properties : [  \n                {\n                     name : {propertyName} ,\n                     canGet : {canGet},\n                     canSet : {canSet},\n                     type : {type} ,\n                     description : {description} ,\n                     properties : ...\n                }, \u2026\n            ],\n             defaultValue : {\n                 {propertyName} : [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}  GET /ws/v2/appPackages/import  Function: List the importable app packages on Gateway's local file\nsystem  Return:  {\n     appPackages: [\n        {\n             file :  {file} ,\n             name :  {name} ,\n             displayName :  {displayName} ,\n             version :  {version} ,\n             description :  {description} \n        }\n    ]\n}  POST /ws/v2/appPackages/import  Function: Import app package from Gateway's local file system  Payload:  {\n         files : [ {file} , \u2026 ]\n}  PUT /ws/v2/systemAlerts/alerts/{name}  Function: Creates or replaces the specified system alert. The condition has access to an object in its scope called  _topic . An example alert might take the form of the following:  _topic[\"applications.application_1400294100000_0001\"].allocatedContainers   5  Payload:  {\n         condition : {condition in javascript} ,\n         email : {email} ,\n         timeThresholdMillis : {time} \n}  DELETE /ws/v2/systemAlerts/alerts/{name}  Function: Deletes the specified system alert  GET /ws/v2/systemAlerts/alerts?inAlert={true/false}  Function: Gets the created alerts  Return:  {\n     alerts : [{\n         name :  {alertName} ,\n         condition : {condition in javascript} ,\n         email : {email} ,\n         timeThresholdMillis : {time} ,\n         alertStatus : {\n             isInAlert :{true/false}\n             inTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }\n    }, \u2026  ]\n}  GET /ws/v2/systemAlerts/alerts/{name}  Function: Gets the specified system alert  Return:  {\n     name :  {alertName} ,\n     condition : {condition in javascript} ,        \n     email : {email} ,\n     timeThresholdMillis : {time} ,\n     alertStatus : {\n         isInAlert :{true/false}\n         inTime :  {time} ,\n         message :  {message} ,\n         emailSent : {true/false}\n    }\n}  GET /ws/v2/systemAlerts/history  Function: Gets the history of alerts  Return:  {\n     history : [\n        {\n             name : {alertName} ,\n             inTime : {time} ,\n             outTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }, ...\n     ]\n}  GET /ws/v2/systemAlerts/topicData  Function: Gets the topic data that is used for evaluating alert\ncondition  Return:  {\n      {topicName} : {json object data}, ...\n}  GET /ws/v2/auth/users/{user}  Function: Gets the info of the given user  Return:  {\n     userName :  {userName} ,\n     roles : [  {role1} ,  {role2}  ]\n}  POST /ws/v2/auth/users/{user}  Function: Changes password and/or roles of the given user  Return:  {\n     userName :  {userName} ,\n     oldPassword :  {oldPassword} ,\n     newPassword :  {newPassword} ,\n     roles : [  {role1} ,  {role2}  ]\n}  PUT /ws/v2/auth/users/{user}  Function: Creates new user  Return:  {\n     userName :  {userName} ,\n     password :  {password} ,\n     roles : [  {role1} ,  {role2}  ]\n}  DELETE /ws/v2/auth/users/{user}  Function: Deletes the specified user  GET /ws/v2/auth/users  Function: Gets the list of users  Return:  {\n     users : [ {\n        userName :  {username1} ,\n        roles : [  {role1} , \u2026 ],\n        permissions : [  {permission1} , \u2026 ]\n    }\n}  POST /ws/v2/login  Function: Login\nPayload:  {\n     userName :  {userName} ,\n     password :  {password} \n}  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}  POST /ws/v2/logout  Function: Log out the current user  Return:  {\n}", 
            "title": "REST URI Specification"
        }, 
        {
            "location": "/dtgateway_api/#publisher-subscriber-websocket-protocol", 
            "text": "", 
            "title": "Publisher-Subscriber WebSocket Protocol"
        }, 
        {
            "location": "/dtgateway_api/#input", 
            "text": "Publishing  {\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}  Subscribing  {\"type\":\"subscribe\", \"topic\":\"{topic}\"}  Unsubscribing  {\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}  Subscribing to the number of subscribers of a topic  {\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}  Subscribing to the number of subscribers of a topic  {\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Input"
        }, 
        {
            "location": "/dtgateway_api/#output", 
            "text": "Normal Published Data  {\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}  Number of Subscribers:  {\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}", 
            "title": "Output"
        }, 
        {
            "location": "/dtgateway_api/#auto-publish-topics", 
            "text": "data that gets published every one second:   applications  - list of streaming applications running in the cluster  applications.[appid]  - information about a particular application  applications.[appid].containers  - information about containers of a particular application  applications.[appid].physicalOperators  - information about operators of a particular application  applications.[appid].logicalOperators  - information about logical operators of a particular application  applications.[appid].events  - events from the AM of a particularapplication   data that gets published every five seconds:   cluster.metrics  - metrics of the cluster", 
            "title": "Auto publish topics"
        }, 
        {
            "location": "/autometrics/", 
            "text": "Apache Apex AutoMetrics\n\n\nIntroduction\n\n\nMetrics collect various statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of \nAutoMetric\n API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.\n\n\nSpecifying AutoMetrics in an Operator\n\n\nAn \nAutoMetric\n can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a \nget\n method in an operator can be annotated with \n@AutoMetric\n to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.\n\n\npublic class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}\n\n\n\n\nThere are 2 auto-metrics declared in the \nLineReceiver\n. At the end of each application window, the platform will send a map with 2 entries - \n[(length, 100), (count, 10)]\n to the application master.\n\n\nAggregating AutoMetrics across Partitions\n\n\nWhen an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.\n\n\nThe AutoMetric API helps to achieve this by providing an interface for writing aggregators- \nAutoMetric.Aggregator\n. Any implementation of \nAutoMetric.Aggregator\n can be set as an operator attribute - \nMETRICS_AGGREGATOR\n for a particular operator which in turn is used for aggregating physical metrics.\n\n\nDefault aggregators\n\n\nMetricsAggregator\n is a simple implementation of \nAutoMetric.Aggregator\n that platform uses as a default for summing up primitive types - int, long, float and double.\n\n\nMetricsAggregator\n is just a collection of \nSingleMetricAggregator\ns. There are multiple implementations of \nSingleMetricAggregator\n that perform sum, min, max, avg which are present in Apex core and Apex malhar.\n\n\nFor the \nLineReceiver\n operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of \nMetricsAggregator\n that contains two \nLongSumAggregator\ns - one for \nlength\n and one for \ncount\n. This aggregator will report sum of length and sum of count across all the partitions of \nLineReceiver\n.\n\n\nBuilding custom aggregators\n\n\nPlatform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the \nLineReceiver\n was modified to have a complex metric as shown below.\n\n\npublic class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}\n\n\n\n\nBelow is a custom aggregator that can calculate average line length across all partitions of \nAnotherLineReceiver\n.\n\n\npublic class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map\nString, Object\n result = Maps.newHashMap();\n\n  @Override\n  public Map\nString, Object\n aggregate(long l, Collection\nAutoMetric.PhysicalMetricsContext\n collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get(\nlineMetrics\n);\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put(\navgLineLength\n, totalLength/totalCount);\n    return result;\n  }\n}\n\n\n\n\nAn instance of above aggregator can be specified as the \nMETRIC_AGGREGATOR\n for \nAnotherLineReceiver\n while creating the DAG as shown below.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }\n\n\n\n\nRetrieving AutoMetrics\n\n\nThe Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \nautoMetrics\n: {\n       \ncount\n: \n71314\n,\n       \nlength\n: \n27780706\n\n    },\n    \nclassName\n: \ncom.datatorrent.autometric.LineReceiver\n,\n    ...\n}\n\n\n\n\nSystem Metrics\n\n\nSystem metrics are standard operator metrics provided by the system.  Examples include:\n\n\n\n\nprocessed tuples per second\n\n\nemitted tuples per second\n\n\ntotal tuples processed\n\n\ntotal tuples emitted\n\n\nlatency\n\n\nCPU percentage\n\n\nfailure count\n\n\ncheckpoint elapsed time\n\n\n\n\nThe Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,  \n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n    ...\n}\n\n\n\n\nHowever, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of App Data Tracker.\n\n\nApp Data Tracker\n\n\nAs discussed above, STRAM aggregates the AutoMetrics from physical operators (partitions) to something that makes sense in one logical operator.  It pushes the aggregated AutoMetrics values using Websocket to the Gateway at every second along with system metrics for each operator.  Gateway relays the information to an application called App Data Tracker.  It is another Apex application that runs in the background and further aggregates the incoming values by time bucket and stores the values in HDHT.  It also allows the outside to retrieve the aggregated AutoMetrics and system metrics through websocket interface.\n\n\n\n\nApp Data Tracker is enabled by having these properties in dt-site.xml:\n\n\nproperty\n\n  \nname\ndt.appDataTracker.enable\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.appDataTracker.transport\n/name\n\n  \nvalue\nbuiltin:AppDataTrackerFeed\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.attr.METRICS_TRANSPORT\n/name\n\n  \nvalue\nbuiltin:AppDataTrackerFeed\n/value\n\n\n/property\n\n\n\n\n\nAll the applications launched after the App Data Tracker is enabled will have metrics sent to it.\n\n\nNote\n: The App Data Tracker will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.\n\n\nBy default, the time buckets App Data Tracker aggregates upon are one minute, one hour and one day.  It can be overridden by changing the operator attribute \nMETRICS_DIMENSIONS_SCHEME\n.\n\n\nAlso by default, the app data tracker performs all these aggregations: SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute \nMETRICS_DIMENSIONS_SCHEME\n, provided the custom aggregator is known to the App Data Tracker.  (See next section)\n\n\nCustom Aggregator in App Data Tracker\n\n\nCustom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to implement a Custom aggregator you have to do two things:\n\n\n\n\nCombining new inputs with the current aggregation\n\n\nCombining two aggregations together into one aggregation\n\n\n\n\nLet\u2019s consider the case where we want to perform the following rolling average:\n\n\nY_n = \u00bd * X_n + \u00bd * X_n-1 + \u00bc * X_n-2 + \u215b * X_n-3 +...\n\n\nThis aggregation could be performed by the following Custom Aggregator:\n\n\n@Name(\nIIRAVG\n)\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index \n destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}\n\n\n\n\nDiscovery of Custom Aggregators\n\n\nAppDataTracker searches for custom aggregator jars under the following directories statically before launching:\n\n\n\n\n{dt_installation_dir}/plugin/aggregators\n\n\n{user_home_dir}/.dt/plugin/aggregators\n\n\n\n\nIt uses reflection to find all the classes that extend from \nIncrementalAggregator\n and \nOTFAggregator\n in these jars and registers them with the name provided by \n@Name\n annotation (or class name when \n@Name\n is absent).\n\n\nUsing \nMETRICS_DIMENSIONS_SCHEME\n\n\nHere is a sample code snippet on how you can make use of \nMETRICS_DIMENSIONS_SCHEME\n to set your own time buckets and your own set of aggregators for certain \nAutoMetric\ns performed by the App Data Tracker in your application.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] { \n1s\n, \n1m\n, \n1h\n };\n      String[] lengthAggregators = new String[] { \nIIRAVG\n, \nSUM\n };\n      String[] countAggregators = new String[] { \nSUM\n };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if (\nlength\n.equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if (\ncount\n.equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }\n\n\n\n\nDashboards\n\n\nWith App Data Tracker enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the diagram in the App Data Tracker section, dtGateway relays queries and query results to and from the App Data Tracker.  In this way, dtManage sends queries and receives results from the App Data Tracker via dtGateway and uses the results to let the user visualize the data.\n\n\nClick on the visualize button in dtManage's application page.\n\n\n\n\nYou will see the dashboard for the AutoMetrics and the system metrics.\n\n\n\n\nThe left widget shows the AutoMetrics of \nline\n and \ncount\n for the LineReceiver operator.  The right widget shows the system metrics.\n\n\nThe Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "AutoMetric API"
        }, 
        {
            "location": "/autometrics/#apache-apex-autometrics", 
            "text": "", 
            "title": "Apache Apex AutoMetrics"
        }, 
        {
            "location": "/autometrics/#introduction", 
            "text": "Metrics collect various statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of  AutoMetric  API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.", 
            "title": "Introduction"
        }, 
        {
            "location": "/autometrics/#specifying-autometrics-in-an-operator", 
            "text": "An  AutoMetric  can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a  get  method in an operator can be annotated with  @AutoMetric  to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.  public class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}  There are 2 auto-metrics declared in the  LineReceiver . At the end of each application window, the platform will send a map with 2 entries -  [(length, 100), (count, 10)]  to the application master.", 
            "title": "Specifying AutoMetrics in an Operator"
        }, 
        {
            "location": "/autometrics/#aggregating-autometrics-across-partitions", 
            "text": "When an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.  The AutoMetric API helps to achieve this by providing an interface for writing aggregators-  AutoMetric.Aggregator . Any implementation of  AutoMetric.Aggregator  can be set as an operator attribute -  METRICS_AGGREGATOR  for a particular operator which in turn is used for aggregating physical metrics.", 
            "title": "Aggregating AutoMetrics across Partitions"
        }, 
        {
            "location": "/autometrics/#default-aggregators", 
            "text": "MetricsAggregator  is a simple implementation of  AutoMetric.Aggregator  that platform uses as a default for summing up primitive types - int, long, float and double.  MetricsAggregator  is just a collection of  SingleMetricAggregator s. There are multiple implementations of  SingleMetricAggregator  that perform sum, min, max, avg which are present in Apex core and Apex malhar.  For the  LineReceiver  operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of  MetricsAggregator  that contains two  LongSumAggregator s - one for  length  and one for  count . This aggregator will report sum of length and sum of count across all the partitions of  LineReceiver .", 
            "title": "Default aggregators"
        }, 
        {
            "location": "/autometrics/#building-custom-aggregators", 
            "text": "Platform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the  LineReceiver  was modified to have a complex metric as shown below.  public class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}  Below is a custom aggregator that can calculate average line length across all partitions of  AnotherLineReceiver .  public class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map String, Object  result = Maps.newHashMap();\n\n  @Override\n  public Map String, Object  aggregate(long l, Collection AutoMetric.PhysicalMetricsContext  collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get( lineMetrics );\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put( avgLineLength , totalLength/totalCount);\n    return result;\n  }\n}  An instance of above aggregator can be specified as the  METRIC_AGGREGATOR  for  AnotherLineReceiver  while creating the DAG as shown below.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator( LineReceiver , new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }", 
            "title": "Building custom aggregators"
        }, 
        {
            "location": "/autometrics/#retrieving-autometrics", 
            "text": "The Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     autoMetrics : {\n        count :  71314 ,\n        length :  27780706 \n    },\n     className :  com.datatorrent.autometric.LineReceiver ,\n    ...\n}", 
            "title": "Retrieving AutoMetrics"
        }, 
        {
            "location": "/autometrics/#system-metrics", 
            "text": "System metrics are standard operator metrics provided by the system.  Examples include:   processed tuples per second  emitted tuples per second  total tuples processed  total tuples emitted  latency  CPU percentage  failure count  checkpoint elapsed time   The Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     failureCount :  {failureCount} ,\n     latencyMA :  {latencyMA} ,  \n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n    ...\n}  However, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of App Data Tracker.", 
            "title": "System Metrics"
        }, 
        {
            "location": "/autometrics/#app-data-tracker", 
            "text": "As discussed above, STRAM aggregates the AutoMetrics from physical operators (partitions) to something that makes sense in one logical operator.  It pushes the aggregated AutoMetrics values using Websocket to the Gateway at every second along with system metrics for each operator.  Gateway relays the information to an application called App Data Tracker.  It is another Apex application that runs in the background and further aggregates the incoming values by time bucket and stores the values in HDHT.  It also allows the outside to retrieve the aggregated AutoMetrics and system metrics through websocket interface.   App Data Tracker is enabled by having these properties in dt-site.xml:  property \n   name dt.appDataTracker.enable /name \n   value true /value  /property  property \n   name dt.appDataTracker.transport /name \n   value builtin:AppDataTrackerFeed /value  /property  property \n   name dt.attr.METRICS_TRANSPORT /name \n   value builtin:AppDataTrackerFeed /value  /property   All the applications launched after the App Data Tracker is enabled will have metrics sent to it.  Note : The App Data Tracker will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.  By default, the time buckets App Data Tracker aggregates upon are one minute, one hour and one day.  It can be overridden by changing the operator attribute  METRICS_DIMENSIONS_SCHEME .  Also by default, the app data tracker performs all these aggregations: SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute  METRICS_DIMENSIONS_SCHEME , provided the custom aggregator is known to the App Data Tracker.  (See next section)", 
            "title": "App Data Tracker"
        }, 
        {
            "location": "/autometrics/#custom-aggregator-in-app-data-tracker", 
            "text": "Custom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to implement a Custom aggregator you have to do two things:   Combining new inputs with the current aggregation  Combining two aggregations together into one aggregation   Let\u2019s consider the case where we want to perform the following rolling average:  Y_n = \u00bd * X_n + \u00bd * X_n-1 + \u00bc * X_n-2 + \u215b * X_n-3 +...  This aggregation could be performed by the following Custom Aggregator:  @Name( IIRAVG )\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index   destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}", 
            "title": "Custom Aggregator in App Data Tracker"
        }, 
        {
            "location": "/autometrics/#discovery-of-custom-aggregators", 
            "text": "AppDataTracker searches for custom aggregator jars under the following directories statically before launching:   {dt_installation_dir}/plugin/aggregators  {user_home_dir}/.dt/plugin/aggregators   It uses reflection to find all the classes that extend from  IncrementalAggregator  and  OTFAggregator  in these jars and registers them with the name provided by  @Name  annotation (or class name when  @Name  is absent).", 
            "title": "Discovery of Custom Aggregators"
        }, 
        {
            "location": "/autometrics/#using-metrics_dimensions_scheme", 
            "text": "Here is a sample code snippet on how you can make use of  METRICS_DIMENSIONS_SCHEME  to set your own time buckets and your own set of aggregators for certain  AutoMetric s performed by the App Data Tracker in your application.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator( LineReceiver , new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] {  1s ,  1m ,  1h  };\n      String[] lengthAggregators = new String[] {  IIRAVG ,  SUM  };\n      String[] countAggregators = new String[] {  SUM  };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if ( length .equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if ( count .equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }", 
            "title": "Using METRICS_DIMENSIONS_SCHEME"
        }, 
        {
            "location": "/autometrics/#dashboards", 
            "text": "With App Data Tracker enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the diagram in the App Data Tracker section, dtGateway relays queries and query results to and from the App Data Tracker.  In this way, dtManage sends queries and receives results from the App Data Tracker via dtGateway and uses the results to let the user visualize the data.  Click on the visualize button in dtManage's application page.   You will see the dashboard for the AutoMetrics and the system metrics.   The left widget shows the AutoMetrics of  line  and  count  for the LineReceiver operator.  The right widget shows the system metrics.  The Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "Dashboards"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Scalable Design"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtingest/", 
            "text": "dtIngest Tutorial\n\n\n\"dtIngest\" is a DataTorrent application that ingest data from various\nsources and egress the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the \ndtIngest\nblog\n.\n\n\nThis tutorial refers to dtIngest version 1.0.0\n\n\nPre-requisites\n\n\n\n\n\n\nDatatorrent RTS on a Hadoop cluster. Please\n    refer to \nInstallation\n    guide\n\u00a0for\n    details.\n\n\n\n\n\n\nSource and destination file systems must be accessible from all\n    DataTorrent RTS nodes. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.\n\n\n\n\n\n\nIf source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the Hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.\n\n\n\n\n\n\nLaunching dtIngest\n\n\ndtIngest application can be configured and launched from \nDatatorrent\nManagement\nConsole\n.\n\n\n\n\n\n\nNavigate to 'Develop' tab.\n    \n\n\n\n\n\n\nThe dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0\n\n\n\n\n\n\nSelect 'Ingestion Application' from the list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n    \n\n\n\n\n\n\nConfiguration page for dtingest is displayed after the 'launch'.\n    Enter the configuration values and click\n    'Launch' to ingest your data.\n\n\n\n\n\n\nConfiguring dtIngest Instance Properties\n\n\n\n\n\n\nIn the 'Name this application' textbox; name the application instance. For example, 'Ingestion\n    test'\n    \n\n\n\n\n\n\nLeave 'Specify a queue' unchecked to use default queue.\n\n\nIf you want to specify a queue to launch this application, check 'Specify a queue' checkbox and select queue from the\ndropdown. For more information, go to \nHadoop Capacity Scheduler Docs\n\n\n\n\n\n\n\n\nUnder 'Use config a file' option, check the box to use existing configuration file. Select file from drop down to load the configuration file.\n    \n\n\nOnce it is loaded, you can modify the values. You can save the new configuration as a new file or overwrite the existing one. \n\n\nLeave 'Use a config file' unchecked to create a new one. \n\n\n\n\n\n\nConfigure input source, refer to \nConfiguring input\n    source\n\u00a0section for details.\n\n\n\n\n\n\nConfigure output destination, refer to \nConfiguring output\n    destination\n\u00a0section for details.\n\n\n\n\n\n\nConfigure processing steps, refer to \nConfiguring processing\n    steps\n\u00a0section for details.\n\n\n\n\n\n\nUnder 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.\u00a0\n\n\n\n\n\n\nConfiguring Input Source\n\n\nConfiguring HDFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n    \nhdfs://namenode1.cluster.company.org:8020/user/john/data\n.\n\n    Where,\n\n\n\n\nhdfs://\n indicates HDFS protocol\n\n\nnamenode1.cluster.company.org\n indicates fully qualified domain\n   name for the namenode of source HDFS.\n\n\n8020\n indicates port number for HDFS namenode service\n\n\n/user/john/data\n indicates full path for destination directory  \n\n\n\n\n\n\nIf there are more than one directories/file to be ingested, click on\n'Add directory' button and specify complete URL file path to be\ningested.\n\n\n\n\n\n\nIn the 'Filtering criteria' field, specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n    \nJava regular expression  documentation\n.\n     For example, if only \n.log\n files need to be ingested, then use \n.*\\.log\n as regular expression.  \n\n\n\n\nWhere,\n-   \n.*\n indicates any character zero or more times\n-   \n\\.\n indicates dot escaped with backslash\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only '.log' files from the source\ndirectories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you want\n    application to shutdown after completing files ingestion.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change is based on timestamp difference. Entire file will be ingested again in case of\nany change.\n\n\nIf 'Polling' mode is selected, then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.  \n\n\n\n\n\n\n\n\nConfiguring\u00a0NFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'File/NFS' option from the\n    drop-down. \n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL file path.\n\n\nFor example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk5/nfsmount/path/to/data/directory\n.\nWhere,\n-   \nfile://\n indicates that it is some file system mounted on the node.\n-   \n/disk5/nfsmount/\n indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-   \npath/to/data/directory\n is the directory to be ingested\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nIf there are more than one directories to be ingested, click on\n    'Add directory' button and specify complete URL file\n    path.\n\n\n\n\n\n\nIf there are specific files, as opposed to a directory,specify complete\n    URL file path.\n\n\nFor example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt\n.\n\n\n\n\n\n\n\n\nIn the 'Filtering criteria' field, specify regular expression for\n    files to be copied.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.\n\n\n\n\nWhere,\n-   \n.\\*\n indicates any character zero or more times\n-   \n\\\\.\n indicates dot escaped with backslash '\\'\n-   \nlog\n indicates desired extension which is 'log'\n\n\nTherefore, dtingest ingests only \n.log\n files from the source directories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you want application\n    to shutdown after ingesting files currently present in the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change detection is based on timestamp. Entire file will be ingested again in case\nof any change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.\n\n\n\n\n\n\n\n\nConfiguring FTP input\n\n\nThis section gives details about how to ingest files/directories from FTP using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0FTP as input type\n    \n\n\n\n\n\n\nAfter selecting the FTP as input type, snapshot of UI as below:     \n\n\n\n\n\n\nThe format for FTP URL input is as follows:  \nftp://username:password@host:port/path\n\n    where,\n\n\n\n\nftp\n : \u00a0protocol name\n\n\nusername\n : \u00a0username for ftp server\n\n\npassword\n : password\n\n\nhost\n : FTP host\n\n\nport\n : port number\n\n\npath\n : path to either file / directory\n\n\n\n\n\n\nTo copy multiple files/directories, see below: \n\n\n\nTo copy multiple directories, see below:\n\n\n\n\n\n\n\nConfiguring Amazon S3 input\n\n\nFor details on Amazon Simple Storage Service (S3), please go to \nAmazon S3\nDocumentation\n. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0S3 as input type\n\n\n\n\n\n\n\nAfter selecting the S3 as source type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure S3 input url.\n\n\nInput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\n\nwhere,\n- \ns3n\n: \u00a0protocol name\n- \nukey\n: access key\n- \nupass\n: secret access key\n- \nbucketName\n : bucketName\n- \npath\n : path to either file / directory\n\n\n\n\n\n\n\n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n  \n\n\nConfiguring Kafka input\n\n\nFor more details on Kafka, please refer to \nApache Kafka\nDocumentation\n.\n\n\nThis section gives details about how to ingest messages from Kafka using dtIngest.\n\n\n\n\n\n\nSelect Kafka as input type\n    \n\n\n\n\n\n\nAfter selecting Kafka as input type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure topic name and Zookeeper quorum.\n    Zookeeper quorum \u00a0is a string in the form of\n    \nhostname1:port1,hostname2:port2,hostname3:port3\n\n\nwhere,\n\n\n\n\nhostname1,hostname2,hostname3\n are hosts\n\n\nport1,port2,port3\n are ports of zookeeper server\n\n\n\n\ne.g. localhost:2181,localhost:2182\n\n\n\n\n\n\n\nSelect the offset type (default is \u201cLatest\u201d). If\n    you want to consume messages from beginning of Kafka queue, then\n    select \u201cEarliest\u201d offset option.\n\n\n\n\n\n\nIf the topic name is same across the Kafka clusters and want to\n    ingest data from these clusters, then configure the Zookeeper quorum\n    as follows:\n\n\nc1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere,\n- \nc1,c2,c3\n indicates the cluster names,\n- \nhs1,hs2,hs3,hs4,hs5,hs6\n are zookeeper host names\n- \np1,p2,p3,p4,p5,p6\n are corresponding ports.\n\n\nFor\u00a0example, ClusterA and ClusterB are 2 Kafka clusters as below, then\nZookeeper quorum would be as \nClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181\n\n\n\n\n\n\n\n\nConfiguring JMS input\n\n\nThis section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0\n\n\n\n\n\n\nSelect JMS as input type.\n    \n\n\n\n\n\n\nAfter selecting the JMS as source type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure Broker URL and topic name as tcp://hostName:port\n    \n\n\n\n\n\n\nConfiguring Output Destination\n\n\nConfiguring HDFS output\n\n\n\n\n\n\nFor 'Output Location' field, select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Target directory' specify complete HDFS path URL of the destination directory. For example,   \nhdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory\n\n\n\n\n\n\n\n\nWhere,\n    - \nhdfs://\n indicates HDFS protocol\n    - \nnamenode1.cluster.company.org\n indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    - \n:8020\n indicates port number for HDFS namenode service\n    - \n/user/username/path/to/destination/directory\n indicates full path\n    for destination directory.\n\n\n\n\n\n\nUnder 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.\n\n\n\n\n\n\nUnder 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.\n\n\n\n\n\n\nCompact files\n\n\nUse 'Compact files' feature if you want to partition data into fix size. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Vice versa, you can break down a very\nlarge file into partitions of manageable size.\n\n\n\n\n\n\n\n\nSelect 'yes' for radio button under 'Compact files' option. This\n    will display additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.\n\n\n\n\n\n\nSelect delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with \n\\\n. For example, tab character\n    \n\\t\n should be specified as \n\\\\t\n.\n\n\n\n\n\n\nSpecify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.\n\n\n\n\n\n\nNote that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.\n\n\nConfiguring NFS output\n\n\n\n\n\n\nFor \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down  \n\n\n\n\n\n\n\n\nUnder \u2018Target directory\u2019 specify complete NFS path URL of the destination directory.\n\n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n    \nfile:///disk5/nfsmount/path/to/data/directory\n\n\nWhere,\n\n\n\n\nfile://\n indicates that it is some file system mounted on the node.\n\n\n/disk5/nfsmount/\n indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.\n\n\npath/to/data/directory\n is the directory to be ingested\n\n\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nConfiguring FTP output\n\n\n\n\n\n\nSelect FTP as output type.\n    \n\n\n\n\n\n\nAfter selecting FTP as output type then UI looks like as below:   \n\n\n\n\n\n\nSpecify the destination URL below the \u201cOutput directory\u201d label.\n    The FTP Output URL is as follows: \nftp://username:password@host:port/path\n\n\nWhere,\n- \nftp\n : \u00a0protocol name\n- \nusername\n : username for ftp server\n- \npassword\n : password\n- \nhost\n : FTP host\n- \nport\n : port number\n- \npath\n : Directory path to ingested\n\n\n\n\n\n\n\n\nConfiguring Amazon S3 output\n\n\n\n\n\n\nSelect S3 as output type.\n    \n\n\n\n\n\n\nAfter selecting S3 as output then UI looks like as below:\n    \n\n\n\n\n\n\nSpecify URL destination below the 'Output directory' label.\nThe S3 output URL is as follows: \ns3n://ukey:upass@bucketName/path\n\n    Where,\n\n\n\n\ns3n\n\u00a0: protocol name\n\n\nukey\n : access key\n\n\nupass\n :\u00a0secret access key\n\n\nbucketName\n : \u00a0bucketName\n\n\npath\n : Directory path\n\n\n\n\n\n\n\n\nConfiguring Kafka output\n\n\n\n\n\n\nSelect Kafka as output type.\n    \n\n\n\n\n\n\nAfter selecting Kafka as output then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure broker list and topic name.\n\n\n\n\n\n\nConfiguring JMS output\n\n\n\n\n\n\nSelect JMS as output type.\n\n\n\n\n\n\n\nAfter selecting JMS as output type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure Broker URL and topic name as tcp://host:port\n\n\n\n\n\n\n\nConfiguring Processing Steps\n\n\nConfiguring compression\n\n\nSelect compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\nSelect LZO radio button to apply LZO compression\n\n\n\n\n\n\nLzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/\n\n\n\n\nSelect GZIP radio button to apply GZIP compression\n\n\n\n\n\nConfiguring encryption\n\n\nSelect encryption type on configuration page.\n\n\n\n\n\nApply AES encryption:\n\n\n\n\nSelect AES radio button to apply AES encryption\n\n\n\n\n\n\nProvide AES symmetric encryption key in \u201cAES key\u201d text box\n\n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.\n\n\n\n\n\n\n\nApply PKI encryption:  \n\n\n\n\nSelect PKI encryption button to apply PKI encryption  \n\n\nProvide Asymmetric public key to be used for PKI encryption", 
            "title": "dtIngest"
        }, 
        {
            "location": "/dtingest/#dtingest-tutorial", 
            "text": "\"dtIngest\" is a DataTorrent application that ingest data from various\nsources and egress the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the  dtIngest\nblog .  This tutorial refers to dtIngest version 1.0.0", 
            "title": "dtIngest Tutorial"
        }, 
        {
            "location": "/dtingest/#pre-requisites", 
            "text": "Datatorrent RTS on a Hadoop cluster. Please\n    refer to  Installation\n    guide \u00a0for\n    details.    Source and destination file systems must be accessible from all\n    DataTorrent RTS nodes. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.    If source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the Hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/dtingest/#launching-dtingest", 
            "text": "dtIngest application can be configured and launched from  Datatorrent\nManagement\nConsole .    Navigate to 'Develop' tab.\n        The dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0    Select 'Ingestion Application' from the list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n        Configuration page for dtingest is displayed after the 'launch'.\n    Enter the configuration values and click\n    'Launch' to ingest your data.", 
            "title": "Launching dtIngest"
        }, 
        {
            "location": "/dtingest/#configuring-dtingest-instance-properties", 
            "text": "In the 'Name this application' textbox; name the application instance. For example, 'Ingestion\n    test'\n        Leave 'Specify a queue' unchecked to use default queue.  If you want to specify a queue to launch this application, check 'Specify a queue' checkbox and select queue from the\ndropdown. For more information, go to  Hadoop Capacity Scheduler Docs     Under 'Use config a file' option, check the box to use existing configuration file. Select file from drop down to load the configuration file.\n      Once it is loaded, you can modify the values. You can save the new configuration as a new file or overwrite the existing one.   Leave 'Use a config file' unchecked to create a new one.     Configure input source, refer to  Configuring input\n    source \u00a0section for details.    Configure output destination, refer to  Configuring output\n    destination \u00a0section for details.    Configure processing steps, refer to  Configuring processing\n    steps \u00a0section for details.    Under 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.", 
            "title": "Configuring dtIngest Instance Properties"
        }, 
        {
            "location": "/dtingest/#configuring-input-source", 
            "text": "Configuring HDFS input    For 'Input data source' field; select 'HDFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested. \n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n     hdfs://namenode1.cluster.company.org:8020/user/john/data . \n    Where,   hdfs://  indicates HDFS protocol  namenode1.cluster.company.org  indicates fully qualified domain\n   name for the namenode of source HDFS.  8020  indicates port number for HDFS namenode service  /user/john/data  indicates full path for destination directory      If there are more than one directories/file to be ingested, click on\n'Add directory' button and specify complete URL file path to be\ningested.    In the 'Filtering criteria' field, specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n     Java regular expression  documentation .\n     For example, if only  .log  files need to be ingested, then use  .*\\.log  as regular expression.     Where,\n-    .*  indicates any character zero or more times\n-    \\.  indicates dot escaped with backslash\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only '.log' files from the source\ndirectories.    Under 'Runs' field, select 'Single run' if you want\n    application to shutdown after completing files ingestion.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change is based on timestamp difference. Entire file will be ingested again in case of\nany change.  If 'Polling' mode is selected, then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.       Configuring\u00a0NFS input    For 'Input data source' field; select 'File/NFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL file path.  For example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk5/nfsmount/path/to/data/directory .\nWhere,\n-    file://  indicates that it is some file system mounted on the node.\n-    /disk5/nfsmount/  indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-    path/to/data/directory  is the directory to be ingested   Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    If there are more than one directories to be ingested, click on\n    'Add directory' button and specify complete URL file\n    path.    If there are specific files, as opposed to a directory,specify complete\n    URL file path.  For example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt .     In the 'Filtering criteria' field, specify regular expression for\n    files to be copied.\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.   Where,\n-    .\\*  indicates any character zero or more times\n-    \\\\.  indicates dot escaped with backslash '\\'\n-    log  indicates desired extension which is 'log'  Therefore, dtingest ingests only  .log  files from the source directories.    Under 'Runs' field, select 'Single run' if you want application\n    to shutdown after ingesting files currently present in the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for changes. File change detection is based on timestamp. Entire file will be ingested again in case\nof any change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.     Configuring FTP input  This section gives details about how to ingest files/directories from FTP using dtIngest. \u00a0    Select \u00a0FTP as input type\n        After selecting the FTP as input type, snapshot of UI as below:         The format for FTP URL input is as follows:   ftp://username:password@host:port/path \n    where,   ftp  : \u00a0protocol name  username  : \u00a0username for ftp server  password  : password  host  : FTP host  port  : port number  path  : path to either file / directory    To copy multiple files/directories, see below:   To copy multiple directories, see below:    Configuring Amazon S3 input  For details on Amazon Simple Storage Service (S3), please go to  Amazon S3\nDocumentation . This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0    Select \u00a0S3 as input type    After selecting the S3 as source type then UI looks like as below:    Configure S3 input url.  Input url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path  where,\n-  s3n : \u00a0protocol name\n-  ukey : access key\n-  upass : secret access key\n-  bucketName  : bucketName\n-  path  : path to either file / directory    \n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n    Configuring Kafka input  For more details on Kafka, please refer to  Apache Kafka\nDocumentation .  This section gives details about how to ingest messages from Kafka using dtIngest.    Select Kafka as input type\n        After selecting Kafka as input type then UI looks like as below:\n        Configure topic name and Zookeeper quorum.\n    Zookeeper quorum \u00a0is a string in the form of\n     hostname1:port1,hostname2:port2,hostname3:port3  where,   hostname1,hostname2,hostname3  are hosts  port1,port2,port3  are ports of zookeeper server   e.g. localhost:2181,localhost:2182    Select the offset type (default is \u201cLatest\u201d). If\n    you want to consume messages from beginning of Kafka queue, then\n    select \u201cEarliest\u201d offset option.    If the topic name is same across the Kafka clusters and want to\n    ingest data from these clusters, then configure the Zookeeper quorum\n    as follows:  c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where,\n-  c1,c2,c3  indicates the cluster names,\n-  hs1,hs2,hs3,hs4,hs5,hs6  are zookeeper host names\n-  p1,p2,p3,p4,p5,p6  are corresponding ports.  For\u00a0example, ClusterA and ClusterB are 2 Kafka clusters as below, then\nZookeeper quorum would be as  ClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181     Configuring JMS input  This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0    Select JMS as input type.\n        After selecting the JMS as source type then UI looks like as below:\n        Configure Broker URL and topic name as tcp://hostName:port", 
            "title": "Configuring Input Source"
        }, 
        {
            "location": "/dtingest/#configuring-output-destination", 
            "text": "Configuring HDFS output    For 'Output Location' field, select 'HDFS' option from the\n    drop-down.     Under 'Target directory' specify complete HDFS path URL of the destination directory. For example,    hdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory     Where,\n    -  hdfs://  indicates HDFS protocol\n    -  namenode1.cluster.company.org  indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    -  :8020  indicates port number for HDFS namenode service\n    -  /user/username/path/to/destination/directory  indicates full path\n    for destination directory.    Under 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.    Under 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.    Compact files  Use 'Compact files' feature if you want to partition data into fix size. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Vice versa, you can break down a very\nlarge file into partitions of manageable size.     Select 'yes' for radio button under 'Compact files' option. This\n    will display additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.    Select delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with  \\ . For example, tab character\n     \\t  should be specified as  \\\\t .    Specify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.    Note that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.  Configuring NFS output    For \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down       Under \u2018Target directory\u2019 specify complete NFS path URL of the destination directory. \n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n     file:///disk5/nfsmount/path/to/data/directory  Where,   file://  indicates that it is some file system mounted on the node.  /disk5/nfsmount/  indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.  path/to/data/directory  is the directory to be ingested    Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    Configuring FTP output    Select FTP as output type.\n        After selecting FTP as output type then UI looks like as below:       Specify the destination URL below the \u201cOutput directory\u201d label.\n    The FTP Output URL is as follows:  ftp://username:password@host:port/path  Where,\n-  ftp  : \u00a0protocol name\n-  username  : username for ftp server\n-  password  : password\n-  host  : FTP host\n-  port  : port number\n-  path  : Directory path to ingested     Configuring Amazon S3 output    Select S3 as output type.\n        After selecting S3 as output then UI looks like as below:\n        Specify URL destination below the 'Output directory' label.\nThe S3 output URL is as follows:  s3n://ukey:upass@bucketName/path \n    Where,   s3n \u00a0: protocol name  ukey  : access key  upass  :\u00a0secret access key  bucketName  : \u00a0bucketName  path  : Directory path     Configuring Kafka output    Select Kafka as output type.\n        After selecting Kafka as output then UI looks like as below:\n        Configure broker list and topic name.    Configuring JMS output    Select JMS as output type.    After selecting JMS as output type then UI looks like as below:    Configure Broker URL and topic name as tcp://host:port", 
            "title": "Configuring Output Destination"
        }, 
        {
            "location": "/dtingest/#configuring-processing-steps", 
            "text": "Configuring compression  Select compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    Select LZO radio button to apply LZO compression    Lzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/   Select GZIP radio button to apply GZIP compression   Configuring encryption  Select encryption type on configuration page.   Apply AES encryption:   Select AES radio button to apply AES encryption    Provide AES symmetric encryption key in \u201cAES key\u201d text box \n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.    Apply PKI encryption:     Select PKI encryption button to apply PKI encryption    Provide Asymmetric public key to be used for PKI encryption", 
            "title": "Configuring Processing Steps"
        }, 
        {
            "location": "/rts/", 
            "text": "DataTorrent RTS Overview\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using \ndtAssemble\n, a graphical application assembly tool.  Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "RTS"
        }, 
        {
            "location": "/rts/#datatorrent-rts-overview", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using  dtAssemble , a graphical application assembly tool.  Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "DataTorrent RTS Overview"
        }, 
        {
            "location": "/dtmanage/", 
            "text": "dtManage Guide\n\n\nIntroduction\n\n\nThe DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and interact with the DataTorrent platform running on your Hadoop cluster. It is a web-based dashboard that is served by the DataTorrent Gateway, and has five areas: configuration, monitoring, development, and , visualization, and learning.\n\n\nTo download the platform or the VM sandbox, go to \nhttp://www.datatorrent.com/download\n.\n\n\n\n\nConnection Requirements\n\n\nWhen you install DataTorrent RTS on your Hadoop cluster using the installer binary, the DataTorrent Gateway service is started on the node where the installer is executed. By default, the Gateway serves the console from port 9090. The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.\n\n\n\n\nBrowser Requirements\n\n\nThe Console currently supports Chrome, Firefox, Safari, and IE (version 10 and up).\n\n\nInstallation Wizard\n\n\nThe first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:\n\n\n\n\nLocation of the hadoop executable\n\n\nDFS location where all the DataTorrent files are stored\n\n\nDataTorrent license\n\n\nSummary and review of any remaining configuration items\n\n\n\n\n\n\nWhen Kerberos Security is Enabled\n\n\nWhen your hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard: \n\n\n\n\nKerberos Principal\n: The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.\n\n\nKerberos Keytab\n: The location (path) of the Kerberos keytab file to use on the gateway node's local file system.\n\n\nYARN delegation token lifetime\n: If the value of the \nyarn.resourcemanager.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\nNamenode delegation token lifetime\n: If the value of the \ndfs.namenode.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\n\n\n\n\nNote:\n The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.\n\n\n\n\nConfigure Tab\n\n\nThe configuration page can be found by clicking the \u201cConfigure\u201d link in the main navigation bar at the top. There are links to various tools to help you configure and troubleshoot your DataTorrent installation. \nThe configuration page links may differ depending on your cluster setup. The following is a screenshot with a cluster that has simple authentication/authorization enabled.\n\n\n\n\nSystem Configuration\n\n\nThis page shows diagnostic information regarding the gateway and console, as well as any issues that the gateway may detect.\n\n\n\n\nIn addition, you can perform the following actions from this page:\n\n\nRestart the Gateway\n\n\n\nThis can be useful when Hadoop configuration has changed or some other factor of your cluster environment has changed.\n\n\nToggle Reporting\n\n\n\n\nIf enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.\n\n\nLicense Information\n\n\nUse the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.\n\n\n\n\nUser Profile\n\n\nThe User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:\n\n\n\n\nchange password \n\n\nchange the default home page\n\n\nchange the theme of the console\n\n\nrestore the default options of the console\n\n\n\n\n\n\nUser Management\n\n\nUse this page to manage users and roles of your DataTorrent cluster:\n\n\n\n\nadd users\n\n\nchange users\u2019 roles\n\n\nchange users\u2019 password\n\n\ndelete users\n\n\nadd roles\n\n\nedit role permissions\n\n\ndelete roles\n\n\n\n\n\n\n\n\nNote:\n With most authentication schemes, the admin role cannot be deleted.\n\n\n\n\nInstallation Wizard\n\n\nAt any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.\n\n\nDevelop Tab\n\n\nThe development area of dtManage is mainly geared towards the creation, upload, configuration, and launch of DataTorrent applications. The development home can be viewed by clicking the \u201cDevelop\u201d tab in the main navigation bar on the top of the screen. A prerequisite to using the development tools of the UI is an understanding of what Apex Application Packages are. For more information, see the \nApplication Packages Guide\n.\n\n\n\n\nApplication Packages\n\n\nTo access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operators directly on application packages:\n\n\n\n\nDownload the app package\n\n\nDelete the app package\n\n\nCreate a new application in an application package via dtAssemble (requires enterprise license)\n\n\nLaunch applications in the app package\n\n\nImport default packages (see below)\n\n\n\n\n\n\n\n\nNote:\n If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.\n\n\n\n\nImporting Default Packages\n\n\nWhen you install the DataTorrent platform, a folder located in the installation directory called \ndemos/app-packages\n will contain various default app packages that can be imported into HDFS for use. Just above the list of Application Packages, there should be a button that says \nImport default packages\n. Clicking this will take you to a page that shows the list of these demo app packages. Select one or more to import and click the \nImport\n button. This will upload the selected app package to HDFS.\n\n\n\n\nApplication Package Page\n\n\nOnce you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.\n\n\n\n\nAside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package. \n\n\nLaunching Apps\n\n\nTo launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options: \n\n\n\n\nSpecify a name for the running app\n\n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Just be weary that it must be unique compared to the other applications running on your DataTorrent installation.\n\n\nSpecify the \nscheduler queue\n\n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be \nroot.[USER_NAME]\n.\n\n\nUse a config file when launching\n\n  App Package config files are xml files that contain \nproperties\n that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.\n\n\nSpecify custom properties\n\n  In addition to choosing a config file, you may also specify properties directly in the launch popup by selecting this option. Note that there are three helpful functions when specifying custom properties:\n\n\nadd required properties\n - App Packages can have required properties (for example, twitter API access keys for the twitter demo). This function adds a new property with the required property name to the form, making it easy to fill in.\n\n\nadd default properties\n - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults\n\n\nsave this configuration as\u2026\n - This function creates a new config file and saves it to the App Package, which can then be used later to launch the app with. That way, you can relaunch the app with the same properties without having to re-enter them.\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n For more information about config files and custom properties, see the \nApplication Packages Guide\n\n\n\n\nViewing an Application DAG\n\n\nAll DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications.\n\n\n\n\nCreating apps with dtAssemble\n\n\nIf you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the \ndtAssemble guide\n.\n\n\nMonitor Tab\n\n\nThe main operations dashboard can be visited by clicking on the \u201cMonitor\u201d link in the main top navigation bar. This section of the Console can be used to monitor, debug, and kill running DataTorrent applications.\n\n\nOperations Home\n\n\nThe operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.\n\n\n\n\nThe cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of: \nretrieve ended apps\n and \ninclude system apps\n. The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.\n\n\nInstance Page\n\n\nTo get to an application instance page, click on either the app name or the app id in the list of running applications.\n\n\n\n\nAll sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:\n\n\n\n\nThere are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations (\nlogical\n, \nphysical\n, \nphysical-dag-view\n, \nmetric-view\n) will suffice. The following is a list of widgets available on an app instance page:\n\n\nApplication Overview Widget\n\n\nAll the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:\n\n\n\n\nThe \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.\n\n\nYou can also use the \nset logging level\n button on this widget to specify what logging level gets written to the dt.log files. \n\n\n\n\nYou will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:\n\n\n\n\nStram Events Widget\n\n\nEach application has a stream of notable events that can be viewed with the StrAM Events widget:\n\n\n\n\nSome events have additional information attached to it, which can be viewed by clicking the \u201cdetails\u201d button in the list:\n\n\n\n\nLogical DAG Widget\n\n\nThis widget visualizes the logical plan of the application being viewed:\n\n\n\n\nAdditionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown. \n\n\n\n\nPro tip:\n Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.\n\n\n\n\nPhysical DAG Widget\n\n\nThis is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.\n\n\n\n\nSame-colored physical operators in this widget indicates that these operators are in the same container.\n\n\nLogical Operators List Widget\n\n\nThis widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing. \n\n\nOne nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:\n\n\n\n\nPhysical Operators List Widget\n\n\nShows the physical operators in the application.\n\n\nContainers List Widget\n\n\nShows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.\n\n\nLogical Streams List Widget\n\n\nShows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.\n\n\nMetrics Chart\n\n\nShows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.\n\n\nRecording and Viewing Sample Tuples\n\n\nThere is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):\n\n\n\n\nViewing Logs\n\n\nAnother useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:\n\n\n\n\nOnce you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:\n\n\n\n\nThere are numerous improvements in store for dtManage, and user feedback is highly valued in the planning, so please provide any that will help in your usage of the tool!\n\n\n~The DataTorrent UI Team", 
            "title": "dtManage"
        }, 
        {
            "location": "/dtmanage/#dtmanage-guide", 
            "text": "", 
            "title": "dtManage Guide"
        }, 
        {
            "location": "/dtmanage/#introduction", 
            "text": "The DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and interact with the DataTorrent platform running on your Hadoop cluster. It is a web-based dashboard that is served by the DataTorrent Gateway, and has five areas: configuration, monitoring, development, and , visualization, and learning.  To download the platform or the VM sandbox, go to  http://www.datatorrent.com/download .   Connection Requirements  When you install DataTorrent RTS on your Hadoop cluster using the installer binary, the DataTorrent Gateway service is started on the node where the installer is executed. By default, the Gateway serves the console from port 9090. The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.   Browser Requirements  The Console currently supports Chrome, Firefox, Safari, and IE (version 10 and up).  Installation Wizard  The first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:   Location of the hadoop executable  DFS location where all the DataTorrent files are stored  DataTorrent license  Summary and review of any remaining configuration items    When Kerberos Security is Enabled  When your hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard:    Kerberos Principal : The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.  Kerberos Keytab : The location (path) of the Kerberos keytab file to use on the gateway node's local file system.  YARN delegation token lifetime : If the value of the  yarn.resourcemanager.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.  Namenode delegation token lifetime : If the value of the  dfs.namenode.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.    Note:  The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.", 
            "title": "Introduction"
        }, 
        {
            "location": "/dtmanage/#configure-tab", 
            "text": "The configuration page can be found by clicking the \u201cConfigure\u201d link in the main navigation bar at the top. There are links to various tools to help you configure and troubleshoot your DataTorrent installation.  The configuration page links may differ depending on your cluster setup. The following is a screenshot with a cluster that has simple authentication/authorization enabled.   System Configuration  This page shows diagnostic information regarding the gateway and console, as well as any issues that the gateway may detect.   In addition, you can perform the following actions from this page:  Restart the Gateway  \nThis can be useful when Hadoop configuration has changed or some other factor of your cluster environment has changed.  Toggle Reporting   If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.  License Information  Use the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.   User Profile  The User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:   change password   change the default home page  change the theme of the console  restore the default options of the console    User Management  Use this page to manage users and roles of your DataTorrent cluster:   add users  change users\u2019 roles  change users\u2019 password  delete users  add roles  edit role permissions  delete roles     Note:  With most authentication schemes, the admin role cannot be deleted.   Installation Wizard  At any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.", 
            "title": "Configure Tab"
        }, 
        {
            "location": "/dtmanage/#develop-tab", 
            "text": "The development area of dtManage is mainly geared towards the creation, upload, configuration, and launch of DataTorrent applications. The development home can be viewed by clicking the \u201cDevelop\u201d tab in the main navigation bar on the top of the screen. A prerequisite to using the development tools of the UI is an understanding of what Apex Application Packages are. For more information, see the  Application Packages Guide .   Application Packages  To access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operators directly on application packages:   Download the app package  Delete the app package  Create a new application in an application package via dtAssemble (requires enterprise license)  Launch applications in the app package  Import default packages (see below)     Note:  If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.   Importing Default Packages  When you install the DataTorrent platform, a folder located in the installation directory called  demos/app-packages  will contain various default app packages that can be imported into HDFS for use. Just above the list of Application Packages, there should be a button that says  Import default packages . Clicking this will take you to a page that shows the list of these demo app packages. Select one or more to import and click the  Import  button. This will upload the selected app package to HDFS.   Application Package Page  Once you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.   Aside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package.   Launching Apps  To launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options:    Specify a name for the running app \n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Just be weary that it must be unique compared to the other applications running on your DataTorrent installation.  Specify the  scheduler queue \n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be  root.[USER_NAME] .  Use a config file when launching \n  App Package config files are xml files that contain  properties  that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.  Specify custom properties \n  In addition to choosing a config file, you may also specify properties directly in the launch popup by selecting this option. Note that there are three helpful functions when specifying custom properties:  add required properties  - App Packages can have required properties (for example, twitter API access keys for the twitter demo). This function adds a new property with the required property name to the form, making it easy to fill in.  add default properties  - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults  save this configuration as\u2026  - This function creates a new config file and saves it to the App Package, which can then be used later to launch the app with. That way, you can relaunch the app with the same properties without having to re-enter them.       Note:  For more information about config files and custom properties, see the  Application Packages Guide   Viewing an Application DAG  All DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications.   Creating apps with dtAssemble  If you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the  dtAssemble guide .", 
            "title": "Develop Tab"
        }, 
        {
            "location": "/dtmanage/#monitor-tab", 
            "text": "The main operations dashboard can be visited by clicking on the \u201cMonitor\u201d link in the main top navigation bar. This section of the Console can be used to monitor, debug, and kill running DataTorrent applications.  Operations Home  The operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.   The cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of:  retrieve ended apps  and  include system apps . The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.  Instance Page  To get to an application instance page, click on either the app name or the app id in the list of running applications.   All sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:   There are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations ( logical ,  physical ,  physical-dag-view ,  metric-view ) will suffice. The following is a list of widgets available on an app instance page:  Application Overview Widget  All the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:   The \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.  You can also use the  set logging level  button on this widget to specify what logging level gets written to the dt.log files.    You will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:   Stram Events Widget  Each application has a stream of notable events that can be viewed with the StrAM Events widget:   Some events have additional information attached to it, which can be viewed by clicking the \u201cdetails\u201d button in the list:   Logical DAG Widget  This widget visualizes the logical plan of the application being viewed:   Additionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown.    Pro tip:  Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.   Physical DAG Widget  This is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.   Same-colored physical operators in this widget indicates that these operators are in the same container.  Logical Operators List Widget  This widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing.   One nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:   Physical Operators List Widget  Shows the physical operators in the application.  Containers List Widget  Shows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.  Logical Streams List Widget  Shows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.  Metrics Chart  Shows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.  Recording and Viewing Sample Tuples  There is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):   Viewing Logs  Another useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:   Once you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:   There are numerous improvements in store for dtManage, and user feedback is highly valued in the planning, so please provide any that will help in your usage of the tool!  ~The DataTorrent UI Team", 
            "title": "Monitor Tab"
        }, 
        {
            "location": "/dtassemble/", 
            "text": "dtAssemble - Graphical Application Builder\n\n\nThe dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application. \n\n\n\n\nAccessing the Builder\n\n\nTo get to the App Builder, you must perform the following steps (illustrated by GIF):\n\n\n\n\n\n\nCreate a DataTorrent Application Package\n  To do this, read through the \nApplication Packages Guide\n, which provides step-by-step instructions on how to do this.\n\n\nUpload it to HDFS using the Console\n  Click on the \nDevelop\n link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.\n\n\nAdd a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.\n\n\nDrag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d\n\n\nConfigure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.\n\n\nConnect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.\n\n\n\n\nUsing Application Builder\n\n\nThe Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:\n\n\n\n\nOperator Library Navigator\n\n\n\n\nUse this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder\nsection). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.\n\n\nOnce you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.\n\n\n\n\nCanvas\n\n\n\n\nThe Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder\nsection.\n\n\n\n\nInspector\n\n\nThe Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.\n\n\nOperator Inspector\n\n\n\n\nWhen an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties. \n\n\nPort Inspector\n\n\nWhen a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port). \n\n\nStream Inspector\n\n\n\n\nThe stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.\n\n\nPrepping Operators for the Application Builder\n\n\nThe way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:\n\n\n\n\nOperators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.\n\n\n\n\nUse javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.\n\n\n/**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }\n\n\n\n\n\n\n\nEvery property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended\n\n\n/**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\n\n\nUtilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.  \n\n\n\n\nWhen a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.\n\n\nMark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization.\n/**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\nMake the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.\n\n\n\n\n\n\n\n\nApp Builder Usage Examples\n\n\nPi Demo\n\n\nAs an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application. \n\n\n\n\n\n\nFirst you will need to clone the malhar repository:\n\n\ngit@github.com:DataTorrent/Malhar.git \n cd Malhar\n\n\n\n\n\n\n\nThen, run maven install, which will create the App Package jar need to upload to your gateway.\n\n\n\n\nFollow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.\n\n\nAdd the Console Output\noperators.\n\n\nConnect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.\n\n\nClick the \u201claunch\u201d button in the top left once it turns purple.\n\n\nIn the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.\n\n\nTo see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "dtAssemble"
        }, 
        {
            "location": "/dtassemble/#dtassemble-graphical-application-builder", 
            "text": "The dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application.", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#accessing-the-builder", 
            "text": "To get to the App Builder, you must perform the following steps (illustrated by GIF):    Create a DataTorrent Application Package\n  To do this, read through the  Application Packages Guide , which provides step-by-step instructions on how to do this.  Upload it to HDFS using the Console\n  Click on the  Develop  link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.  Add a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.  Drag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d  Configure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.  Connect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.", 
            "title": "Accessing the Builder"
        }, 
        {
            "location": "/dtassemble/#using-application-builder", 
            "text": "The Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:   Operator Library Navigator   Use this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder section). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.  Once you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.   Canvas   The Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder section.   Inspector  The Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.  Operator Inspector   When an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties.   Port Inspector  When a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).   Stream Inspector   The stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.", 
            "title": "Using Application Builder"
        }, 
        {
            "location": "/dtassemble/#prepping-operators-for-the-application-builder", 
            "text": "The way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:   Operators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.   Use javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.  /**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }    Every property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended  /**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }    Utilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.     When a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.  Mark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization. /**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }   Make the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.", 
            "title": "Prepping Operators for the Application Builder"
        }, 
        {
            "location": "/dtassemble/#app-builder-usage-examples", 
            "text": "Pi Demo  As an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application.     First you will need to clone the malhar repository:  git@github.com:DataTorrent/Malhar.git   cd Malhar    Then, run maven install, which will create the App Package jar need to upload to your gateway.   Follow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.  Add the Console Output operators.  Connect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.  Click the \u201claunch\u201d button in the top left once it turns purple.  In the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.  To see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "App Builder Usage Examples"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.\n\n\nAnd below is the DAG for the Ads Dimension Demo:\n\n\n\n\nIn this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.\n\n\nAll these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.  \nApp Data Framework Programming Guide\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new button on the Dashboards screen\n\n\ngenerate new dashboard option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nSelect compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.\n\n\n\n\n\n\nChoose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nModifying Dashboards\n\n\nDashboards controls are presented as a row of buttons just below the dashboard title and description.\n\n\n\n\nNew widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget \n\n\nAlternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.\n\n\n\n\nWhether using auto generate buttons, results are not persisted until save changes is applied.\n\n\nEach data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "Twitter Example  The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:   Ads Dimension Example  The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.  And below is the DAG for the Ads Dimension Demo:   In this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.  All these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.   App Data Framework Programming Guide", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.   Below is an example of accessing the data visualization dashboard from a running application.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new button on the Dashboards screen  generate new dashboard option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)    Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Select compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.    Choose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#modifying-dashboards", 
            "text": "Dashboards controls are presented as a row of buttons just below the dashboard title and description.   New widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.", 
            "title": "Modifying Dashboards"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget   Alternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.   Whether using auto generate buttons, results are not persisted until save changes is applied.  Each data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "dtGateway"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/apex/", 
            "text": "Apache Apex\n\n\nApache Apex (incubating) is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex/#apache-apex", 
            "text": "Apache Apex (incubating) is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.\n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.\n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.\n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows.\n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files, syslog, etc.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nMalhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Apache Apex-Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.  Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.   Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.  Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.  Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files, syslog, etc.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "Malhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Social Media"
        }, 
        {
            "location": "/installation/", 
            "text": "DataTorrent RTS Installation Guide\n\n\nThis guide covers installation of the DataTorrent RTS platform.\n\n\nRequirements\n\n\n\n\nLinux operating system (tested on CentOS 6.x and Ubuntu 12.04)\n\n\nJava 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)\n\n\nHadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nGoogle Chrome, Firefox, or Safari to access the DataTorrent Console UI\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\n\n\nInstallation\n\n\nComplete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]\n\n\nBinaries are available in /opt/datatorrent/current/bin and links in /usr/bin\n\n\nConfiguration files located in /opt/datatorrent/current/conf\n\n\nLog files located in /var/log/datatorrent\n\n\nDataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\na.  Installing from self-extracting archive (*.bin)\n\n\n    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin\n\n\n\nb.  Installing from RedHat Package Manager archive (*.rpm)\n\n\n  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm\n\n\n\nLimited Local Installation\n\n\nA limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]\n\n\nBinaries are available under $HOME/datatorrent/current/bin\n\n\nConfiguration files located under $HOME/datatorrent/conf\n\n\nLog files located under $HOME/.dt/logs\n\n\n\n\nDataTorrent Gateway running as current user, and managed with dtgateway command\n\n\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\ncurl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin\n\n\n\n\n\n\n\nAdd DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.\n\n\nDATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin\n\n\n\n\n\n\n\nUpgrades\n\n\nDataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.\n\n\nAutomatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.  \n\n\nFull uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.\n\n\nCustomizing Installation\n\n\nVarious options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.\n\n\n./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B \npath\n      Use \npath\n as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U \nuser\n      Use \nuser\n user account for installation.  Default: dtadmin\n-G \ngroup\n     Use \ngroup\n group for installation.  Default: dtadmin ( based on value of \nuser\n )\n-H \npath\n      Use \npath\n for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E \nexpr\n      Adds export \nexpr\n to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway  and dtcli\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s \nfile\n      Use \nfile\n DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e \nfile\n      Use \nfile\n DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.\n\n\n\nSome Hadoop distributions may rquire changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):\n\n\nsudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.\n\n\n\nIf JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:\n\n\nsudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path\n\n\n\nIn some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.\n\n\nsudo ./datatorrent-rts*.bin -U myuser\n\n\n\nInstallation Wizard\n\n\nAfter the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.\n\n\nhttp://\ninstallation_host\n:9090/\n\n\n\nThe Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.\n\n\nDataTorrent installation can be verified by running included demo applications.  See \nLaunching Demo Applications\n for details.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#datatorrent-rts-installation-guide", 
            "text": "This guide covers installation of the DataTorrent RTS platform.", 
            "title": "DataTorrent RTS Installation Guide"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "Linux operating system (tested on CentOS 6.x and Ubuntu 12.04)  Java 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)  Hadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH  Minimum of 8G RAM available on the Hadoop cluster  Google Chrome, Firefox, or Safari to access the DataTorrent Console UI  Permissions to create HDFS directory for DataTorrent user", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Complete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following   DataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]  Binaries are available in /opt/datatorrent/current/bin and links in /usr/bin  Configuration files located in /opt/datatorrent/current/conf  Log files located in /var/log/datatorrent  DataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command   Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  a.  Installing from self-extracting archive (*.bin)      curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin  b.  Installing from RedHat Package Manager archive (*.rpm)    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#limited-local-installation", 
            "text": "A limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.   DataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]  Binaries are available under $HOME/datatorrent/current/bin  Configuration files located under $HOME/datatorrent/conf  Log files located under $HOME/.dt/logs   DataTorrent Gateway running as current user, and managed with dtgateway command    Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin    Add DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.  DATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin", 
            "title": "Limited Local Installation"
        }, 
        {
            "location": "/installation/#upgrades", 
            "text": "DataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.  Automatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.    Full uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.", 
            "title": "Upgrades"
        }, 
        {
            "location": "/installation/#customizing-installation", 
            "text": "Various options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.  ./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B  path       Use  path  as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U  user       Use  user  user account for installation.  Default: dtadmin\n-G  group      Use  group  group for installation.  Default: dtadmin ( based on value of  user  )\n-H  path       Use  path  for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E  expr       Adds export  expr  to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway  and dtcli\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s  file       Use  file  DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e  file       Use  file  DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.  Some Hadoop distributions may rquire changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):  sudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.  If JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:  sudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path  In some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.  sudo ./datatorrent-rts*.bin -U myuser", 
            "title": "Customizing Installation"
        }, 
        {
            "location": "/installation/#installation-wizard", 
            "text": "After the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.  http:// installation_host :9090/  The Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.  DataTorrent installation can be verified by running included demo applications.  See  Launching Demo Applications  for details.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/configuration/", 
            "text": "Coming Soon", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtgateway_security/", 
            "text": "DataTorrent Gateway Security\n\n\nDataTorrent Gateway supports different authentication mechanisms to\nsecure access to the Console. The supported types are local password\nauthentication, kerberos authentication and J\n\n\nConfiguring Authentication\n\n\nAfter DataTorrent RTS installation, you can turn on authentication and\nauthorization support to secure the DataTorrent Gateway. Gateway\nsupports three types of authentication.\n\n\n\n\nPassword\n\n\nKerberos\n\n\nJAAS\n\n\n\n\nEnabling Password Auth\n\n\n\u201cPassword\u201d is the only authentication mechanism presented here that does\nnot depend on any external systems.  When enabled, all users will be\npresented with the login prompt before being able to use the DT Console.\nPassword authentication can be enabled by performing following two\nsteps.\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n    \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\npassword\n/value\n\n    \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n( when running Gateway in local mode use  dtgateway restart command)\n\n\n\n\n\n\nOpen the Gateway URL in your browser, and you should be prompted for\nuser name and password.  Starting with DataTorrent RTS 2.0.0, the\ndefault username and password is \ndtadmin\n and \ndtadmin\n.\n\n\n\n\nOnce authenticated, active user name and an option to log out is\npresented in the top right corner of the DT Console screen.\n\n\n\n\nEnabling Kerberos Auth\n\n\nKerberos authentication can optionally be enabled for Hadoop web access.\nIf this is configured then all web browser access to the Hadoop\nmanagement consoles is Kerberos authenticated. Access to all the web\nservices is also Kerberos authenticated. This Kerberos authentication is\nperformed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.\n\n\nKerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by\nlogging in to kerberos system in a terminal emulator using kinit. Then\nthe user launches a browser and accesses the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.\n\n\nWhen this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the \nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nAdditional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install)\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\nkerberos\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.principal\n/name\n\n    \nvalue\n{kerberos-principal-of-web-service}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.keytab\n/name\n\n    \nvalue\n{absolute-path-to-keytab-file}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.token.validity\n/name\n\n    \nvalue\n{authentication-token-validity-in-seconds}\n/value\n\n  \n/property\n\n  \nproperty\n\n  \nname\\\ndt.gateway.http.authentication.cookie.domain\n/name\n\n  \nvalue\\\n{http-cookie-domain-for-authentication-token}\n/value\n\n  \nproperty\n\n    \nname\\\ndt.gateway.http.authentication.cookie.path\n/name\n\n    \nvalue\n{http-cookie-path}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\\\ndt.gateway.http.authentication.signature.secret\n/name\n\n    \nvalue\n{absolute-path-of-secret-file-for-signing-authentication-tokens} \n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nNote that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property\n\ndt.gateway.http.authentication.type\n should be replaced with the\nappropriate values for your setup.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nEnabling JAAS Auth\n\n\nJAAS is Java Authentication and Authorization Service. It is a pluggable\nand extensible mechanism for authentication. It is a generic framework\nand the actual authentication is performed by a JAAS plugin module which\ncan be configured using a configuration file. Among others LDAP and PAM\nauthentication can be performed via JAAS.\n\n\nSimilar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the\n\nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nThis authentication can be enabled by performing the following general\nsteps. The specific steps for a couple of authentication mechanisms LDAP\nand PAM are shown in the next sections but other authentication\nmechanisms that have a JAAS plugin module can also be used.\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or\n    \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.type\n/name\n\n      \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\\\n\n      \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n      \nvalue\nname-of-jaas-module\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\nThe \ndt.gateway.http.authentication.jaas.name\n property specifies the\nplugin module to use with JAAS and the value should be the name of the\nplugin module.\n\n\n\n\n\n\nThe name of the plugin module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be \n/home/dtadmin/.java.login.config\n, if running as a\n    normal user it would be \n~/.java.login.config\n. The sample\n    configurations for LDAP and PAM are shown in the next sections.\n\n\n\n\n\n\nThe classes for the plugin module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.\n\n\nThe following step shows how to do this\n\n\na.  Edit the \ncustom-env.sh\n configuration file, typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like\n\n\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nLDAP\n\n\nLDAP authentication is a directory based authentication mechanism used\nin many enterprises. To enable LDAP authentication following are the\nspecifics for the configuration steps described above.\n\n\n\n\n\n\nFor step 1 above specify LDAP as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be \u201cldap\u201d\n    (without the quotes). This name should now be configured with the\n    appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, the JAAS name specified above should be configured with\n    the appropriate LDAP settings in the .java.login.config file. A\n    sample configuration is shown below\n\n\nldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case\n\u201cldap\u201d must match the jaas name specified in step 1. The first property\nwithin the braces \ncom.sun.security.auth.module.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The next settings are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.\n\n\nPAM\n\n\nPAM is Pluggable Authentication Module. It is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules\nthey can be used in Gateway as well.\n\n\nPAM is implemented in C language and has C API. JPam is Java PAM bridge\nthat uses JNI to make PAM calls. It is available here\n\nhttp://jpam.sourceforge.net/\n and has detailed documentation on how to install and set it up. JPam also has a JAAS plugin module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS plugin module is available.\n\n\nTo enable JPAM following are the specifics for the configuration steps\nto enable JAAS authentication described above.\n\n\n\n\n\n\nJPAM has to be first installed on the system. Please following the\n    installation instructions from the JPAM website.\n\n\n\n\n\n\nFor step 1 above Specify JPAM as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be\n    \u201cnet-sf-jpam\u201d (without the quotes). This name should now be\n    configured with the appropriate settings as described in the next\n    step.\n\n\n\n\n\n\nFor step 2 the JAAS name specified above should be configured with\n    the appropriate JPAM settings in the .java.login.config file. A\n    sample configuration is shown below\n\n\nnet-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case\n\u201cnet-sf-jpam\u201d must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM specific settings. The\nserviceName setting for example specifies the PAM service which would\nneed to be further configured in /etc/pam.d/net-sf-jpam to specify the\nPAM modules to use. Refer to PAM documentation on how to configure a PAM\nservice with PAM modules. If using Linux local accounts system-auth\ncould be specified as the PAM module in this file. The above settings\nare only provided as a reference example and a different serviceName for\nexample can be chosen.\n\n\n\n\nFor step 3 add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format \nJPam-\nversion\n.jar\n where\n   \nversion\n denotes the version, version 1.1 has been tested.\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/JPam-\\\nversion\\\n.jar\n\n\n\n\n\n\n\nGroups\n\n\nFor group support such as using LDAP groups for authorization refer to\nthe \nAuthorization using external roles\n section below.\n\n\nConfiguring Authorization\n\n\nWhen any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.\n\n\nPermissions\n\n\nThe list of all possible permissions in the DT Gateway is as follow:\n\n\n ACCESS_RM_PROXY\n\n\nAllow HTTP proxying requests to YARN\u2019s Resource Manager REST API\n\n\n EDIT_GLOBAL_CONFIG\n\n\nEdit global settings\n\n\n EDIT_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\n LAUNCH_APPS\n\n\nLaunch Apps\n\n\nMANAGE_LICENSES\n\n\nManage DataTorrent RTS licenses\n\n\n MANAGE_OTHER_USERS_APPS\n\n\nManage (e.g. edit, kill, etc) applications launched by other users\n\n\n MANAGE_OTHER_USERS_APP_PACKAGES\n\n\nManage App Packages uploaded by other users  \n\n\n MANAGE_ROLES\n\n\nManage roles (create/delete roles, or assign permissions to roles)\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nManage system alerts\n\n\n MANAGE_USERS\n\n\nManage users (create/delete users, change password)\n\n\n UPLOAD_APP_PACKAGES\n\n\nUpload App Packages and use the app builder\n\n\n VIEW_GLOBAL_CONFIG\n\n\nView global settings   \n\n\n VIEW_LICENSES\n\n\nView DataTorrent RTS licenses\n\n\n VIEW_OTHER_USERS_APPS\n\n\nView applications launched by others\n\n\n VIEW_OTHER_USERS_APP_PACKAGES\n\n\nView App Packages uploaded by other users\n\n\n VIEW_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\n VIEW_SYSTEM_ALERTS\n\n\nView system alerts\n\n\nDefault Roles\n\n\nDataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.\n\n\nAdmin\n\n\nAn administrator of DataTorrent RTS is intended to be able to install,\nmanage \n modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.\n\n\nOperator\n\n\nOperators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.\n\n\nHere is the list of default permissions given to operators\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_OTHER_USERS_APPS\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nNote that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.  \n\n\nDeveloper\n\n\nDevelopers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.\n\n\nHere is the list of default permissions given to developers\n\n\nLAUNCH_APPS\n\n\nUPLOAD_APP_PACKAGES\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nApp Permissions and App Package Permissions\n\n\nUsers can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.\n\n\nThe default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.\n\n\nAs of RTS 2.0.0, the Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the \nDT Gateway REST API document\n and \nhere\n for examples on how to use the REST API.\n\n\nViewing and Managing Auth in the Console\n\n\nViewing User Profile\n\n\nAfter you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.\n\n\nAdministering Auth\n\n\n\n\nFrom the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:\n\n\n\n\nCreate new users\n\n\nDelete users\n\n\nChange existing users\u2019 passwords\n\n\nAssign roles to users\n\n\nCreate roles\n\n\nAssign permissions to roles\n\n\n\n\n\n\nDataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.\n\n\n Authorization using external roles\n\n\nWhen using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections \nKerberos roles\n and\n\nJAAS roles\n. Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the \nExternal Role Mapping\n section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.\n\n\nKerberos roles \n\n\nWhen Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form \nuser/group@DOMAIN\n\nthe group portion is used as the external role and no additional\nconfiguration is necessary.\n\n\nJAAS roles \n\n\nTo use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the implementing JAAS authentication module.\nSome of these principals can be for roles and these role principals need\nto be identified from the list. Additional configuration is needed to do\nthis and this configuration is specific to the JAAS authentication\nmodule being used. Specifically the JAVA class name identifying the role\nprincipal needs to be specified. This can be specified using the\nproperty \n\u201cdt.gateway.http.authentication.jaas.role.class.name\u201d\n in the\nconfiguration file as shown below\n\n\nconfiguration\n\n...\n  \nproperty\n\n       \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n          \nvalue\nfull-class-name-of-role\n/value\n\n \n/property\n\n...\n\n/configuration\n\n\n\n\n\nCallback Handlers\n\n\nIn JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. The callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS supports\nspecification of a custom callback handler to handle these callbacks.\nThe custom callback handler can be specified using the property\n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d. When this\nproperty is not specified a default callback handler is used but it may\nnot be sufficient for all login modules like in the LDAP case described\nbelow. The property can be specified as follows\n\n\nconfiguration\n\n...\n\nproperty\n                                                                            \nname\ndt.gateway.http.authentication.jaas.callback.class.name\n/name\n\n\nvalue\nfull-class-name-of-callback\n/value\n\n\n/property\n\n...\n\n/configuration\n\n\n\n\n\nCustom callback handlers can be implemented by extending the default\ncallback handler so they can build upon it or they can be built from\nscratch. The LDAP callback handler described below also extends the\ndefault callback handler. The source for the default callback handler\ncan be found here \nDefaultCallbackHandler\n can be used as a reference when\nimplementing new callback handlers.\n\n\nLDAP\n\n\nWhen using LDAP with JAAS, to use LDAP roles, a  LDAP login module\nsupporting roles should be used. Any LDAP module that supports roles can\nbe used. Jetty implements one such login module. The steps to configure\nthis module are as follows.\n\n\n\n\n\n\nThe Gateway service in DataTorrent RTS 2.0 is compatible with Jetty 8. The class name identifying the \n    role principal is \n\u201corg.eclipse.jetty.plus.jaas.JAASRole\u201d\n. When using the Jetty LDAP\n    login module a custom JAAS callback has to be handled by the caller.\n    This has been implemented by DataTorrent in a callback handler. The\n    class name for the callback handler implementation should be\n    specified as a property. The property name is\n    \n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d\n. The class\n    name of the callback handler is\n    \n\u201ccom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\u201d\n.\n    This property should be specified along with the class name\n    identifying the role principal for the Jetty login module as\n    described in the section above. The configuration file with all the\n    JAAS properties including these looks as follows\n\n\nconfiguration\n\n...\n  \nproperty\n\n          \nname\ndt.gateway.http.authentication.type\n/name\n\n          \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n         \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n         \nvalue\nldap\n/value\n\n  \n/property\n\n  \nproperty\n  \n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n         \nvalue\norg.eclipse.jetty.plus.jaas.JAASRole\n/value\n\n  \n/property\n\n  \n/property\n\n\nname\n\n    dt.gateway.http.authentication.jaas.callback.class.name \n/name\n            \n\nvalue\n\n    com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\nNote that the JAAS callback handler property can be used to specify a\ncustom callback handler. The source for the Jetty custom callback\nhandler used above can be found here JettyJAASCallbackHandler\n\n\n\n\n\n\nAn issue was discovered with the Jetty login module supplied with\n    Jetty 8 that prevented LDAP authentication to be successful even\n    when the user credentials were correct. DataTorrent has a fix for\n    this and is providing the login module with the fix in a separate\n    package called dt-auth. The classname for the module is\n    \u201ccom.datatorrent.auth.jetty.JettyLdapLoginModule\u201d. The dt-auth\n    project along with the source can be found here \nAuth\n. DataTorrent\n    is working on submitting this fix back to Jetty project so that it\n    gets back into the main source.\n\n\nThe JAAS configuration file as described in\n\nLDAP\n section under \nEnabling JAAS Auth\n should be configured to specify the ldap settings\nfor roles. A sample configuration  roles based parameters to the\nconfiguration shown before\n\n\nldap {\n        com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" authenticationMethod=\"simple\"\n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\u201dcn\u201d\n    contextFactory=\u201dcom.sun.jndi.ldap.LdapCtxFactory\u201d;\n};\n\n\n\nFor more ldap settings refer to the java documentation of the login\n\n\n\n\n\n\nAfter the above configuration changes are made the gateway service\n    needs to be restarted. Before restarting however the different\n    classes specified in the configuration files above namely the\n    DataTorrent Jetty callback handler, the Jetty login module with the\n    DataTorrent fix and the Jetty dependencies containing the role class\n    should all be available for Gateway. This can be done by specifying\n    the jars containing these classes in a classpath variable that is\n    used by Gateway.\n\n\nThe jars can be obtained from the \nDataTorrent Auth\n project.\n\n\nPlease follow the instructions in the above url to obtain the project\njar files. After obtaining the jar files perform the following step to\nmake them available to Gateway\n\n\nEdit the custom-env.sh configuration file, typically located under\n\n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\nlocal install) and append the list of jars obtained above to the\n\nDT_CLASSPATH\n variable. This needs to be added at the end of the\nfile in the section for specifying local overrides to environment\nvariables. The line would look like\n\n\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\nwhen running Gateway in local mode use dtgateway restart command.\n\n\n\n\n\n\nExternal Role Mapping \n\n\nExternal role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps\n\n\n\n\n\n\nIn the configuration folder typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is\n\n\nadmins:admin\nstaff: developer\n\n\n\nThis maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\nwhen running Gateway in local mode use  dtgateway restart command\n\n\n\n\n\n\nAdministering Using Command Line \n\n\nYou can also utilize the \ndtGateway REST API\n (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.\n\n\nLog in as admin:\n\n\n% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login\n\n\n\nThis curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar\n\n\nChanging the admin password:\n\n\n% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin\n\n\n\nThis uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.\n\n\nAdding a second admin user:\n\n\n% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.\n\n\nAdding a user in the developer role:\n\n\n% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.\n\n\nListing all users:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users\n\n\n\nGetting info for a specific user:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command returns the information about the user \u201cjohn\u201d.\n\n\nRemoving a user:\n\n\n% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command removes the user \u201cjane\u201d.\n\n\nEnabling HTTPS in Gateway\n\n\nHTTPS in the Gateway can be enabled by performing following two steps.\n\n\n\n\n\n\nGenerate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here: \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n\n\n\n\n\n\nAdd a property to dt-site.xml configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n           \nname\ndt.gateway.sslKeystorePath\n/name\n\n           \nvalue\n{/path/to/keystore}\n/value\n\n  \n/property\n\n  \nproperty\n\n            \nname\ndt.gateway.sslKeystorePassword\n/name\n\n             \nvalue\n{keystore-password}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.attr.GATEWAY_USE_SSL\n/name\n\n          \nvalue\ntrue\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n( when running Gateway in local mode use \ndtgateway restart\n command)", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway_security/#datatorrent-gateway-security", 
            "text": "DataTorrent Gateway supports different authentication mechanisms to\nsecure access to the Console. The supported types are local password\nauthentication, kerberos authentication and J", 
            "title": "DataTorrent Gateway Security"
        }, 
        {
            "location": "/dtgateway_security/#configuring-authentication", 
            "text": "After DataTorrent RTS installation, you can turn on authentication and\nauthorization support to secure the DataTorrent Gateway. Gateway\nsupports three types of authentication.   Password  Kerberos  JAAS", 
            "title": "Configuring Authentication"
        }, 
        {
            "location": "/dtgateway_security/#enabling-password-auth", 
            "text": "\u201cPassword\u201d is the only authentication mechanism presented here that does\nnot depend on any external systems.  When enabled, all users will be\npresented with the login prompt before being able to use the DT Console.\nPassword authentication can be enabled by performing following two\nsteps.    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n     property \n     name dt.gateway.http.authentication.type /name \n     value password /value \n     /property \n... /configuration     Restart the Gateway by running  sudo service dtgateway restart  ( when running Gateway in local mode use  dtgateway restart command)    Open the Gateway URL in your browser, and you should be prompted for\nuser name and password.  Starting with DataTorrent RTS 2.0.0, the\ndefault username and password is  dtadmin  and  dtadmin .   Once authenticated, active user name and an option to log out is\npresented in the top right corner of the DT Console screen.", 
            "title": "Enabling Password Auth"
        }, 
        {
            "location": "/dtgateway_security/#enabling-kerberos-auth", 
            "text": "Kerberos authentication can optionally be enabled for Hadoop web access.\nIf this is configured then all web browser access to the Hadoop\nmanagement consoles is Kerberos authenticated. Access to all the web\nservices is also Kerberos authenticated. This Kerberos authentication is\nperformed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.  Kerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by\nlogging in to kerberos system in a terminal emulator using kinit. Then\nthe user launches a browser and accesses the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.  When this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the  External Role Mapping  in the  Authorization using external roles  section below for that.  Additional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install)  configuration \n...\n   property \n     name dt.gateway.http.authentication.type /name \n     value kerberos /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.principal /name \n     value {kerberos-principal-of-web-service} /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.keytab /name \n     value {absolute-path-to-keytab-file} /value \n   /property \n   property \n     name dt.gateway.http.authentication.token.validity /name \n     value {authentication-token-validity-in-seconds} /value \n   /property \n   property \n   name\\ dt.gateway.http.authentication.cookie.domain /name \n   value\\ {http-cookie-domain-for-authentication-token} /value \n   property \n     name\\ dt.gateway.http.authentication.cookie.path /name \n     value {http-cookie-path} /value \n   /property \n   property \n     name\\ dt.gateway.http.authentication.signature.secret /name \n     value {absolute-path-of-secret-file-for-signing-authentication-tokens}  /value \n   /property  /configuration   Note that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property dt.gateway.http.authentication.type  should be replaced with the\nappropriate values for your setup.    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)", 
            "title": "Enabling Kerberos Auth"
        }, 
        {
            "location": "/dtgateway_security/#enabling-jaas-auth", 
            "text": "JAAS is Java Authentication and Authorization Service. It is a pluggable\nand extensible mechanism for authentication. It is a generic framework\nand the actual authentication is performed by a JAAS plugin module which\ncan be configured using a configuration file. Among others LDAP and PAM\nauthentication can be performed via JAAS.  Similar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the External Role Mapping  in the  Authorization using external roles  section below for that.  This authentication can be enabled by performing the following general\nsteps. The specific steps for a couple of authentication mechanisms LDAP\nand PAM are shown in the next sections but other authentication\nmechanisms that have a JAAS plugin module can also be used.    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or\n     ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n       name dt.gateway.http.authentication.type /name \n       value jaas /value \n   /property \n   property\\ \n       name dt.gateway.http.authentication.jaas.name /name \n       value name-of-jaas-module /value \n   /property \n... /configuration   The  dt.gateway.http.authentication.jaas.name  property specifies the\nplugin module to use with JAAS and the value should be the name of the\nplugin module.    The name of the plugin module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be  /home/dtadmin/.java.login.config , if running as a\n    normal user it would be  ~/.java.login.config . The sample\n    configurations for LDAP and PAM are shown in the next sections.    The classes for the plugin module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.  The following step shows how to do this  a.  Edit the  custom-env.sh  configuration file, typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like  export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)    LDAP  LDAP authentication is a directory based authentication mechanism used\nin many enterprises. To enable LDAP authentication following are the\nspecifics for the configuration steps described above.    For step 1 above specify LDAP as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be \u201cldap\u201d\n    (without the quotes). This name should now be configured with the\n    appropriate settings as described in the next step.    For step 2, the JAAS name specified above should be configured with\n    the appropriate LDAP settings in the .java.login.config file. A\n    sample configuration is shown below  ldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};    Note that the first string before the open brace, in this case\n\u201cldap\u201d must match the jaas name specified in step 1. The first property\nwithin the braces  com.sun.security.auth.module.LdapLoginModule  specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The next settings are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.  PAM  PAM is Pluggable Authentication Module. It is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules\nthey can be used in Gateway as well.  PAM is implemented in C language and has C API. JPam is Java PAM bridge\nthat uses JNI to make PAM calls. It is available here http://jpam.sourceforge.net/  and has detailed documentation on how to install and set it up. JPam also has a JAAS plugin module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS plugin module is available.  To enable JPAM following are the specifics for the configuration steps\nto enable JAAS authentication described above.    JPAM has to be first installed on the system. Please following the\n    installation instructions from the JPAM website.    For step 1 above Specify JPAM as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be\n    \u201cnet-sf-jpam\u201d (without the quotes). This name should now be\n    configured with the appropriate settings as described in the next\n    step.    For step 2 the JAAS name specified above should be configured with\n    the appropriate JPAM settings in the .java.login.config file. A\n    sample configuration is shown below  net-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};    Note that the first string before the open brace, in this case\n\u201cnet-sf-jpam\u201d must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM specific settings. The\nserviceName setting for example specifies the PAM service which would\nneed to be further configured in /etc/pam.d/net-sf-jpam to specify the\nPAM modules to use. Refer to PAM documentation on how to configure a PAM\nservice with PAM modules. If using Linux local accounts system-auth\ncould be specified as the PAM module in this file. The above settings\nare only provided as a reference example and a different serviceName for\nexample can be chosen.   For step 3 add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format  JPam- version .jar  where\n    version  denotes the version, version 1.1 has been tested. export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/JPam-\\ version\\ .jar    Groups  For group support such as using LDAP groups for authorization refer to\nthe  Authorization using external roles  section below.", 
            "title": "Enabling JAAS Auth"
        }, 
        {
            "location": "/dtgateway_security/#configuring-authorization", 
            "text": "When any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.", 
            "title": "Configuring Authorization"
        }, 
        {
            "location": "/dtgateway_security/#permissions", 
            "text": "The list of all possible permissions in the DT Gateway is as follow:   ACCESS_RM_PROXY  Allow HTTP proxying requests to YARN\u2019s Resource Manager REST API   EDIT_GLOBAL_CONFIG  Edit global settings   EDIT_OTHER_USERS_CONFIG  Edit other users\u2019 settings   LAUNCH_APPS  Launch Apps  MANAGE_LICENSES  Manage DataTorrent RTS licenses   MANAGE_OTHER_USERS_APPS  Manage (e.g. edit, kill, etc) applications launched by other users   MANAGE_OTHER_USERS_APP_PACKAGES  Manage App Packages uploaded by other users     MANAGE_ROLES  Manage roles (create/delete roles, or assign permissions to roles)  MANAGE_SYSTEM_ALERTS  Manage system alerts   MANAGE_USERS  Manage users (create/delete users, change password)   UPLOAD_APP_PACKAGES  Upload App Packages and use the app builder   VIEW_GLOBAL_CONFIG  View global settings      VIEW_LICENSES  View DataTorrent RTS licenses   VIEW_OTHER_USERS_APPS  View applications launched by others   VIEW_OTHER_USERS_APP_PACKAGES  View App Packages uploaded by other users   VIEW_OTHER_USERS_CONFIG  Edit other users\u2019 settings   VIEW_SYSTEM_ALERTS  View system alerts", 
            "title": "Permissions"
        }, 
        {
            "location": "/dtgateway_security/#default-roles", 
            "text": "DataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.  Admin  An administrator of DataTorrent RTS is intended to be able to install,\nmanage   modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.  Operator  Operators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.  Here is the list of default permissions given to operators  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_OTHER_USERS_APPS  VIEW_OTHER_USERS_APP_PACKAGES  VIEW_SYSTEM_ALERTS  Note that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.    Developer  Developers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.  Here is the list of default permissions given to developers  LAUNCH_APPS  UPLOAD_APP_PACKAGES  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_SYSTEM_ALERTS", 
            "title": "Default Roles"
        }, 
        {
            "location": "/dtgateway_security/#app-permissions-and-app-package-permissions", 
            "text": "Users can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.  The default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.  As of RTS 2.0.0, the Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the  DT Gateway REST API document  and  here  for examples on how to use the REST API.", 
            "title": "App Permissions and App Package Permissions"
        }, 
        {
            "location": "/dtgateway_security/#viewing-and-managing-auth-in-the-console", 
            "text": "", 
            "title": "Viewing and Managing Auth in the Console"
        }, 
        {
            "location": "/dtgateway_security/#viewing-user-profile", 
            "text": "After you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has. \nAdministering Auth   From the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:   Create new users  Delete users  Change existing users\u2019 passwords  Assign roles to users  Create roles  Assign permissions to roles    DataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.", 
            "title": "Viewing User Profile"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-roles", 
            "text": "When Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form  user/group@DOMAIN \nthe group portion is used as the external role and no additional\nconfiguration is necessary.", 
            "title": "Kerberos roles "
        }, 
        {
            "location": "/dtgateway_security/#jaas-roles", 
            "text": "To use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the implementing JAAS authentication module.\nSome of these principals can be for roles and these role principals need\nto be identified from the list. Additional configuration is needed to do\nthis and this configuration is specific to the JAAS authentication\nmodule being used. Specifically the JAVA class name identifying the role\nprincipal needs to be specified. This can be specified using the\nproperty  \u201cdt.gateway.http.authentication.jaas.role.class.name\u201d  in the\nconfiguration file as shown below  configuration \n...\n   property \n        name dt.gateway.http.authentication.jaas.role.class.name /name \n           value full-class-name-of-role /value \n  /property \n... /configuration   Callback Handlers  In JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. The callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS supports\nspecification of a custom callback handler to handle these callbacks.\nThe custom callback handler can be specified using the property\n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d. When this\nproperty is not specified a default callback handler is used but it may\nnot be sufficient for all login modules like in the LDAP case described\nbelow. The property can be specified as follows  configuration \n... property                                                                              name dt.gateway.http.authentication.jaas.callback.class.name /name  value full-class-name-of-callback /value  /property \n... /configuration   Custom callback handlers can be implemented by extending the default\ncallback handler so they can build upon it or they can be built from\nscratch. The LDAP callback handler described below also extends the\ndefault callback handler. The source for the default callback handler\ncan be found here  DefaultCallbackHandler  can be used as a reference when\nimplementing new callback handlers.  LDAP  When using LDAP with JAAS, to use LDAP roles, a  LDAP login module\nsupporting roles should be used. Any LDAP module that supports roles can\nbe used. Jetty implements one such login module. The steps to configure\nthis module are as follows.    The Gateway service in DataTorrent RTS 2.0 is compatible with Jetty 8. The class name identifying the \n    role principal is  \u201corg.eclipse.jetty.plus.jaas.JAASRole\u201d . When using the Jetty LDAP\n    login module a custom JAAS callback has to be handled by the caller.\n    This has been implemented by DataTorrent in a callback handler. The\n    class name for the callback handler implementation should be\n    specified as a property. The property name is\n     \u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d . The class\n    name of the callback handler is\n     \u201ccom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\u201d .\n    This property should be specified along with the class name\n    identifying the role principal for the Jetty login module as\n    described in the section above. The configuration file with all the\n    JAAS properties including these looks as follows  configuration \n...\n   property \n           name dt.gateway.http.authentication.type /name \n           value jaas /value \n   /property \n   property \n          name dt.gateway.http.authentication.jaas.name /name \n          value ldap /value \n   /property \n   property   \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n          value org.eclipse.jetty.plus.jaas.JAASRole /value \n   /property \n   /property  name \n    dt.gateway.http.authentication.jaas.callback.class.name  /name              value \n    com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler /value \n   /property \n... /configuration   Note that the JAAS callback handler property can be used to specify a\ncustom callback handler. The source for the Jetty custom callback\nhandler used above can be found here JettyJAASCallbackHandler    An issue was discovered with the Jetty login module supplied with\n    Jetty 8 that prevented LDAP authentication to be successful even\n    when the user credentials were correct. DataTorrent has a fix for\n    this and is providing the login module with the fix in a separate\n    package called dt-auth. The classname for the module is\n    \u201ccom.datatorrent.auth.jetty.JettyLdapLoginModule\u201d. The dt-auth\n    project along with the source can be found here  Auth . DataTorrent\n    is working on submitting this fix back to Jetty project so that it\n    gets back into the main source.  The JAAS configuration file as described in LDAP  section under  Enabling JAAS Auth  should be configured to specify the ldap settings\nfor roles. A sample configuration  roles based parameters to the\nconfiguration shown before  ldap {\n        com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" authenticationMethod=\"simple\"\n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\u201dcn\u201d\n    contextFactory=\u201dcom.sun.jndi.ldap.LdapCtxFactory\u201d;\n};  For more ldap settings refer to the java documentation of the login    After the above configuration changes are made the gateway service\n    needs to be restarted. Before restarting however the different\n    classes specified in the configuration files above namely the\n    DataTorrent Jetty callback handler, the Jetty login module with the\n    DataTorrent fix and the Jetty dependencies containing the role class\n    should all be available for Gateway. This can be done by specifying\n    the jars containing these classes in a classpath variable that is\n    used by Gateway.  The jars can be obtained from the  DataTorrent Auth  project.  Please follow the instructions in the above url to obtain the project\njar files. After obtaining the jar files perform the following step to\nmake them available to Gateway  Edit the custom-env.sh configuration file, typically located under /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\nlocal install) and append the list of jars obtained above to the DT_CLASSPATH  variable. This needs to be added at the end of the\nfile in the section for specifying local overrides to environment\nvariables. The line would look like  export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart  when running Gateway in local mode use dtgateway restart command.", 
            "title": "JAAS roles "
        }, 
        {
            "location": "/dtgateway_security/#external-role-mapping", 
            "text": "External role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps    In the configuration folder typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is  admins:admin\nstaff: developer  This maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.    Restart the Gateway by running  sudo service dtgateway restart  when running Gateway in local mode use  dtgateway restart command", 
            "title": "External Role Mapping "
        }, 
        {
            "location": "/dtgateway_security/#administering-using-command-line", 
            "text": "You can also utilize the  dtGateway REST API  (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.", 
            "title": "Administering Using Command Line "
        }, 
        {
            "location": "/dtgateway_security/#log-in-as-admin", 
            "text": "% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login  This curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar", 
            "title": "Log in as admin:"
        }, 
        {
            "location": "/dtgateway_security/#changing-the-admin-password", 
            "text": "% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin  This uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.", 
            "title": "Changing the admin password:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-second-admin-user", 
            "text": "% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john  This command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.", 
            "title": "Adding a second admin user:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-user-in-the-developer-role", 
            "text": "% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane  This command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.", 
            "title": "Adding a user in the developer role:"
        }, 
        {
            "location": "/dtgateway_security/#listing-all-users", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users", 
            "title": "Listing all users:"
        }, 
        {
            "location": "/dtgateway_security/#getting-info-for-a-specific-user", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john  This command returns the information about the user \u201cjohn\u201d.", 
            "title": "Getting info for a specific user:"
        }, 
        {
            "location": "/dtgateway_security/#removing-a-user", 
            "text": "% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane  This command removes the user \u201cjane\u201d.", 
            "title": "Removing a user:"
        }, 
        {
            "location": "/dtgateway_security/#enabling-https-in-gateway", 
            "text": "HTTPS in the Gateway can be enabled by performing following two steps.    Generate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here:  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html    Add a property to dt-site.xml configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n            name dt.gateway.sslKeystorePath /name \n            value {/path/to/keystore} /value \n   /property \n   property \n             name dt.gateway.sslKeystorePassword /name \n              value {keystore-password} /value \n   /property \n   property \n     name dt.attr.GATEWAY_USE_SSL /name \n           value true /value \n   /property \n... /configuration     Restart the Gateway by running  sudo service dtgateway restart  ( when running Gateway in local mode use  dtgateway restart  command)", 
            "title": "Enabling HTTPS in Gateway"
        }, 
        {
            "location": "/dtcli/", 
            "text": "Coming Soon", 
            "title": "dtCli"
        }, 
        {
            "location": "/dtcli/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting DataTorrent RTS\n\n\nDownload\n\n\nWhere can I get DataTorrent RTS software?\n\n\nDataTorrent products are available for download from \nhttps://www.datatorrent.com/download/\n\n\n\n\nCommunity Edition\n:  It is a packaged version of Apache Apex and enables developers to quickly develop their big data streaming and batch projects.\n\n\nEnterprise Edition\n:  Designed for enterprise production deployment and includes security, advanced monitoring and troubleshooting, graphical application assembly, and application data visualization.\n\n\nSandbox Edition\n:  Enterprise Edition and demo applications pre-installed and configured with a single-node Hadoop cluster running in a virtual machine.  Optimized for evaluation and training purposes.\n\n\ndtIngest Application\n: simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.\n\n\n\n\nWhat is the difference between DataTorrent RTS editions?\n\n\nPlease refer to \nDataTorrent RTS editions overview\n\n\nWhere can I find the Standard edition installer?\n\n\nYou can use the download link for Enterprise edition as the package is\nsame for both editions. But, you have to apply the license to enable the\nStandard edition. You can upgrade the license by using dtManage.\nLicenses are available in 2 types : evaluation and production.\n\n\nWhat are DataTorrent RTS package contents of Community vs Enterprise edition?\n\n\nPackage contents for Community edition:\n\n\n\n\nApache Apex (incubating)\n\n\nDataTorrent Demo Applications\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtIngest\n\n\n\n\nPackage contents for Enterprise edition:\n\n\n\n\nApache Apex (incubating)\n\n\nDataTorrent Demo Applications\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtIngest\n\n\nDataTorrent dtAssemble\n\n\nDataTorrent dtDashboard\n\n\n\n\nHow do I confirm the package downloaded correctly?\n\n\nYou can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:\n\n\n# md5sum \nDT_RTS_Package\n\n\n\n\nCompare the result with MD5 sum posted on the product download page.\n\n\nHow do I download the DataTorrent RTS package using CLI?\n\n\nUse following curl command to download DataTorrent RTS package:\n\n\ncurl -LSO\u00a0\nDT_RTS_download_link\n\n\n\n\nWe recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.\n\n\nWhat are the prerequisites of DataTorrent RTS ?\n\n\nDataTorrent RTS platform has following Hadoop cluster requirements:\n\n\n\n\nOperating system supported by Hadoop distribution\n\n\nHadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable\n\n\nJava 7 or 8 as supported by Hadoop distribution\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\nGoogle Chrome, Firefox, or Safari to access dtManage (DataTorrent UI\n    console)\n\n\n\n\nWhere can I start from after downloading DataTorrent RTS?\n\n\n\n\nAfter successful download of DataTorrent RTS, make sure all prerequisites are satisfied.\n\n\nYou will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to \ninstallation guide\n\n\nOnce installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.\n\n\n\n\nWhat are the supported Hadoop distribution by DataTorrent RTS?\n\n\nDataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nHadoop Distribution\n\n\nSupported Version\n\n\n\n\n\n\nAmazon EMR\n\n\nHadoop 2.4 and higher\n\n\n\n\n\n\nApache Hadoop\n\n\nHadoop 2.2 and higher\n\n\n\n\n\n\nCloudera\n\n\nCDH 5.0 and higher\n\n\n\n\n\n\nHortonworks\n\n\nHDP 2.0 and higher\n\n\n\n\n\n\nMapR\n\n\n4.0 and higher\n\n\n\n\n\n\nMicrosoft\n\n\nHDInsight\n\n\n\n\n\n\nPivotal\n\n\n2.1 and higher\n\n\n\n\n\n\n\n\n\nWhat is the Datatorrent Sandbox?\n\n\nThe Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.\n\n\nWhere do I get DataTorrent Sandbox download link?\n\n\nSandbox can be downloaded by visiting \ndatatorrent.com/download\n\n\nWhat are the system requirements for sandbox deployment?\n\n\nThe DataTorrent RTS Sandbox is a complete, stand-alone, instance of the\nEnterprise Edition as a single-node Hadoop cluster on your local\nmachine. Following are prerequisites for DataTorrent RTS:\n\n\n\n\nVirtualBox\n 4.3 or greater installed.\n\n\n6GB RAM or greater available for Sandbox VM.\n\n\n\n\nWhat are the DataTorrent RTS package content details in sandbox?\n\n\n\n\nUbuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)\n\n\nLubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)\n\n\nApache Apex (incubating), Apache Apex-Malhar (incubating)\n\n\nDataTorrent Operator Library\n\n\nDataTorrent Enterprise Security\n\n\nDataTorrent dtManage\n\n\nDataTorrent dtIngest\n\n\nDataTorrent dtAssemble\n\n\nDataTorrent dtDashboard\n\n\nDemo Applications\n\n\n\n\nWhat is dtIngest applicaiton?\n\n\nDataTorrent dtIngest simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.\n\n\nWhere do I get dtingest application?\n\n\nApplication can be downloaded by visiting \ndatatorrent.com/download\n\n\nWhat are the dtingest package contents?\n\n\nPackage comprises of DataTorrent RTS and dtIngest application.\n\n\nWhat are the prerequisites of dtIngest?\n\n\nDataTorrent RTS 3.x and above. Please refer \ndtIngest tutorial\n\u00a0for more details.\n\n\nWhere can I start from after downloading dtingest?\n\n\n\n\nMake sure all DataTorrent RTS prerequisites are satisfied before dtIngest installation.\n\n\nRun the downloaded installer installer. Refer to DataTorrent RTS \ninstallation guide\n.\n\n\nAfter DataTorrent RTS installation and configuration, you can configure and launch the\n    dtIngest application from dtManage, the DataTorrent console. Refer to \ndtIngest tutorial\n\u00a0for more details.\n\n\n\n\nHow do I get specific DT version ?\n\n\nYou can find archive list of various DataTorrent RTS versions at the bottom of each product download page.\n\n\nWhere can I request new / upgrade current license?\n\n\nPlease follow the instructions at \nLicense Upgrade\n\n\nWhere do I find product documentation?\n\n\nPlease refer to: \nDataTorrent Documentation\n\n\nWhere can I learn more about Apache Apex?\n\n\nYou can refer Apex page for more details: \nApache Apex\n\n\nDo you need help?\n\n\nYou can contact us at \nhttps://www.datatorrent.com/contact\n\n\nInstallation\n\n\nThere are multiple installations available e.g. Sandbox Edition, Community Edition, Enterprise Edition, dtIngest. Supported operating systems are which support Hadoop platform (tested on CentOS 6.x and Ubuntu 12.04).\n\n\nMinimum hardware requirements, what happens if certain minimum configuration requirement has not been met?\n\n\nMinimum of 8G RAM is required on the Hadoop cluster.\n\n\nWhat happens if java is not installed?\n\n\nFollowing message can be seen when Java is not abilable on the system\n\n\nError: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.\n\n\n\nInstall Java 7 from package manager of Linux Distribution and try running installer again.\n\n\nWhat happens if Hadoop is not installed?\n\n\nInstallation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect hadoop binary (/usr/bin/hadoop) \n DFS location.\n\n\n\n\nInstall Hadoop > 2.2.0 and update the configuration parameters above.\n\n\nHow do I check if Hadoop is installed and running correctly?\n\n\nFollowing commands can be used to confirm installed Hadoop version and if Hadoop services are running.\n\n\n$ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager\n\n\n\nWhat happens if the downloaded file is corrupted?\n\n\nMD5 checksum will result in the following error:\n\n\n\u201cVerifying archive integrity...Error in MD5 checksums: \nMD5 checksum\n is different from \nMD5 checksum\n\u201d.\n\n\n\nDownloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use \ncurl\n instead of \nwget\n.\n\n\nWhy do I see the following permissions errors?\n\n\nDuring installation following error message will be seen on screen\n\n\n\n\nThese typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:\n\n\n$ hadoop dfs -ls /user/\nUSER\n/datatorrent\n$ hadoop dfs -mkdir /user/\nUSER\n/datatorrent  \n$ hadoop dfs -chown \nUSER\n /user/\nUSER\n/datatorrent\n\n\n\nUpgrade\n\n\nLicense agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0.\n\n\nIf your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See \ninstallation guide\n for details.\n\n\nConfiguration\n\n\nComing soon.\n\n\nDevelopment\n\n\nHadoop dependencies conflicts\n\n\nComing soon.  (Use provided scope, don\u2019t bundle any Hadoop jars.)\n\n\nGetting this message in STRAM logs. Is anything wrong in my code?\n\n\n2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)\n\n\n\nComing soon.\n\n\nDebugging\n\n\nHow to remote debug gateway service?\n\n\nUpdate hadoop OPTS variable by running,\n\n\nexport HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS\n\n\n\nHow to setup DEBUG level while running an application?\n\n\nAdd the property :\n\n\nproperty\n\n  \nname\ndt.application.\nAPP-NAME\n.attr.DEBUG\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\n\n\nMy gateway is throwing the following exception.\n\n\n  ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...\n\n\n\nCheck if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.\n\n\nLog analysis\n\n\nHow to set custom log4j properties for an app package\n\n\nThere are two ways of setting custom log4j.properties in Apex\n\n\n\n\n\n\nSetting custom log4j.properties at the Application level.  This will ensure that custom log4j properties is used for all containers including Application Master.  Following attribute should be set at application level:\n\n\nproperty\n\n  \nname\ndt.attr.CONTAINER_JVM_OPTIONS\n/name\n\n  \nvalue\n-Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nSetting custom log4j.properties at an individual operator level.  This sets the custom log4j properties only on the container that is hosting the operator.  Following attribute should be set at an operator level:\n\n\nproperty\n\n  \nname\ndt.operator.\nOPERATOR_NAME\n.attr.JVM_OPTIONS\n/name\n\n  \nvalue\n -Dlog4j.configuration=custom_log4j.properties\n/value\n\n\n/property\n\n\n\n\n\n\n\n\nMake sure that custom_log4j.properties is part of your application jar and is located under \nsrc/main/resources\n.  Following are examples of custom log4j properties files\n\n\n\n\n\n\nWriting to a file\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n\n\n\n\n\n\nWriting to Console\n\n\nlog4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\n\n\n\n\n\n\n\nHow to check STRAM logs\n\n\nYou can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.  \n\n\nIn dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.  \n\n\nAlternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.\n\n\nyarn logs -applicationId \napplicationId\n\n\n\n\nHow to check application logs\n\n\nOn dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.\n\n\n\n\nHow to check killed operator\u2019s state\n\n\nOn dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.\n\n\n\n\nHow to search for particular any application or container?\n\n\nIn applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.\n\n\nHow do I search within logs?\n\n\nOnce you navigate to the logs page,  \n\n\n\n\nDownload log file to search using your preferred editor  \n\n\nuse \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d\n\n\n\n\nLaunching Applications\n\n\nApplication goes from accepted state to Finished(FAILED) state\n\n\nCheck if your application name conflicts with any of the already running\napplications in your cluster. Apex do not allow two application with\nsame names run simultaneously.\n\nYour STRAM logs will have following error:\n\n\u201cForced shutdown due to Application master failed due to application\n\\\nappId> with duplicate application name \\\nappName> by the same user\n\\\nuser name> is already started.\u201d  \n\n\nConstraintViolationException while application launch\n\n\nCheck if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException. \u00a0  \n\n\nEvents\n\n\nHow to check container failures\n\n\nIn StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.\n\n\nHow to search within events\n\n\nYou can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.\n\n\ntail vs range mode\n\n\ntail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.\n\n\nWhat is \u201cfollowing\u201d button in events pane\n\n\nWhen we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.\n\n\nComing Soon\n\n\n\n\nConnection Refused Exception\n\n\nClassNotFound Exception\n\n\nLaunching apa vs jar\n\n\nDAG validation failed\n\n\nMultiple gateways running simultaneously, app not launched.\n\n\nHDFS in safe mode\n\n\nApplication stays in accepted state\n\n\nSome containers do not get resources (specially in case of repartition)\n\n\nInsufficient memory set to operator causes operator kill continuously.\n\n\n\n\nWhy is the number of events same/different at input and output port of each operator?\n\n\n\n\n\n\nShutdown vs kill option\n\n\n\n\nWhy shutdown doesn\u2019t work? (if some containers are not running)\n\n\nCan I kill multiple applications at same time?\n\n\nKilling containers vs killing application\n\n\nSTRAM failures (during define partitions)\n\n\nThread local + partition parallel configuration\n\n\nWhat to do when downstream operators are slow than the input  operators.\n\n\nI am seeing high latency, what to do?\n\n\nappConf in ADT (inside apa file) vs conf option in dtcli\n\n\nApplication keeps restarting (has happened once due to license agent during upgrade)\n\n\nOperator getting killed after every 60 secs (Timeout issue)\n\n\nHow to change commit frequency\n\n\nDifference between exactly once, at least once and at most once\n\n\nThread local vs container local vs node local\n\n\nSetting operator memory\n\n\nTurning Bufferserver memory\n\n\nTurning STRAM memory\n\n\nCluster nodes not able to access edge node where Gateway is running\n\n\n\n\nDevelopers not sure when to process incoming tuples in end window or when to do it in process function of operator\n\n\n\n\n\n\nHow partitioning works\n\n\n\n\nHow the data is partitioned between different partitions.\n\n\nHow to use stream-codec\n\n\nData on which ports is partitioned? By default default partitioner partitions data on first port.\n\n\nHow to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).\n\n\n\n\n\n\n\n\npom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc\n\n\n\n\n\n\nAll non-transient members of the operator object need to be\n    serializable. All members that are not serializable cannot be saved\n    during checkpoint and must be declared transient (e.g. connection\n    objects). This is such a common problem that we need to dedicate a\n    section to it.\n\n\n\n\n\n\nExactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.\n\n\n\n\n\n\nHow to check checkpoint size. (large checkpoint size cause instability in the DAG).\n\n\n\n\nHow to add custom metrics and metric aggregator.\n\n\nExample of how to implement dynamic partitioning.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/troubleshooting/#troubleshooting-datatorrent-rts", 
            "text": "", 
            "title": "Troubleshooting DataTorrent RTS"
        }, 
        {
            "location": "/troubleshooting/#download", 
            "text": "Where can I get DataTorrent RTS software?  DataTorrent products are available for download from  https://www.datatorrent.com/download/   Community Edition :  It is a packaged version of Apache Apex and enables developers to quickly develop their big data streaming and batch projects.  Enterprise Edition :  Designed for enterprise production deployment and includes security, advanced monitoring and troubleshooting, graphical application assembly, and application data visualization.  Sandbox Edition :  Enterprise Edition and demo applications pre-installed and configured with a single-node Hadoop cluster running in a virtual machine.  Optimized for evaluation and training purposes.  dtIngest Application : simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.   What is the difference between DataTorrent RTS editions?  Please refer to  DataTorrent RTS editions overview  Where can I find the Standard edition installer?  You can use the download link for Enterprise edition as the package is\nsame for both editions. But, you have to apply the license to enable the\nStandard edition. You can upgrade the license by using dtManage.\nLicenses are available in 2 types : evaluation and production.  What are DataTorrent RTS package contents of Community vs Enterprise edition?  Package contents for Community edition:   Apache Apex (incubating)  DataTorrent Demo Applications  DataTorrent dtManage  DataTorrent dtIngest   Package contents for Enterprise edition:   Apache Apex (incubating)  DataTorrent Demo Applications  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtIngest  DataTorrent dtAssemble  DataTorrent dtDashboard   How do I confirm the package downloaded correctly?  You can verify the downloaded DataTorrent RTS package by comparing with\nMD5 sum. The command to get md5 sum of downloaded package:  # md5sum  DT_RTS_Package   Compare the result with MD5 sum posted on the product download page.  How do I download the DataTorrent RTS package using CLI?  Use following curl command to download DataTorrent RTS package:  curl -LSO\u00a0 DT_RTS_download_link   We recommend using \u2018curl\u2019 instead of \u2018wget\u2019, which lacks chunked transfer encoding support, potentially resulting in corrupted downloads.  What are the prerequisites of DataTorrent RTS ?  DataTorrent RTS platform has following Hadoop cluster requirements:   Operating system supported by Hadoop distribution  Hadoop (2.2 or above) cluster with HDFS, YARN configured. Make sure\n    hadoop executable available in PATH variable  Java 7 or 8 as supported by Hadoop distribution  Minimum of 8G RAM available on the Hadoop cluster  Permissions to create HDFS directory for DataTorrent user  Google Chrome, Firefox, or Safari to access dtManage (DataTorrent UI\n    console)   Where can I start from after downloading DataTorrent RTS?   After successful download of DataTorrent RTS, make sure all prerequisites are satisfied.  You will need to install DataTorrent RTS on Hadoop cluster using the downloaded installer. Refer to  installation guide  Once installed, you will be prompted to proceed to dtManage, the DataTorrent management console, where you will be able to launch and manage applications.   What are the supported Hadoop distribution by DataTorrent RTS?  DataTorrent RTS is a Hadoop 2.x native application and is fully\nintegrated with YARN and HDFS providing tight integration with any\nApache Hadoop 2.x based distribution.       Hadoop Distribution  Supported Version    Amazon EMR  Hadoop 2.4 and higher    Apache Hadoop  Hadoop 2.2 and higher    Cloudera  CDH 5.0 and higher    Hortonworks  HDP 2.0 and higher    MapR  4.0 and higher    Microsoft  HDInsight    Pivotal  2.1 and higher     What is the Datatorrent Sandbox?  The Sandbox provides a quick and simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster. The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.  Where do I get DataTorrent Sandbox download link?  Sandbox can be downloaded by visiting  datatorrent.com/download  What are the system requirements for sandbox deployment?  The DataTorrent RTS Sandbox is a complete, stand-alone, instance of the\nEnterprise Edition as a single-node Hadoop cluster on your local\nmachine. Following are prerequisites for DataTorrent RTS:   VirtualBox  4.3 or greater installed.  6GB RAM or greater available for Sandbox VM.   What are the DataTorrent RTS package content details in sandbox?   Ubuntu 12.04 LTS + Apache Hadoop 2.2 (DataTorrent RTS 3.1 or earlier)  Lubuntu 14.04 LTS + Apache Hadoop 2.7 (DataTorrent RTS 3.2 or later)  Apache Apex (incubating), Apache Apex-Malhar (incubating)  DataTorrent Operator Library  DataTorrent Enterprise Security  DataTorrent dtManage  DataTorrent dtIngest  DataTorrent dtAssemble  DataTorrent dtDashboard  Demo Applications   What is dtIngest applicaiton?  DataTorrent dtIngest simplifies the collection, aggregation and movement of large amounts of data to and from Hadoop and is available for production use at no cost.  Where do I get dtingest application?  Application can be downloaded by visiting  datatorrent.com/download  What are the dtingest package contents?  Package comprises of DataTorrent RTS and dtIngest application.  What are the prerequisites of dtIngest?  DataTorrent RTS 3.x and above. Please refer  dtIngest tutorial \u00a0for more details.  Where can I start from after downloading dtingest?   Make sure all DataTorrent RTS prerequisites are satisfied before dtIngest installation.  Run the downloaded installer installer. Refer to DataTorrent RTS  installation guide .  After DataTorrent RTS installation and configuration, you can configure and launch the\n    dtIngest application from dtManage, the DataTorrent console. Refer to  dtIngest tutorial \u00a0for more details.   How do I get specific DT version ?  You can find archive list of various DataTorrent RTS versions at the bottom of each product download page.  Where can I request new / upgrade current license?  Please follow the instructions at  License Upgrade  Where do I find product documentation?  Please refer to:  DataTorrent Documentation  Where can I learn more about Apache Apex?  You can refer Apex page for more details:  Apache Apex  Do you need help?  You can contact us at  https://www.datatorrent.com/contact", 
            "title": "Download"
        }, 
        {
            "location": "/troubleshooting/#installation", 
            "text": "There are multiple installations available e.g. Sandbox Edition, Community Edition, Enterprise Edition, dtIngest. Supported operating systems are which support Hadoop platform (tested on CentOS 6.x and Ubuntu 12.04).  Minimum hardware requirements, what happens if certain minimum configuration requirement has not been met?  Minimum of 8G RAM is required on the Hadoop cluster.  What happens if java is not installed?  Following message can be seen when Java is not abilable on the system  Error: java executable not found. Please ensure java\nor hadoop are installed and available in PATH environment variable\nbefore proceeding with this installation.  Install Java 7 from package manager of Linux Distribution and try running installer again.  What happens if Hadoop is not installed?  Installation will be successful, however Hadoop Configuration page in dtManage (e.g. http://localhost:9090) will expect hadoop binary (/usr/bin/hadoop)   DFS location.   Install Hadoop > 2.2.0 and update the configuration parameters above.  How do I check if Hadoop is installed and running correctly?  Following commands can be used to confirm installed Hadoop version and if Hadoop services are running.  $ hadoop version\n\nHadoop 2.4.0\nSubversion [http://svn.apache.org/repos/asf/hadoop/common](http://svn.apache.org/repos/asf/hadoop/common)\u00a0-r\n1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using\n/usr/local/hadoop/share/hadoop/common/hadoop-common-2.4.0.jar\n\n$ jps\n\n10211 NameNode\n10772 ResourceManager\n10427 DataNode\n14691 Jps\n10995 NodeManager  What happens if the downloaded file is corrupted?  MD5 checksum will result in the following error:  \u201cVerifying archive integrity...Error in MD5 checksums:  MD5 checksum  is different from  MD5 checksum \u201d.  Downloaded installer could be corrupted.  Try downloading the installer again.  If using command line, use  curl  instead of  wget .  Why do I see the following permissions errors?  During installation following error message will be seen on screen   These typically happen when specified directory does not exist, and the installation user does not have permissions to create it.  Following commands can be executed by user with sufficient privileges to resolve this issue:  $ hadoop dfs -ls /user/ USER /datatorrent\n$ hadoop dfs -mkdir /user/ USER /datatorrent  \n$ hadoop dfs -chown  USER  /user/ USER /datatorrent", 
            "title": "Installation"
        }, 
        {
            "location": "/troubleshooting/#upgrade", 
            "text": "License agent errors cause problems during upgrade from DataTorrent RTS 2.0 to 3.0.  If your applications are being launched continuously, or you are unable to launch apps due to licensing errors, try running complete uninstall and re-installation of DataTorrent RTS.  See  installation guide  for details.", 
            "title": "Upgrade"
        }, 
        {
            "location": "/troubleshooting/#configuration", 
            "text": "Coming soon.", 
            "title": "Configuration"
        }, 
        {
            "location": "/troubleshooting/#development", 
            "text": "Hadoop dependencies conflicts  Coming soon.  (Use provided scope, don\u2019t bundle any Hadoop jars.)  Getting this message in STRAM logs. Is anything wrong in my code?  2015-10-09 04:31:06,749 INFO com.datatorrent.stram.StreamingContainerManager: Heartbeat for unknown operator 3 (container container_1443694550865_0150_01_000007)  Coming soon.", 
            "title": "Development"
        }, 
        {
            "location": "/troubleshooting/#debugging", 
            "text": "How to remote debug gateway service?  Update hadoop OPTS variable by running,  export HADOOP_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5432 $HADOOP_OPTS  How to setup DEBUG level while running an application?  Add the property :  property \n   name dt.application. APP-NAME .attr.DEBUG /name \n   value true /value  /property   My gateway is throwing the following exception.    ERROR YARN Resource Manager has problem: java.net.ConnectException: Call From myexample.com/192.168.3.21 to 0.0.0.0:8032\u00a0failed on connection\n  exception: java.net.ConnectException: Connection refused; For more  details see:[http://wiki.apache.org/hadoop/ConnectionRefused](http://wiki.apache.org/hadoop/ConnectionRefused)\u00a0at\n  sun.reflect.GeneratedConstructorAccessor27.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  ...  Check if the host where gateway is running has yarn-site.xml file. You need to have all Hadoop configuration files accessible to dtGateway for it to run successfully.", 
            "title": "Debugging"
        }, 
        {
            "location": "/troubleshooting/#log-analysis", 
            "text": "How to set custom log4j properties for an app package  There are two ways of setting custom log4j.properties in Apex    Setting custom log4j.properties at the Application level.  This will ensure that custom log4j properties is used for all containers including Application Master.  Following attribute should be set at application level:  property \n   name dt.attr.CONTAINER_JVM_OPTIONS /name \n   value -Dlog4j.configuration=custom_log4j.properties /value  /property     Setting custom log4j.properties at an individual operator level.  This sets the custom log4j properties only on the container that is hosting the operator.  Following attribute should be set at an operator level:  property \n   name dt.operator. OPERATOR_NAME .attr.JVM_OPTIONS /name \n   value  -Dlog4j.configuration=custom_log4j.properties /value  /property     Make sure that custom_log4j.properties is part of your application jar and is located under  src/main/resources .  Following are examples of custom log4j properties files    Writing to a file  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}    Writing to Console  log4j.rootLogger=${hadoop.root.logger} // this is set to INFO / DEBUG, RFA\nlog4j.appender.RFA=org.apache.log4j.ConsoleAppender\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} [%t] %-5p %c{2} %M - %m%n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout    How to check STRAM logs  You can get STRAM logs by retrieving YARN logs from command line, or by using dtManage web interface.    In dtManage console, select first container from the Containers List in Physical application view.  The first container is numbered 000001. Then click the logs dropdown and select the log you want to look at.    Alternatively, the following command can retrieve all application logs, where the first container includes the STRAM log.  yarn logs -applicationId  applicationId   How to check application logs  On dt console, select a container from the Containers List widget\n(default location of this widget is in the \u201cphysical\u201d dashboard). Then\nclick the logs dropdown and select the log you want to look at.   How to check killed operator\u2019s state  On dt console, click on \u201cretrieve killed\u201d button of container List.\nContainers List widget\u2019s default location is on the \u201cphysical\u201d\ndashboard. Then select the appropriate container of killed operator and\ncheck the state.   How to search for particular any application or container?  In applications or containers table there is search text box. You can\ntype in application name or container number to locate particular\napplication or container.  How do I search within logs?  Once you navigate to the logs page,     Download log file to search using your preferred editor    use \u201cgrep\u201d option and provide the search range \u201cwithin specified range\u201d or \u201cover entire log\u201d", 
            "title": "Log analysis"
        }, 
        {
            "location": "/troubleshooting/#launching-applications", 
            "text": "Application goes from accepted state to Finished(FAILED) state  Check if your application name conflicts with any of the already running\napplications in your cluster. Apex do not allow two application with\nsame names run simultaneously. \nYour STRAM logs will have following error: \n\u201cForced shutdown due to Application master failed due to application\n\\ appId> with duplicate application name \\ appName> by the same user\n\\ user name> is already started.\u201d    ConstraintViolationException while application launch  Check if all @NotNull properties of application are set. Apex operator\nproperties are meant to configure parameter to operators. Some of the\nproperties are must have, marked as @NotNull, to use an operator. If you\ndon\u2019t set any of such @NotNull properties application launch will fail\nand stram will throw ConstraintViolationException.", 
            "title": "Launching Applications"
        }, 
        {
            "location": "/troubleshooting/#events", 
            "text": "How to check container failures  In StramEvents list (default location of this widget is in the \u201clogical\u201d\ndashboard), look for event \u201cStopContainer\u201d. Click on \u201cdetails\u201d button in\nfront of event to get details of container failure.  How to search within events  You can search events in specified time range. Select \u201crange\u201d mode in\nStramEvents widget. Then select from and to timestamp and hit the search\nbutton.  tail vs range mode  tail mode allows you to see events as they come in while range mode\nallows you to search for events by time range.  What is \u201cfollowing\u201d button in events pane  When we enable \u201cfollowing\u201d button the stram events list will\nautomatically scroll to bottom when new events come in.  Coming Soon   Connection Refused Exception  ClassNotFound Exception  Launching apa vs jar  DAG validation failed  Multiple gateways running simultaneously, app not launched.  HDFS in safe mode  Application stays in accepted state  Some containers do not get resources (specially in case of repartition)  Insufficient memory set to operator causes operator kill continuously.   Why is the number of events same/different at input and output port of each operator?    Shutdown vs kill option   Why shutdown doesn\u2019t work? (if some containers are not running)  Can I kill multiple applications at same time?  Killing containers vs killing application  STRAM failures (during define partitions)  Thread local + partition parallel configuration  What to do when downstream operators are slow than the input  operators.  I am seeing high latency, what to do?  appConf in ADT (inside apa file) vs conf option in dtcli  Application keeps restarting (has happened once due to license agent during upgrade)  Operator getting killed after every 60 secs (Timeout issue)  How to change commit frequency  Difference between exactly once, at least once and at most once  Thread local vs container local vs node local  Setting operator memory  Turning Bufferserver memory  Turning STRAM memory  Cluster nodes not able to access edge node where Gateway is running   Developers not sure when to process incoming tuples in end window or when to do it in process function of operator    How partitioning works   How the data is partitioned between different partitions.  How to use stream-codec  Data on which ports is partitioned? By default default partitioner partitions data on first port.  How to enable stream-codec on multiple ports. (Join operator?? where both input-ports needs to receive same set of keys).     pom dependency management, exclusions etc. eg: Malhar library and\n    contrib, Hive (includes hadoop dependencies, we need to explicitly\n    exclude), Jersey(we work only with 1.9 version) etc    All non-transient members of the operator object need to be\n    serializable. All members that are not serializable cannot be saved\n    during checkpoint and must be declared transient (e.g. connection\n    objects). This is such a common problem that we need to dedicate a\n    section to it.    Exactly once processing mode. Commit operation is supposed to be\n    done at endWindow. This is only best-effort exactly once and not\n    100% guaranteed exactly once because operators may go down after\n    endWindow and before checkpointing finishes.    How to check checkpoint size. (large checkpoint size cause instability in the DAG).   How to add custom metrics and metric aggregator.  Example of how to implement dynamic partitioning.", 
            "title": "Events"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary of Terms\n\n\nApache Apex\n\n\n\n\nApache Hadoop\n\u00a0- \u00a0\nApache Hadoop\n is a programming framework that supports the processing of large data sets in a distributed computing environment.\n\n\nYARN\n\u00a0- \nApache Hadoop YARN\n (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.\n\n\nResource Manager\n\u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.\n\n\nContainer\n\u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.\n\n\nApplication\n\u00a0- unified batch and real-time stream processing application running on Apache Apex platform.\n\n\nOperator\n\u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.\n\n\nPhysical Operator\n - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.\n\n\nPort\n\u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.\n\n\nStream\n\u00a0- A stream consists of data tuples that flow from one port of an operator to another.\n\n\nSTRAM\n - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.\n\n\nLogical Plan\n\u00a0- Logical representation of an Apache Apex application, where the computational nodes are called \nOperators\n and the data-flow edges are called\u00a0\nStreams\n.\n\n\nPhysical Plan\n\u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.\n\n\nDAG\n\u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their \nLogical\n / \nPhysical\n plans, composed of operators connected by streams.\n\n\nData Tuples Processed\n\u00a0- Number of data objects processed by the operators in an Apache Apex application.\n\n\nData Tuples Emitted\n\u00a0- Number of data objects emitted by the operators with an output port.\n\n\nCurrent Window Id\n\u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.\n\n\nRecovery Window Id\n\u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.\n\n\n\n\nDataTorrent RTS\n\n\n\n\ndtGateway\n\u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS\n\n\ndtManage\n\u00a0- the web based interface to install, configure, manage \n monitor Apache Apex applications running in a Hadoop Cluster\n\n\ndtAssemble\n\u00a0- graphical application assembly tool used to develop applications.\n\n\ndtDashboard\n\u00a0- graphical visualization tool to view and query system and application data.", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary-of-terms", 
            "text": "", 
            "title": "Glossary of Terms"
        }, 
        {
            "location": "/glossary/#apache-apex", 
            "text": "Apache Hadoop \u00a0- \u00a0 Apache Hadoop  is a programming framework that supports the processing of large data sets in a distributed computing environment.  YARN \u00a0-  Apache Hadoop YARN  (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.  Resource Manager \u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.  Container \u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.  Application \u00a0- unified batch and real-time stream processing application running on Apache Apex platform.  Operator \u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.  Physical Operator  - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.  Port \u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.  Stream \u00a0- A stream consists of data tuples that flow from one port of an operator to another.  STRAM  - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.  Logical Plan \u00a0- Logical representation of an Apache Apex application, where the computational nodes are called  Operators  and the data-flow edges are called\u00a0 Streams .  Physical Plan \u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.  DAG \u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their  Logical  /  Physical  plans, composed of operators connected by streams.  Data Tuples Processed \u00a0- Number of data objects processed by the operators in an Apache Apex application.  Data Tuples Emitted \u00a0- Number of data objects emitted by the operators with an output port.  Current Window Id \u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.  Recovery Window Id \u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/glossary/#datatorrent-rts", 
            "text": "dtGateway \u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS  dtManage \u00a0- the web based interface to install, configure, manage   monitor Apache Apex applications running in a Hadoop Cluster  dtAssemble \u00a0- graphical application assembly tool used to develop applications.  dtDashboard \u00a0- graphical visualization tool to view and query system and application data.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Additional Resources\n\n\nApache Apex\n\n\nTo find out more about Apache Apex visit\n\n\n\n\nApache Apex (incubating): \nhttp://apex.incubator.apache.org/\n\n\nApex Mailing List: \ndev@apex.incubator.apache.org\n\n\nApex Overview and Comparison\n\n\n\n\nDataTorrent RTS\n\n\nFor more DataTorrent documentation visit\n\n\n\n\nBlogs\n\n\nFeatured Resources\n\n\nProduct Features\n\n\nArchitecture Overview\n\n\nDocumentation and Guides\n\n\n\n\nFor webinars and videos check out\n\n\n\n\nWebinars\n\n\nSolution Demos", 
            "title": "Resources"
        }, 
        {
            "location": "/additional_docs/#additional-resources", 
            "text": "Apache Apex  To find out more about Apache Apex visit   Apache Apex (incubating):  http://apex.incubator.apache.org/  Apex Mailing List:  dev@apex.incubator.apache.org  Apex Overview and Comparison   DataTorrent RTS  For more DataTorrent documentation visit   Blogs  Featured Resources  Product Features  Architecture Overview  Documentation and Guides   For webinars and videos check out   Webinars  Solution Demos", 
            "title": "Additional Resources"
        }
    ]
}