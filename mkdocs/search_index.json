{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to DataTorrent RTS!\n\n\nDataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.  \n\n\n\n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n\n\n\n\n\n  \n\n    \n\n    \nRun Demos\n\n    \nExperience the power of DataTorrent RTS quickly. Import, launch, manage and visualize applications in minutes.\n\n  \n\n\n  \n\n\n  \n\n    \n\n    \nDiscover RTS\n\n    \nLearn about the architecture and the rich feature set of the DataTorrent RTS platform and applications.\n\n  \n  \n\n  \n\n\n  \n\n    \n\n    \nCreate Apps\n\n    \nTutorials and code samples to rapidly create DataTorrent applications using Java or dtAssemble.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/#welcome-to-datatorrent-rts", 
            "text": "DataTorrent RTS, built on Apache Apex, provides a high-performing, fault-tolerant, scalable, easy to use data processing platform for both batch and streaming workloads. DataTorrent RTS includes advanced management, monitoring, development, visualization, and data ingestion and distribution features.    \n  #docs-jumbotron {\n    margin-top: 40px;\n    font-size: 0;\n    background: rgba(0,0,0,0.05);\n    margin-bottom: 20px;\n    box-sizing: border-box;\n    padding: 0;\n    background-color: transparent;\n  }\n\n  .jumbotron {\n    display: -webkit-flex;\n    display: -ms-flexbox;\n    display: flex;\n  }\n\n  .jumbotron-section-space {\n    flex: .07;\n    background-color: transparent;\n  }\n\n  .jumbotron-section {\n    flex: 1;\n    padding-top: 20px;\n    width: 33.3%;\n    display: inline-block;\n    font-size: 18px;\n    vertical-align: top;\n    text-align: center;\n    color: #444 !important;\n    margin-bottom: 0;\n    padding-bottom: 20px;\n    border-radius: 4px;\n    box-shadow: inset 0 0 4px -1px rgba(0,0,0,0.3);\n    background-color: #e3e0eb;\n  }\n  .jumbotron-section:visited {\n    color: #444;\n  }\n  .jumbotron-section img {\n    height: 100px !important;\n    display: block;\n    margin: 10px auto 0;\n  }\n  .jumbotron-section h2 {\n    margin: 0;\n  }\n\n  .jumbotron-section:hover{\n    background-color: #bcb3ce;\n    cursor: pointer;\n  }\n  .jumbotron-section p {\n    padding: 0 1em;\n    margin: 0 auto;\n    max-width: 300px;\n    font-size: 80%;\n  }\n\n  @media all and (max-width: 1032px) and (min-width: 769px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }\n  @media all and (max-width: 569px){\n    .jumbotron {\n      display: block;\n    }\n    .jumbotron-section {\n      width: 100%;\n      display: block;\n    }\n  }", 
            "title": "Welcome to DataTorrent RTS!"
        }, 
        {
            "location": "/demo_videos/", 
            "text": "DataTorrent RTS Recorded Demos\n\n\nThe following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance. \n\n\nApplication Builder\n\n\n\n\n\nDimensional Computing\n\n\n\n\n\nElastic Scalability\n\n\n\n\n\nFault Tolerance", 
            "title": "Videos"
        }, 
        {
            "location": "/demo_videos/#datatorrent-rts-recorded-demos", 
            "text": "The following videos provide an introduction to DataTorrent RTS. Application Builder shows how you can an application and the next 3 videos show how the platform provides built-in dimension computing, elastic scalability and fault tolerance.   Application Builder   Dimensional Computing   Elastic Scalability   Fault Tolerance", 
            "title": "DataTorrent RTS Recorded Demos"
        }, 
        {
            "location": "/demos/", 
            "text": "Running Demo Applications\n\n\nDataTorrent RTS includes a number of demo applications, which help demonstrate the features of the platform.  These demos are available for import from the \nDevelopment \n App Packages\n section of the DataTorrent management console.\n\n\nImporting Demo Applications\n\n\n\n\nNavigate to \nDevelop \n App Packages \n Import\n section of the DataTorrent console.\n\n\nSelect one of the available packages, such as \nApache Apex Malhar Pi Demo\n and click \nImport\n button.\n\n\nImported packages and included applications will be listed under \nDevelop \n App Packages\n page.\n\n\n\n\nLaunching Demo Applications\n\n\nAfter the right application packages have been imported, many demo applications can be launched with a single click.  \nNote\n: Ensure Hadoop YARN and HDFS services are active and ready by making sure there are no errors in the DataTorrent console before launching demo applications.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use \nPiDemo\n application package.\n\n\n\n\n\n\nFrom the list of available Applications, locate PiDemo and press the launch button.\n\n\n\n\n\n\n\n\nProceed with default options on launch confirmation screen by pressing the Launch button.\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\n\n\nMore information about using DataTorrent console is available in \ndtManage Guide\n\n\nConfiguring Launch Parameters\n\n\nSome applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to \n~/.dt/dt-site.xml\n configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the \nCreating Applications\n guide.\n\n\n\n\n\n\nNavigate to \nApp Packages\n under \nDevelop\n tab of the DataTorrent console.  In this example we will use \nApache Apex Malhar Twitter Demo\n application package.  Import this package from \nDevelop \n App Packages \n Import\n if it is not available.\n\n\n\n\n\n\nFrom the list of Applications, select TwitterDemo and press the corresponding launch button.\n\n\n\n\n\n\nRetrieve Twitter API access information by registering for \nTwitter Developer\n account, creating a new \nTwitter Application\n, and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:\n\n\ndt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret\n\n\n\n\n\n\nSelect \nSpecify custom properties\n on the launch confirmation screen, click \nadd required properties\n button, and provide Twitter API access values.  Choose to save this configuration as \ntwitter.xml\n file and proceed to Launch the application.\n\n\n\n\n\n\n\n\nOnce launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the \nMonitor\n section of the console and selecting the launched application.\n\n\n\n\n\n\nView the top 10 tweeted hashtags in real time by generating and viewing the \ndashboard\n.\n\n\n\n\n\n\nStopping Applications\n\n\nApplications can be shut down or killed from the Monitor section of \ndtManage\n by selecting application from the list and clicking \nshutdown\n or \nkill\n buttons.", 
            "title": "Running Apps"
        }, 
        {
            "location": "/demos/#running-demo-applications", 
            "text": "DataTorrent RTS includes a number of demo applications, which help demonstrate the features of the platform.  These demos are available for import from the  Development   App Packages  section of the DataTorrent management console.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/demos/#importing-demo-applications", 
            "text": "Navigate to  Develop   App Packages   Import  section of the DataTorrent console.  Select one of the available packages, such as  Apache Apex Malhar Pi Demo  and click  Import  button.  Imported packages and included applications will be listed under  Develop   App Packages  page.", 
            "title": "Importing Demo Applications"
        }, 
        {
            "location": "/demos/#launching-demo-applications", 
            "text": "After the right application packages have been imported, many demo applications can be launched with a single click.   Note : Ensure Hadoop YARN and HDFS services are active and ready by making sure there are no errors in the DataTorrent console before launching demo applications.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console, and select one of the imported demo packages.  In this example we will use  PiDemo  application package.    From the list of available Applications, locate PiDemo and press the launch button.     Proceed with default options on launch confirmation screen by pressing the Launch button.    Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.     More information about using DataTorrent console is available in  dtManage Guide", 
            "title": "Launching Demo Applications"
        }, 
        {
            "location": "/demos/#configuring-launch-parameters", 
            "text": "Some applications may require additional configuration changes prior to launching.  Configuration changes can be made on the launch confirmation screen or manually applied to  ~/.dt/dt-site.xml  configuration file.  These typically include adding Twitter API keys for twitter demo, or changing performance settings for larger applications.  Guides for various demo applications can be found in the  Creating Applications  guide.    Navigate to  App Packages  under  Develop  tab of the DataTorrent console.  In this example we will use  Apache Apex Malhar Twitter Demo  application package.  Import this package from  Develop   App Packages   Import  if it is not available.    From the list of Applications, select TwitterDemo and press the corresponding launch button.    Retrieve Twitter API access information by registering for  Twitter Developer  account, creating a new  Twitter Application , and navigating to Keys and Access Tokens tab.  Twitter Demo application requires the following to be specified by the user:  dt.operator.TweetSampler.accessToken\ndt.operator.TweetSampler.accessTokenSecret\ndt.operator.TweetSampler.consumerKey\ndt.operator.TweetSampler.consumerSecret    Select  Specify custom properties  on the launch confirmation screen, click  add required properties  button, and provide Twitter API access values.  Choose to save this configuration as  twitter.xml  file and proceed to Launch the application.     Once launched, view the running application by following the link provided in the launch confirmation dialog, or by navigating to the  Monitor  section of the console and selecting the launched application.    View the top 10 tweeted hashtags in real time by generating and viewing the  dashboard .", 
            "title": "Configuring Launch Parameters"
        }, 
        {
            "location": "/demos/#stopping-applications", 
            "text": "Applications can be shut down or killed from the Monitor section of  dtManage  by selecting application from the list and clicking  shutdown  or  kill  buttons.", 
            "title": "Stopping Applications"
        }, 
        {
            "location": "/sandbox/", 
            "text": "DataTorrent RTS Sandbox\n\n\nWelcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  The Sandbox provides a quick an simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster.  The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.\n\n\nInstallation\n\n\nIf you have not already, sandbox can be downloaded by visiting \ndatatorrent.com/download\n.  To run the DataTorrent Sandbox, ensure you have downloaded and installed \nVirtualBox\n 4.3 or greater.\n\n\nAccessing Console\n\n\nWhen accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username \ndtadmin\n and password \ndtadmin\n.  Same credentials are also valid for sandbox system access.\n\n\n\n\n\n\n\n\nInside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting \nhttp://localhost:9090/\n\n\nRunning Demo Applications\n\n\nOnce authenticated, you can continue to \nDemo Applications\n section to learn how to import, launch, and run demo applications.\n\n\nService Management \n\n\nThe DataTorrent Sandbox automatically launches Hadoop HDFS and YARN, dtGateway, and other required services during startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching applications.  \n\n\n\n\nNote\n: By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and applications startup.\n\n\n\n\nDataTorrent Sandbox automatically launches following services on startup.\n\n\n\n\nHadoop HDFS NameNode\n\n\nHadoop HDFS DataNode\n\n\nHadoop YARN ResourceManager\n\n\nHadoop YARN NodeManager\n\n\nDataTorrent Gateway\n\n\n\n\nFollowing service management actions are available:\n\n\n\n\nStart all Hadoop and DataTorrent services\n\n\nShut down all Hadoop and DataTorrent services\n\n\nRebuild HDFS (deletes all data!) and restart all services.\n\n\n\n\nTo manage the services following desktop launchers have been set up\n\n\n\n\n\n\nUbuntu Sandbox Edition\n - right-click on \nDataTorrent Services\n desktop launcher\n\n\n\n\n\n\n\n\nLubuntu Sandbox Edition\n - hover the mouse over Lubuntu launcher hidden in the bottom left corner of the sandbox screen, and navigate to DataTorrent menu\n\n\n\n\n\n\n\n\nSupport\n\n\nIf you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on \ndatatorrent.com/contact\n page.", 
            "title": "Sandbox"
        }, 
        {
            "location": "/sandbox/#datatorrent-rts-sandbox", 
            "text": "Welcome to the DataTorrent Sandbox, an introduction to DataTorrent RTS, the industry\u2019s only unified stream and batch processing platform.  The Sandbox provides a quick an simple way to experience DataTorrent RTS without setting up and managing a complete Hadoop cluster.  The Sandbox contains pre-installed DataTorrent RTS Enterprise Edition along with all the Hadoop services required to launch and run the included demo applications.", 
            "title": "DataTorrent RTS Sandbox"
        }, 
        {
            "location": "/sandbox/#installation", 
            "text": "If you have not already, sandbox can be downloaded by visiting  datatorrent.com/download .  To run the DataTorrent Sandbox, ensure you have downloaded and installed  VirtualBox  4.3 or greater.", 
            "title": "Installation"
        }, 
        {
            "location": "/sandbox/#accessing-console", 
            "text": "When accessing DataTorrent console in the sandbox for the first time, you will be required to log in.  Use username  dtadmin  and password  dtadmin .  Same credentials are also valid for sandbox system access.     Inside the DataTorrent RTS Sandbox console can be accessed by opening a browser and visiting  http://localhost:9090/", 
            "title": "Accessing Console"
        }, 
        {
            "location": "/sandbox/#running-demo-applications", 
            "text": "Once authenticated, you can continue to  Demo Applications  section to learn how to import, launch, and run demo applications.", 
            "title": "Running Demo Applications"
        }, 
        {
            "location": "/sandbox/#service-management", 
            "text": "The DataTorrent Sandbox automatically launches Hadoop HDFS and YARN, dtGateway, and other required services during startup.  Depending on the host machine capabilities, these may take from several seconds to several minutes to start up.  Until Hadoop services are active and ready, it is normal to see error messages about availability of HDFS and YARN in the Issues section of the DataTorrent console.  Ensure there are no errors remaining in the console by allowing sufficient time for Hadoop services startup prior to launching applications.     Note : By default, this sandbox is designed to run with 6 GB of RAM.  Limited resources may cause delays during Hadoop services and applications startup.   DataTorrent Sandbox automatically launches following services on startup.   Hadoop HDFS NameNode  Hadoop HDFS DataNode  Hadoop YARN ResourceManager  Hadoop YARN NodeManager  DataTorrent Gateway   Following service management actions are available:   Start all Hadoop and DataTorrent services  Shut down all Hadoop and DataTorrent services  Rebuild HDFS (deletes all data!) and restart all services.   To manage the services following desktop launchers have been set up    Ubuntu Sandbox Edition  - right-click on  DataTorrent Services  desktop launcher     Lubuntu Sandbox Edition  - hover the mouse over Lubuntu launcher hidden in the bottom left corner of the sandbox screen, and navigate to DataTorrent menu", 
            "title": "Service Management "
        }, 
        {
            "location": "/sandbox/#support", 
            "text": "If you experience issues while experimenting with the sandbox, or have any feedback and comments, please let us know, and we will be happy to help!  Contact us using one of the methods listed on  datatorrent.com/contact  page.", 
            "title": "Support"
        }, 
        {
            "location": "/create/", 
            "text": "Getting Started\n\n\nGet started quickly by following one of these tutorials, and create your first Apache Apex application today!\n\n\nTop N Words\n\n\nTop N Words\n is a complete guide to writing your first Apache Apex application using \nJava\n or \ndtAssemble\n\n\n\n\nSales Dimensions\n\n\nSales Dimensions\n is an introduction to assembling and visualizing sales analytics applicaiton with \ndtAssemble\n \n\n\n\n\n\n\nAdvanced Topics\n\n\n\n\nApplication Development\n - comprehensive guide to developing Apache Apex applications\n\n\nApplication Packaging\n - creating application packages, changing settings, and launching application packages \n\n\nOperator Development\n - creating new operators for Apache Apex applications\n\n\ndtGateway REST API\n - complete listing of all services offered by dtGateway", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/create/#getting-started", 
            "text": "Get started quickly by following one of these tutorials, and create your first Apache Apex application today!", 
            "title": "Getting Started"
        }, 
        {
            "location": "/create/#top-n-words", 
            "text": "Top N Words  is a complete guide to writing your first Apache Apex application using  Java  or  dtAssemble", 
            "title": "Top N Words"
        }, 
        {
            "location": "/create/#sales-dimensions", 
            "text": "Sales Dimensions  is an introduction to assembling and visualizing sales analytics applicaiton with  dtAssemble", 
            "title": "Sales Dimensions"
        }, 
        {
            "location": "/create/#advanced-topics", 
            "text": "Application Development  - comprehensive guide to developing Apache Apex applications  Application Packaging  - creating application packages, changing settings, and launching application packages   Operator Development  - creating new operators for Apache Apex applications  dtGateway REST API  - complete listing of all services offered by dtGateway", 
            "title": "Advanced Topics"
        }, 
        {
            "location": "/tutorials/topnwords/", 
            "text": "Top N Words Application\n\n\nThe Top N words application is a tutorial on building a word counting application using:\n\n\n\n\nApache Apex platform\n\n\nApache Apex Malhar, an associated library of operators\n\n\nOther related tools\n\n\n\n\nNote: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.\n\n\nThe Top N words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words, and computes the word-frequency for that specific file,\nas well as across all files that are processed. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on \ndtDashboard\n, the browser-based dashboard of DataTorrent RTS.\n\n\nA simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:\n\n\n\n\nThe Apex platform\n\n\nThe operator library\n\n\nThe tools required for developing and deploying\n    applications on a cluster\n\n\ndtcli\n \n the command-line tool for managing\n    application packages and the constituent applications\n\n\ndtManage\n \n for monitoring the applications\n\n\ndtDashboard\n \n for visualizing the output\n\n\ndtAssemble\n \n for  visual application assembly\n\n\n\n\nIn the context of such an application, a number of questions arise:\n\n\n\n\nWhat operators do we need ?\n\n\nHow many are present in the Malhar library ?\n\n\nHow many need to be written from scratch ?\n\n\nHow are operators wired together ?\n\n\nHow do we monitor the running application ?\n\n\nHow do we display the output data in an aesthetically pleasing way ?\n\n\n\n\nThe answers to these and other questions are explored in the sections below.\n\n\nFor this tutorial, use the Data Torrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and DataTorrent RTS 3.1.1 configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/topnwords/#top-n-words-application", 
            "text": "The Top N words application is a tutorial on building a word counting application using:   Apache Apex platform  Apache Apex Malhar, an associated library of operators  Other related tools   Note: Before you begin, ensure that you have internet connectivity\nbecause, in order to complete this tutorial, you will need to download\nthe Apex and Malhar code.  The Top N words application monitors an input directory for new\nfiles. When the application detects a new file, it reads its lines,\nsplits them into words, and computes the word-frequency for that specific file,\nas well as across all files that are processed. The top N words (by\nfrequency) and their frequencies are output to a corresponding file in\nan output directory. Simultaneously, the word-frequency pairs are also\nupdated on  dtDashboard , the browser-based dashboard of DataTorrent RTS.  A simple word counting exercise was chosen because the goal of this tutorial is to focus on the use of:   The Apex platform  The operator library  The tools required for developing and deploying\n    applications on a cluster  dtcli    the command-line tool for managing\n    application packages and the constituent applications  dtManage    for monitoring the applications  dtDashboard    for visualizing the output  dtAssemble    for  visual application assembly   In the context of such an application, a number of questions arise:   What operators do we need ?  How many are present in the Malhar library ?  How many need to be written from scratch ?  How are operators wired together ?  How do we monitor the running application ?  How do we display the output data in an aesthetically pleasing way ?   The answers to these and other questions are explored in the sections below.  For this tutorial, use the Data Torrent RTS Sandbox; it comes pre-installed\nwith Apache Hadoop and DataTorrent RTS 3.1.1 configured as a single-node\ncluster and includes a time-limited enterprise license. If you've already installed the RTS Enterprise Edition (evaluation or production license), you\ncan use that setup instead.", 
            "title": "Top N Words Application"
        }, 
        {
            "location": "/tutorials/topnwords-c1/", 
            "text": "Setting up your development environment\n\n\nThis section describes how you can set up your development environment\nincluding starting the sandbox and downloading some sample input files for\ntesting the application.\n\n\nSample input files\n\n\nFor this tutorial, you need some sample text files to use as input application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:\n\n\nOpen a terminal and run the following commands to create a directory named\n\ndata\n under your home directory and download 3 files there:\n\n\ncd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt\n\n\n\nValidation for third-party applications\n\n\nIf you are using your own installation of DataTorrent RTS instead\nof Sandbox, make sure that you have Java JDK (version 1.7.0_79 or\nlater), Maven (version 3.0.5 or later), and Git (version 1.7.1 or later)\nby running the following commands:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nExpected output\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_79\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_79-b15) \n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.0.5 \n\n\nMaven home: /usr/share/maven \n\n\nJava version: 1.7.0_79, vendor: Oracle Corporation \n\n\nJava home: /home/\nuser\n/Software/java/jdk1.7.0_79/jre \n\n\nDefault locale: en_US, platform encoding: UTF-8 \n\n\nOS name: \nlinux\n, version: \n3.16.0-44-generic\n, arch: \namd64\n, family: \nunix\n \n\n\n\n\n\n\ngit --version\n\n\ngit version 1.7.1\n\n\n\n\n\n\n\n\n\n\n\nSet up the sandbox\n\n\nAt the time of writing, the sandbox corresponds to version 3.1.1 of DataTorrent\nRTS, which, as noted above, includes a complete, stand-alone, instance of the\nEnterprise Edition configured as a single-node Hadoop cluster.\n\n\nBefore you begin, ensure that you have Oracle VM VirtualBox version 4.3 or\nlater on your development machine. The sandbox is a virtual appliance bundled\nalong with Ubuntu 12.0.4 (or later) that needs to be imported into VirtualBox.\n\n\nThese steps describe how to download, import, and start the sandbox.\n\n\n\n\n\n\nDownload Sandbox:\n\n\n\n\nOpen \nhttps://www.datatorrent.com/download/\n in a web browser.\n\n\nUnder \nDataTorrent RTS Sandbox\n, click \nDOWNLOAD NOW\n button.\n\n\nOn the contact details form that appears, provide your name, email, and\n   your organization s name, and click \nSubmit\n.\n\n\nClick the link named \nclick here\n (scroll the page down if necessary to\n   see the link).\n\n\nOn the Sandbox downloads page that appears, click \nDownload\n under\n    \nRequirements\n.\n\n\n\n\n\n\n\n\nImport the sandBox into Oracle VirtualBox:\n\n\n\n\nOpen the VirtualBox Manager.\n\n\nIn the \nFile\n menu, choose \nImport Appliance\n.  \n\n\nOn the \nAppliance to import\n dialog box, type or select the full path to\n    the OVA template file that you downloaded and click \nNext\n.\n\n\nClick \nImport\n.\n\n\n\n\n\n\n\n\nAfter the import completes, click the \nStart\n button on the VirtualBox\n    Manager. If this is a first-time start, select the default operating\n    system and wait till the virtual machine initializes.\n\n\nAfter the virtual machine initializes, wait for a few minutes to allow all\nHadoop and Sandbox processes to start.\n\n\n\n\n\n\nLog on to Sandbox.\n\n\n\n\n\n\nIn the browser that appears displaying the Readme, click DataTorrent\n    Console. You can also click the \nDataTorrent Console\n button on the toolbar of your virtual machine.\n\n\n\n\n\n\nType the username and password (dtadmin/dtadmin), and click \nLogin\n.\n\n\n\n\n\n\n\n\n\n\n\n\nYou should see a welcome page with a diagram illustrating the components in the\nplatform stack; each block of the diagram is a clickable link for exploring\nthat component.\n\n\nValidate the Sandbox setup\n\n\nAfter setting up the sandbox, validate the installation:\n\n\n\n\n\n\nOn the top navigation bar, look for links named \nConfigure\n, \nDevelop\n,\n   \nMonitor\n, \nVisualize\n and \nLearn\n.\n\n\n\n\n\n\nIf Visualize is not present, click \nConfigure\n, and then \nLicense\n    Information\n.\n\n\n\n\n\n\nA page showing license details including the text: \nLicense Edition:\n  enterprise\n appears.\n\n\n\n\nIf the license details display   \ncommunity\n instead of \nenterprise\n, wait\n  for a few minutes, refresh the page, and check again.\n\n\nIf that does not work, navigate to the other tabs, and repeat these steps\n  again. The license edition should be \nenterprise\n and you should see the\n  \nVisualize\n link on the top navigation bar.\n\n\n\n\nNote: The enterprise license is present on the Hadoop HDFS file system. To\nretrieve details of this license, the HDFS servers need to be up and running,\nwhich can take a few minutes.", 
            "title": "Development Environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#setting-up-your-development-environment", 
            "text": "This section describes how you can set up your development environment\nincluding starting the sandbox and downloading some sample input files for\ntesting the application.", 
            "title": "Setting up your development environment"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#sample-input-files", 
            "text": "For this tutorial, you need some sample text files to use as input application.\nBinary files such as PDF or DOCX files are not suitable since they contain a\nlot of meaningless strings that look like words (for example,  Wqgi ).\nSimilarly, files using markup languages such as XML or HTML files are also not\nsuitable since the tag names such as  div ,  td  and  p  dominate the word\ncounts. The RFC (Request for Comment) files that are used as de-facto\nspecifications for internet standards are good candidates since they contain\npure text; download a few of them as follows:  Open a terminal and run the following commands to create a directory named data  under your home directory and download 3 files there:  cd; mkdir data; cd data  \nwget http://tools.ietf.org/rfc/rfc1945.txt  \nwget https://www.ietf.org/rfc/rfc2616.txt  \nwget https://tools.ietf.org/rfc/rfc4844.txt", 
            "title": "Sample input files"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#validation-for-third-party-applications", 
            "text": "If you are using your own installation of DataTorrent RTS instead\nof Sandbox, make sure that you have Java JDK (version 1.7.0_79 or\nlater), Maven (version 3.0.5 or later), and Git (version 1.7.1 or later)\nby running the following commands:         Command  Expected output    java -version  java version  1.7.0_79  Java(TM) SE Runtime Environment (build 1.7.0_79-b15)   Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)    mvn --version  Apache Maven 3.0.5   Maven home: /usr/share/maven   Java version: 1.7.0_79, vendor: Oracle Corporation   Java home: /home/ user /Software/java/jdk1.7.0_79/jre   Default locale: en_US, platform encoding: UTF-8   OS name:  linux , version:  3.16.0-44-generic , arch:  amd64 , family:  unix      git --version  git version 1.7.1", 
            "title": "Validation for third-party applications"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#set-up-the-sandbox", 
            "text": "At the time of writing, the sandbox corresponds to version 3.1.1 of DataTorrent\nRTS, which, as noted above, includes a complete, stand-alone, instance of the\nEnterprise Edition configured as a single-node Hadoop cluster.  Before you begin, ensure that you have Oracle VM VirtualBox version 4.3 or\nlater on your development machine. The sandbox is a virtual appliance bundled\nalong with Ubuntu 12.0.4 (or later) that needs to be imported into VirtualBox.  These steps describe how to download, import, and start the sandbox.    Download Sandbox:   Open  https://www.datatorrent.com/download/  in a web browser.  Under  DataTorrent RTS Sandbox , click  DOWNLOAD NOW  button.  On the contact details form that appears, provide your name, email, and\n   your organization s name, and click  Submit .  Click the link named  click here  (scroll the page down if necessary to\n   see the link).  On the Sandbox downloads page that appears, click  Download  under\n     Requirements .     Import the sandBox into Oracle VirtualBox:   Open the VirtualBox Manager.  In the  File  menu, choose  Import Appliance .    On the  Appliance to import  dialog box, type or select the full path to\n    the OVA template file that you downloaded and click  Next .  Click  Import .     After the import completes, click the  Start  button on the VirtualBox\n    Manager. If this is a first-time start, select the default operating\n    system and wait till the virtual machine initializes.  After the virtual machine initializes, wait for a few minutes to allow all\nHadoop and Sandbox processes to start.    Log on to Sandbox.    In the browser that appears displaying the Readme, click DataTorrent\n    Console. You can also click the  DataTorrent Console  button on the toolbar of your virtual machine.    Type the username and password (dtadmin/dtadmin), and click  Login .       You should see a welcome page with a diagram illustrating the components in the\nplatform stack; each block of the diagram is a clickable link for exploring\nthat component.", 
            "title": "Set up the sandbox"
        }, 
        {
            "location": "/tutorials/topnwords-c1/#validate-the-sandbox-setup", 
            "text": "After setting up the sandbox, validate the installation:    On the top navigation bar, look for links named  Configure ,  Develop ,\n    Monitor ,  Visualize  and  Learn .    If Visualize is not present, click  Configure , and then  License\n    Information .    A page showing license details including the text:  License Edition:\n  enterprise  appears.   If the license details display    community  instead of  enterprise , wait\n  for a few minutes, refresh the page, and check again.  If that does not work, navigate to the other tabs, and repeat these steps\n  again. The license edition should be  enterprise  and you should see the\n   Visualize  link on the top navigation bar.   Note: The enterprise license is present on the Hadoop HDFS file system. To\nretrieve details of this license, the HDFS servers need to be up and running,\nwhich can take a few minutes.", 
            "title": "Validate the Sandbox setup"
        }, 
        {
            "location": "/tutorials/topnwords-c2/", 
            "text": "Building top N words using JAVA\n\n\nThis chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on the sandbox. We will use the \ndtManage\n GUI tool to launch the\napplication.\n\n\nNote\n: You can build the top N words application using an IDE such as NetBeans\nor maven on the command line. The following sections describe both ways.\n\n\nStep I: Clone the Apex Malhar repository\n\n\nClone the Malhar repository (we will use some of these source files in a later\nsection):\n\n\n\n\n\n\nOpen a terminal window and create a new directory where you want the code\n    to reside, for example: \ncd ~/src; mkdir dt; cd dt\n\n\n\n\n\n\nDownload the code for Malhar:\n\n\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\nYou should now see a directory named \nincubator-apex-malhar\n.\n\n\n\n\n\n\nNavigate to the \nincubator-apex-malhar\n directory and switch to the\n    \ndevel-3\n branch:\n\n\ncd incubator-apex-malhar\ngit checkout devel-3\n\n\n\n\n\n\n\nStep II: Create the application project using NetBeans IDE\n\n\nThis section describes how to generate a new Apache Apex maven project using\nthe NetBeans IDE (downloadable from \nhttps://netbeans.org/downloads/\n). You can\nalso use other IDEs such as Eclipse if you wish. The next section describes an\nalternative method of doing the same thing from the command-line without using\nan IDE.\n\n\nGenerate a new Maven archetype project as follows:\n\n\n\n\nOpen NetBeans.\n\n\nClick File \n New Project.\n\n\n\n\nFrom the projects list, select \nProject from Archetype\n, and click \nNext\n.\n    \n\n\n\n\n\n\nOn the Maven Archetype window, type \napex\n in the \nSearch\n box, and\n     from the list of \nKnown Archetypes\n, select \napex-app-archetype\n.\n     \n\n\n\n\nMake sure that the values for the fields match the values shown in this\n     table:\n\n\n\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \nField\n\n  \nValue\n\n  \n\n  \n\n  \nGroup ID\n\n  \ncom.datatorrent\n\n  \n\n  \n\n  \nArtifact ID\n\n  \napex-app-archetype\n\n  \n\n  \n\n  \nVersion\n\n  \n3.1.1\n\n  \n\n  \n\n  \nRepository\n\n  \nhttps://www.datatorrent.com/maven/content/repositories/releases/\n\n  \n\n  \n\n  \n\n\n\n\nClick Next.\n\n\n\n\nOn the \nName and Location\n window, do the following:\n\n\n\n\nEnter a name for this project in the \nProject Name\n box, for example,\n    \nTopNWordCount\n.\n\n\nEnter a location for this project in the \nProject Location\n box, for\n     example, \n/home/dtadmin/NetBeansProjects\n.\n\n\nEnter an ID in the \nGroup Id\n box, for example, \ncom.example\n.\n\n\nEnter a version for this project in the \nVersion\n box, for example,\n     \n1.0-SNAPSHOT\n.\n\n\nEnter the package name in the \nPackage\n box, for example,\n      \ncom.example.topnwordcount\n.\n\n\n\n\n\n\n\n\n\n\nClick Finish.\n\n\n\n\n\n\nThe project is generated at the specified location and should be visible in\nthe left panel with the name \nMy Apex Application\n. You can right-click the\nproject and choose \nRename\n to provide a more descriptive name such as\n\nTopNWordCount\n.\n\n\n\n\nStep II (Optional): Create the application project using the command line\n\n\nThe new maven project can be created using the command line (instead of an IDE)\nas follows:\n\n\n\n\n\n\nCopy this script to a simple text file named, for example, \nnewdt.sh\n.\n\n\n#!/bin/bash\n# script to create a new project\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n  -DarchetypeGroupId=com.datatorrent \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=3.1.1 \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.topnwordcount \\\n  -DartifactId=topNwordcount \\\n  -Dversion=1.0-SNAPSHOT\n\n\n\n\n\n\n\nRun the file: \nbash newdt.sh\n\n\nNote\n: The command parameters might require minor adjustments as newer\nversions of Apache Apex are released. The parameters are displayed when you run this command.\n\n\n\n\n\n\nPress \nEnter\n when prompted with \nY : :\n. A new project directory named\n    \ntopNwordcount\n containing source files for a simple application should\n    appear.\n\n\n\n\n\n\nStep III: Copy application files to the new project\n\n\nWe assume you now have a new project created via one of the two methods\noutlined above. We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.\n\n\n\n\nDelete files \nApplication.java\n and \nRandomNumberGenerator.java\n\n   under \nsrc/main/java/com/example/topnwordcount\n.\n\n\nDelete file \nApplicationTest.java\n file under\n   \nsrc/test/java/com/example/topnwordcount\n.\n\n\n\n\nCopy the following files from:\n\n\nincubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/\n\n\n\nto\n\n\nsrc/main/java/com/example/topnwordcount\n\n\n\n\n\nApplicationWithQuerySupport.java\n\n\nFileWordCount.java\n\n\nLineReader.java\n\n\nWCPair.java\n\n\nWindowWordCount.java\n\n\nWordCountWriter.java\n\n\nWordReader.java\n\n\n\n\n\n\n\n\nCopy the file \nWordDataSchema.json\n from\n\n\nincubator-apex-malhar/demos/wordcount/src/main/resources/\n\n\n\nto\n\n\nsrc/main/resources/\n\n\n\nin the new project.\n\n\nNote\n: This file defines the format of data sent to the visualization widgets within \ndtDashboard\n.\n\n\n\n\n\n\nStep IV: Customize the application and operators\n\n\nWe will now customize the application so that we can build and run it in our\nsandbox environment.\n\n\nThe first customization involves the package name within the files which\ncurrently reflects the package from which they were copied. The relevant line\nwithin each file might look like this:\n\n\npackage com.datatorrent.demos.wordcount;\n\n\n\n\nChange this line to reflect the current location, for example:\n\n\npackage com.example.topnwordcount;\n\n\n\n\nNote: This change is easily accomplished in NetBeans by opening each file,\nclicking the red error icon in the left margin, and selecting the\n\nChange package declaration to com.example.topnwordcount\n entry from the\ndrop-down list. Do this for each Java file that you copied.\n\n\nCustomize the operators\n\n\nThe next customization involves the query operators: The new code has two\nquery operators that are embedded within other operators. Support for such\nembedding is absent in the 3.1.1 code, so we must:\n\n\n\n\nRemove the embedding calls\n\n\nAdd the operators directly to the DAG\n\n\nConnect these query operators to the rest of the DAG via suitable streams\n\n\n\n\nNote\n: All the operators used in this application are described in detail in the\nAppendix.\n\n\nTo make these changes, edit the file \nApplicationWithQuerySupport.java\n:\n\n\n\n\n\n\nRemove the lines containing calls to \nsetEmbeddableQueryInfoProvider()\n\n    and add these two lines in their place:\n\n\ndag.addOperator(\"QueryFile\",   wsQueryFile);  \ndag.addOperator(\"QueryGlobal\", wsQueryGlobal);\n\n\n\n\n\n\n\nAdd streams to connect the two query operators to the DAG by adding these\n    lines before the four existing \naddstream()\n calls:\n\n\ndag.addStream(\"QueryFileStream\", wsQueryFile.outputPort, snapshotServerFile.query);\ndag.addStream(\"QueryGlobalStream\", wsQueryGlobal.outputPort, snapshotServerGlobal.query);\n\n\n\n\n\n\n\nSave the file.\n\n\n\n\n\n\nNote\n: After you complete this procedure, the newly copied Java files\nshould not show the red error icon in the IDE.\n\n\nCustomize the application configuration\n\n\nLastly, you must add configure some properties of the application. These\nproperties accomplish the following aims:\n\n\n\n\nLimit the amount of memory used by most operators so that more memory can\n  be allocated for \nfileWordCount\n which maintains the frequency counts.\n\n\nSet the locality of a couple of streams to \nCONTAINER_LOCAL\n to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).\n\n\nDefine the regular expression for matching the non-word string that\n  delimits words.\n\n\nDefine number of top (word, frequency) pairs we want output.\n\n\nDefine the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.\n\n\nDefine the topics for sending queries and retrieving data for visualization.\n\n\n\n\nTo do this:\n\n\nOpen the src/main/resources/META-INF/properties.xml file, and replace its\ncontent with the following:\n\n\nconfiguration\n\n \nproperty\n\n   \nname\ndt.attr.MASTER_MEMORY_MB\n/name\n\n   \nvalue\n500\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.*.operator.*.attr.MEMORY_MB\n/name\n\n   \nvalue\n200\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB\n/name\n\n   \nvalue\n512\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.lineReader.directory\n/name\n\n   \nvalue\n/tmp/test/input-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n/name\n\n   \nvalue\n[\\p{Punct}\\s]+\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wcWriter.filePath\n/name\n\n   \nvalue\n/tmp/test/output-dir\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n/name\n\n   \nvalue\n10\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryFileStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality\n/name\n\n   \nvalue\nCONTAINER_LOCAL\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFile\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\n/name\n\n   \nvalue\nTopNWordsQueryFileResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobal\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n/name\n\n   \nvalue\nTopNWordsQueryGlobalResult\n/value\n\n \n/property\n \nproperty\n\n   \nname\ndt.application.TwitterDemo.operator.wsResult.numRetries\n/name\n\n   \nvalue\n2147483647\n/value\n\n \n/property\n\n\n/configuration\n\n\n\n\n\nStep V: Build the top N words count application\n\n\nTo build the application from NetBeans\n\n\n\n\nOpen NetBeans IDE.\n\n\nRight-click the project, and click \nBuild\n.\n\n\n\n\nBuilding the application from the command line is equally simple:\n\n\ncd topNwordcount; mvn clean package -DskipTests\n\n\n\nIn either case, if the build is successful, it should have created the\napplication package file\n\ntopNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa\n.\n\n\nStep VI: Upload the top N words application package\n\n\nTo upload the top N words application package\n\n\n\n\nLog on to the DataTorrent Console using the default username and password\n   (both are \ndtadmin\n).\n\n\nOn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, click \nupload a package\n.\n\n\n\n\nNavigate to the location of the \ntopNwordcount-1.0-SNAPSHOT.apa\n\n   application package file is stored.\n\n\nWait till the package is successfully uploaded.\n\n\n\n\nStep VII: Launch the top N words application\n\n\nNote\n: Before launching the top N words application, shut down the IDE. If your\nIDE is running at the time of a launch, the sandbox might hang due to resource\nexhaustion.\n\n\n\n\nLog on to the DataTorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nIn the top navigation bar, click \nDevelop\n.\n\n\nUnder \nApp Packages\n, locate the top N word count application, and click\n   \nLaunch Application\n.\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n. To specify individual properties, select \nSpecify\n    custom properties\n.\n\n\nClick Launch.\n\n\n\n\nA message indicating success of the launch operation should appear along with\nthe application ID.\n\n\nNote\n: After a successful launch, monitor the top N words application following\ninstructions in the chapter \nMonitoring with dtManage\n.", 
            "title": "Building in Java"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#building-top-n-words-using-java", 
            "text": "This chapter describes the steps to build the application in Java using some\nsource files from the Malhar repository, suitably modified and customized to\nrun on the sandbox. We will use the  dtManage  GUI tool to launch the\napplication.  Note : You can build the top N words application using an IDE such as NetBeans\nor maven on the command line. The following sections describe both ways.", 
            "title": "Building top N words using JAVA"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-i-clone-the-apex-malhar-repository", 
            "text": "Clone the Malhar repository (we will use some of these source files in a later\nsection):    Open a terminal window and create a new directory where you want the code\n    to reside, for example:  cd ~/src; mkdir dt; cd dt    Download the code for Malhar:  git clone https://github.com/apache/incubator-apex-malhar  You should now see a directory named  incubator-apex-malhar .    Navigate to the  incubator-apex-malhar  directory and switch to the\n     devel-3  branch:  cd incubator-apex-malhar\ngit checkout devel-3", 
            "title": "Step I: Clone the Apex Malhar repository"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-create-the-application-project-using-netbeans-ide", 
            "text": "This section describes how to generate a new Apache Apex maven project using\nthe NetBeans IDE (downloadable from  https://netbeans.org/downloads/ ). You can\nalso use other IDEs such as Eclipse if you wish. The next section describes an\nalternative method of doing the same thing from the command-line without using\nan IDE.  Generate a new Maven archetype project as follows:   Open NetBeans.  Click File   New Project.   From the projects list, select  Project from Archetype , and click  Next .\n        On the Maven Archetype window, type  apex  in the  Search  box, and\n     from the list of  Known Archetypes , select  apex-app-archetype .\n        Make sure that the values for the fields match the values shown in this\n     table:   \n   \n   \n   \n   \n   \n   \n   Field \n   Value \n   \n   \n   Group ID \n   com.datatorrent \n   \n   \n   Artifact ID \n   apex-app-archetype \n   \n   \n   Version \n   3.1.1 \n   \n   \n   Repository \n   https://www.datatorrent.com/maven/content/repositories/releases/ \n   \n   \n     Click Next.   On the  Name and Location  window, do the following:   Enter a name for this project in the  Project Name  box, for example,\n     TopNWordCount .  Enter a location for this project in the  Project Location  box, for\n     example,  /home/dtadmin/NetBeansProjects .  Enter an ID in the  Group Id  box, for example,  com.example .  Enter a version for this project in the  Version  box, for example,\n      1.0-SNAPSHOT .  Enter the package name in the  Package  box, for example,\n       com.example.topnwordcount .      Click Finish.    The project is generated at the specified location and should be visible in\nthe left panel with the name  My Apex Application . You can right-click the\nproject and choose  Rename  to provide a more descriptive name such as TopNWordCount .", 
            "title": "Step II: Create the application project using NetBeans IDE"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-ii-optional-create-the-application-project-using-the-command-line", 
            "text": "The new maven project can be created using the command line (instead of an IDE)\nas follows:    Copy this script to a simple text file named, for example,  newdt.sh .  #!/bin/bash\n# script to create a new project\nmvn archetype:generate \\\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases \\\n  -DarchetypeGroupId=com.datatorrent \\\n  -DarchetypeArtifactId=apex-app-archetype \\\n  -DarchetypeVersion=3.1.1 \\\n  -DgroupId=com.example \\\n  -Dpackage=com.example.topnwordcount \\\n  -DartifactId=topNwordcount \\\n  -Dversion=1.0-SNAPSHOT    Run the file:  bash newdt.sh  Note : The command parameters might require minor adjustments as newer\nversions of Apache Apex are released. The parameters are displayed when you run this command.    Press  Enter  when prompted with  Y : : . A new project directory named\n     topNwordcount  containing source files for a simple application should\n    appear.", 
            "title": "Step II (Optional): Create the application project using the command line"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iii-copy-application-files-to-the-new-project", 
            "text": "We assume you now have a new project created via one of the two methods\noutlined above. We will now copy over a few of the application files downloaded\nin Step I to the appropriate subdirectory of the new project.   Delete files  Application.java  and  RandomNumberGenerator.java \n   under  src/main/java/com/example/topnwordcount .  Delete file  ApplicationTest.java  file under\n    src/test/java/com/example/topnwordcount .   Copy the following files from:  incubator-apex-malhar/demos/wordcount/src/main/java/com/datatorrent/demos/wordcount/  to  src/main/java/com/example/topnwordcount   ApplicationWithQuerySupport.java  FileWordCount.java  LineReader.java  WCPair.java  WindowWordCount.java  WordCountWriter.java  WordReader.java     Copy the file  WordDataSchema.json  from  incubator-apex-malhar/demos/wordcount/src/main/resources/  to  src/main/resources/  in the new project.  Note : This file defines the format of data sent to the visualization widgets within  dtDashboard .", 
            "title": "Step III: Copy application files to the new project"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-iv-customize-the-application-and-operators", 
            "text": "We will now customize the application so that we can build and run it in our\nsandbox environment.  The first customization involves the package name within the files which\ncurrently reflects the package from which they were copied. The relevant line\nwithin each file might look like this:  package com.datatorrent.demos.wordcount;  Change this line to reflect the current location, for example:  package com.example.topnwordcount;  Note: This change is easily accomplished in NetBeans by opening each file,\nclicking the red error icon in the left margin, and selecting the Change package declaration to com.example.topnwordcount  entry from the\ndrop-down list. Do this for each Java file that you copied.  Customize the operators  The next customization involves the query operators: The new code has two\nquery operators that are embedded within other operators. Support for such\nembedding is absent in the 3.1.1 code, so we must:   Remove the embedding calls  Add the operators directly to the DAG  Connect these query operators to the rest of the DAG via suitable streams   Note : All the operators used in this application are described in detail in the\nAppendix.  To make these changes, edit the file  ApplicationWithQuerySupport.java :    Remove the lines containing calls to  setEmbeddableQueryInfoProvider() \n    and add these two lines in their place:  dag.addOperator(\"QueryFile\",   wsQueryFile);  \ndag.addOperator(\"QueryGlobal\", wsQueryGlobal);    Add streams to connect the two query operators to the DAG by adding these\n    lines before the four existing  addstream()  calls:  dag.addStream(\"QueryFileStream\", wsQueryFile.outputPort, snapshotServerFile.query);\ndag.addStream(\"QueryGlobalStream\", wsQueryGlobal.outputPort, snapshotServerGlobal.query);    Save the file.    Note : After you complete this procedure, the newly copied Java files\nshould not show the red error icon in the IDE.  Customize the application configuration  Lastly, you must add configure some properties of the application. These\nproperties accomplish the following aims:   Limit the amount of memory used by most operators so that more memory can\n  be allocated for  fileWordCount  which maintains the frequency counts.  Set the locality of a couple of streams to  CONTAINER_LOCAL  to further\n  reduce memory pressure (necessary on the memory-limited environment of the\n  sandbox).  Define the regular expression for matching the non-word string that\n  delimits words.  Define number of top (word, frequency) pairs we want output.  Define the path to the monitored input directory where input files are\n  dropped and the output directory (both HDFS) to which the per-file top N\n  (word, frequency) pairs are output.  Define the topics for sending queries and retrieving data for visualization.   To do this:  Open the src/main/resources/META-INF/properties.xml file, and replace its\ncontent with the following:  configuration \n  property \n    name dt.attr.MASTER_MEMORY_MB /name \n    value 500 /value \n  /property   property \n    name dt.application.*.operator.*.attr.MEMORY_MB /name \n    value 200 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.attr.MEMORY_MB /name \n    value 512 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.lineReader.directory /name \n    value /tmp/test/input-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr /name \n    value [\\p{Punct}\\s]+ /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wcWriter.filePath /name \n    value /tmp/test/output-dir /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.fileWordCount.topN /name \n    value 10 /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryFileStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.stream.QueryGlobalStream.locality /name \n    value CONTAINER_LOCAL /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryFile.topic /name \n    value TopNWordsQueryFile /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultFile.topic /name \n    value TopNWordsQueryFileResult /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.QueryGlobal.topic /name \n    value TopNWordsQueryGlobal /value \n  /property   property \n    name dt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic /name \n    value TopNWordsQueryGlobalResult /value \n  /property   property \n    name dt.application.TwitterDemo.operator.wsResult.numRetries /name \n    value 2147483647 /value \n  /property  /configuration", 
            "title": "Step IV: Customize the application and operators"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-v-build-the-top-n-words-count-application", 
            "text": "To build the application from NetBeans   Open NetBeans IDE.  Right-click the project, and click  Build .   Building the application from the command line is equally simple:  cd topNwordcount; mvn clean package -DskipTests  In either case, if the build is successful, it should have created the\napplication package file topNwordcount/target/topNwordcount-1.0-SNAPSHOT.apa .", 
            "title": "Step V: Build the top N words count application"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vi-upload-the-top-n-words-application-package", 
            "text": "To upload the top N words application package   Log on to the DataTorrent Console using the default username and password\n   (both are  dtadmin ).  On the top navigation bar, click  Develop .  Under  App Packages , click  upload a package .   Navigate to the location of the  topNwordcount-1.0-SNAPSHOT.apa \n   application package file is stored.  Wait till the package is successfully uploaded.", 
            "title": "Step VI: Upload the top N words application package"
        }, 
        {
            "location": "/tutorials/topnwords-c2/#step-vii-launch-the-top-n-words-application", 
            "text": "Note : Before launching the top N words application, shut down the IDE. If your\nIDE is running at the time of a launch, the sandbox might hang due to resource\nexhaustion.   Log on to the DataTorrent Console (the default username and password are\n   both  dtadmin ).  In the top navigation bar, click  Develop .  Under  App Packages , locate the top N word count application, and click\n    Launch Application .  (Optional) To configure the application using a configuration file, select\n     Use a config file . To specify individual properties, select  Specify\n    custom properties .  Click Launch.   A message indicating success of the launch operation should appear along with\nthe application ID.  Note : After a successful launch, monitor the top N words application following\ninstructions in the chapter  Monitoring with dtManage .", 
            "title": "Step VII: Launch the top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c3/", 
            "text": "Building top N words using dtAssemble\n\n\nYou can build top N words using dtAssemble \n the graphical drag-and-drop\napplication builder.\n\n\nNote\n: This tool is not available with the Community edition license.\n\n\nUsing the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.\n\n\nNote\n: You cannot undo the auto-layout or zoom-to-fit operations.\n\n\nPrerequisites\n\n\nTo use \ndtAssemble\n, you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.\n\n\nTo ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the \nMonitor\n tab, and under \nDataTorrent Applications\n, kill any\nrunning applications.\n\n\nStep I: Create top N words using dtAssemble\n\n\n\n\nLog on to the DataTorrent RTS console (default username and password are\n   both \ndtadmin\n).\n\n\nOn the DataTorrent RTS console, click \nDevelop\n \n \nApp Packages\n.\n\n\nMake sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.\n\n\nClick \nTopNWordCount\n in the name column to see the application details.\n  \n\n\nClick \ncreate new application\n button.\n  \n\n\nType a name for your application, for example, \nTop N words\n, and click\n   \nCreate\n. The \nApplication Canvas\n should open.\n\n\n\n\nThe existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For each JSON application, there are\nthree additional buttons that allow editing, deleting, and cloning operations.\nThese operations are not supported for JAVA applications.\n\n\n\n\nStep II: Drag operators to the application canvas\n\n\n\n\nWait till the Application Canvas opens.\n  \n\n\nFrom the Operator Library list on the left, locate the desired\n   operators by either:\n\n\n\n\nExploring the categories.\n\n\n-or-\n\n\n\n\n\n\nUsing the \nquick find\n feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type \nfile\n  into the search box to see a list of\n   matching operators. For example, the first operator is \nLineReader\n.\n    \n\n\n\n\nDrag the operator onto the canvas.\n\n\nRepeat this process for all the operators described in Appendix entitled\n    \nOperators in Top N words application\n, and arrange them on the canvas.\n\n\nTo magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.\n\n\n\n\nStep III: Connect the operators\n\n\nObserve that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.\n\n\nConnect the operators as shown in the diagram below. Note the following points about\nthe \nFileWordCount\n operator which has the largest number of connections:\n\n\n\n\nThe control port is connected to the control port of \nLineReader\n.\n\n\nThe input port is connected to the output port of \nWindowWordCount\n.\n\n\nThe \nfileOutput\n port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of \nWordCountWriter\n.\n\n\nThe \noutputGlobal\n port emits the global top N pairs and so is connected to\n  the input port of \nAppDataSnapshotMapServer\n.\n\n\nThe \noutputPerFile\n port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to \nConsoleOutput\n as\n  well as to \nAppDataSnapshotMapServer\n.\n\n\n\n\nNote\n: As you make changes, the top left corner displays \nAll\nchanges saved to HDFS\n. No explicit save step is needed.\n\n\nAfter you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.\n\n\n\n\nStep IV: Configure the operator properties\n\n\nThe last step before running this application is to configure\nproperties of the operators.\n\n\n\n\nClick the first operator (\nLineReader\n) in the canvas to see the list of\n   configurable properties in the right panel.\n\n\n\n\nLocate the property labelled \nDirectory\n and enter the path to the input\n   directory: \n/tmp/test/input-dir\n:\n\n\n\n\n\n\n\n\nConfigure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nProperty Name\n\n\nValue\n\n\n\n\n\n\n5\n\n\nFile Path\n\n\n/tmp/test/output-dir\n\n\n\n\n\n\n2\n\n\nNon Word Str\n\n\n[\\p{Punct}\\s]+\n\n\n\n\n\n\n9\n\n\nTopic\n\n\nTopNWordsQueryFile\n\n\n\n\n\n\n10\n\n\nTopic\n\n\nTopNWordsQueryGlobal\n\n\n\n\n\n\n11\n\n\nTopic\n\n\nTopNWordsQueryFileResult\n\n\n\n\n\n\n12\n\n\nTopic\n\n\nTopNWordsQueryGlobalResult\n\n\n\n\n\n\n7, 8\n\n\nSnapshot Schema JSON\n\n\n{ \nvalues\n: [{\nname\n: \nword\n, \ntype\n: \nstring\n},\n\n\n{\nname\n: \ncount\n, \ntype\n: \ninteger\n}] }\n\n\n\n\n\n\n4\n\n\nTop N\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\nClick Stream 8 and Stream 9, and change \nStream Locality\n from\n  \nAUTOMATIC\n to \nCONTAINER_LOCAL\n to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.\n\n\n\n\n\n\nClick the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to \nMaster Memory Mb\n, and\n   change its value to 500.\n\n\n\n\n\n\nClick each operator, navigate to the \nMemory Mb\n attribute in the\n   \nAttributes\n section, and change the value to 200 except for \nOperator\n    4\n for which the value is 512.\n\n\n\n\n\n\nClick \nlaunch\n in the top-left corner. \nNote\n: Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n  \n\n\n\n\n\n\nOn the launch application dialog window, type a name for your application.\n  \n\n\n\n\n\n\n(Optional) To configure the application using a configuration file, select\n    \nUse a config file\n checkbox. To specify individual properties, select\n    \nSpecify custom properties\n checkbox.\n\n\n\n\nClick \nLaunch\n.\n\n\n\n\nA transient pop-up at the top-right indicating that the launch was successful\nshould appear.\n\n\n\nAfter a successful launch, monitor the application following\ninstructions in the Chapter entitled \nMonitoring with dtManage\n.", 
            "title": "Building with dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#building-top-n-words-using-dtassemble", 
            "text": "You can build top N words using dtAssemble   the graphical drag-and-drop\napplication builder.  Note : This tool is not available with the Community edition license.  Using the application builder, you can manually drag and drop participating operators on to the application canvas and connect them to build the DAG for\nthe top N words application. You can move the application canvas in any\ndirection using your mouse. You can also try the auto-layout and zoom-to-fit\nbuttons at the top-right of the canvas to create an initial arrangement.  Note : You cannot undo the auto-layout or zoom-to-fit operations.  Prerequisites  To use  dtAssemble , you should have uploaded an application package that\ncontains all the operators you intend to use. The new application will reside\nin that package. For this exercise, we will use the package that you built and\nuploaded in the earlier chapter.  To ensure that the full complement of sandbox resources are available for\nrunning the current application, ensure that no other applications are running\nby clicking the  Monitor  tab, and under  DataTorrent Applications , kill any\nrunning applications.", 
            "title": "Building top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-i-create-top-n-words-using-dtassemble", 
            "text": "Log on to the DataTorrent RTS console (default username and password are\n   both  dtadmin ).  On the DataTorrent RTS console, click  Develop     App Packages .  Make sure that the top N words package that you built following the\n   instructions of the previous chapter is uploaded.  Click  TopNWordCount  in the name column to see the application details.\n    Click  create new application  button.\n    Type a name for your application, for example,  Top N words , and click\n    Create . The  Application Canvas  should open.   The existing application is a JAVA application. Applications that you create\nusing dtAssemble are JSON applications. For each JSON application, there are\nthree additional buttons that allow editing, deleting, and cloning operations.\nThese operations are not supported for JAVA applications.", 
            "title": "Step I: Create top N words using dtAssemble"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-ii-drag-operators-to-the-application-canvas", 
            "text": "Wait till the Application Canvas opens.\n    From the Operator Library list on the left, locate the desired\n   operators by either:   Exploring the categories.  -or-    Using the  quick find  feature by typing the first few letters of the name of\n   the implementing class or related terms in the search box. For example, to\n   find the file reader, type  file   into the search box to see a list of\n   matching operators. For example, the first operator is  LineReader .\n       Drag the operator onto the canvas.  Repeat this process for all the operators described in Appendix entitled\n     Operators in Top N words application , and arrange them on the canvas.  To magnify or shrink the operators, use the buttons in the top-right corner.\n   You can also use the scroll wheel of your mouse to zoom in or out. The\n   entire canvas can also be moved in any direction with the mouse.", 
            "title": "Step II: Drag operators to the application canvas"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iii-connect-the-operators", 
            "text": "Observe that each operator shows output ports in pink and input ports in blue.\nThe names of the operators and the corresponding JAVA classes are also shown.\nYou can change the operator name by clicking it, and then changing the name in\nthe Operator Name box in the Operator Inspector panel.  Connect the operators as shown in the diagram below. Note the following points about\nthe  FileWordCount  operator which has the largest number of connections:   The control port is connected to the control port of  LineReader .  The input port is connected to the output port of  WindowWordCount .  The  fileOutput  port emits the final top N pairs to the corresponding\n  output file and so is connected to the input port of  WordCountWriter .  The  outputGlobal  port emits the global top N pairs and so is connected to\n  the input port of  AppDataSnapshotMapServer .  The  outputPerFile  port emits the top N pairs for the current file (while\n  the file is still being read) and so it is connected to  ConsoleOutput  as\n  well as to  AppDataSnapshotMapServer .   Note : As you make changes, the top left corner displays  All\nchanges saved to HDFS . No explicit save step is needed.  After you connect all the operators, the canvas looks like this.\nAlthough presented differently, it is the same as the logical DAG for\nthe Java application.", 
            "title": "Step III: Connect the operators"
        }, 
        {
            "location": "/tutorials/topnwords-c3/#step-iv-configure-the-operator-properties", 
            "text": "The last step before running this application is to configure\nproperties of the operators.   Click the first operator ( LineReader ) in the canvas to see the list of\n   configurable properties in the right panel.   Locate the property labelled  Directory  and enter the path to the input\n   directory:  /tmp/test/input-dir :     Configure the properties of the remaining operators using this table for\n   reference. The table contains the same values that we set in the properties\n   file for the Java application.          Operator  Property Name  Value    5  File Path  /tmp/test/output-dir    2  Non Word Str  [\\p{Punct}\\s]+    9  Topic  TopNWordsQueryFile    10  Topic  TopNWordsQueryGlobal    11  Topic  TopNWordsQueryFileResult    12  Topic  TopNWordsQueryGlobalResult    7, 8  Snapshot Schema JSON  {  values : [{ name :  word ,  type :  string },  { name :  count ,  type :  integer }] }    4  Top N  10       Click Stream 8 and Stream 9, and change  Stream Locality  from\n   AUTOMATIC  to  CONTAINER_LOCAL  to match our earlier properties file.\n  After you perform this step, the line for the stream will change\n  from a solid line to a dotted line.    Click the blank area of the canvas to see the\n   application attributes in the right panel. Scroll to  Master Memory Mb , and\n   change its value to 500.    Click each operator, navigate to the  Memory Mb  attribute in the\n    Attributes  section, and change the value to 200 except for  Operator\n    4  for which the value is 512.    Click  launch  in the top-left corner.  Note : Before launching the\n   application, shut down the IDE; if it is running at the time of a launch,\n   the sandbox might hang due to resource exhaustion.\n      On the launch application dialog window, type a name for your application.\n      (Optional) To configure the application using a configuration file, select\n     Use a config file  checkbox. To specify individual properties, select\n     Specify custom properties  checkbox.   Click  Launch .   A transient pop-up at the top-right indicating that the launch was successful\nshould appear.  After a successful launch, monitor the application following\ninstructions in the Chapter entitled  Monitoring with dtManage .", 
            "title": "Step IV: Configure the operator properties"
        }, 
        {
            "location": "/tutorials/topnwords-c4/", 
            "text": "Monitoring top N words using dtManage\n\n\ndtManage\n is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.\n\n\nMonitor the Application\n\n\nTo monitor the top N words application\n\n\n\n\nLog on to the Datatorrent Console (the default username and password\n   are both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nUnder \nDatatorrent Applications\n, check if the application started.\n\n\nWait till the state entry changes to \nRUNNING\n.\n\n\nClick \nTopNWordsWithQueries\n to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and \nmetric-view\n.\n\n\nUnder \nStramEvents\n, ensure that all the operators have started.\n    \n\n\n\n\nDAGs and widgets\n\n\nWhen monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets: \nStram Events\n, \nApplication\nOverview\n, \nLogical DAG\n, \nLogical Operators\n, \nStreams\n, and \nMetrics Chart\n.\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).\n\n\nLogical view and associated widgets (panels)\n\n\nThis section describes the widgets that you see when you select the logical\ntab.\n\n\nStram Events\n\n\nAs shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled \ndetails\n\nappears next to the event; click on it for additional details about the\nfailure.\n\n\nApplication Overview\n\n\nThis panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The \nvisualize\n button allows you to\ncreate one or more custom dashboards to visualize the application output.\n\n\n\nLogical DAG\n\n\nThe logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.\n\n\nTo customize these properties\n\n\n\n\nClick an operator for which you want to display additional details.\n\n\nTo display a detail on the top of this operator, click the Top list and\n   select a metric.\n\n\nTo display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.\n\n\n\n\n\n\nLogical Operators\n\n\nThis panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.\n\n\n\n\nStreams\n\n\nThis panel displays details of each stream such as the name, locality, source,\nand sinks.\n\n\n\n\nMetrics Chart\n\n\nThis panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:\n\n\n\n\nPhysical view and associated widgets\n\n\nThe physical tab displays the \nApplication Overview\n and \nMetrics Chart\n\ndiscussed above along with additional panels: \nPhysical Operators\n and\n\nContainers\n. The \nPhysical Operators\n table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.\n\n\nPhysical Operators\n\n\n\n\nContainers\n\n\nFor each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the \nContainers\n table.\n\n\n\n\nIf the state of all the physical operators and containers is \nACTIVE\n\nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.\n\n\nThe physical-dag-view\n\n\nThe \nphysical-dag-view\n tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.\n\n\nThe metric-view\n\n\nThe metric-view tab displays only the \nMetrics Chart\n widget.\n\n\nView application logs\n\n\nWhen debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines. \ndtManage\n simplifies\nthis task by making all relevant logs accessible from the console.\n\n\nFor example, to examine logs for the \nFileWordCount\n operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.\n\n\nThe numeric values in the \ncontainer\n column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the \nContainer Overview\n panel, you should see a blue \nlogs\n dropdown\nbutton; click on it to see a menu containing three entries: \ndt.log\n, \nstderr\n,\nand \nstdout\n.\n\n\n\n\nAll messages output using \nlog4j\n classes will appear in \ndt.log\n\nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "Monitoring with dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitoring-top-n-words-using-dtmanage", 
            "text": "dtManage  is an invaluable tool for monitoring the state of a running\napplication as well as for troubleshooting problems.", 
            "title": "Monitoring top N words using dtManage"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#monitor-the-application", 
            "text": "To monitor the top N words application   Log on to the Datatorrent Console (the default username and password\n   are both  dtadmin ).  On the top navigation bar, click  Monitor .  Under  Datatorrent Applications , check if the application started.  Wait till the state entry changes to  RUNNING .  Click  TopNWordsWithQueries  to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and  metric-view .  Under  StramEvents , ensure that all the operators have started.", 
            "title": "Monitor the Application"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#dags-and-widgets", 
            "text": "When monitoring an application, the logical view is selected by default, with\nthe following six panels, also called widgets:  Stram Events ,  Application\nOverview ,  Logical DAG ,  Logical Operators ,  Streams , and  Metrics Chart .\nThese panels can be resized, moved around, configured (using the gear wheel\nicon in the top-right corner), or removed (using the delete button in the\ntop-right corner).  Logical view and associated widgets (panels)  This section describes the widgets that you see when you select the logical\ntab.  Stram Events  As shown in the screenshot above, this panel shows the lifecycle events of all\nthe operators. If one of the operators fails, a white button labelled  details \nappears next to the event; click on it for additional details about the\nfailure.  Application Overview  This panel displays application properties such as state, number of operators,\nallocated memory, and the number of tuples processed. You can use the kill\nbutton to terminate the application. The  visualize  button allows you to\ncreate one or more custom dashboards to visualize the application output.  Logical DAG  The logical DAG illustrates operators and their interconnections. You can\ncustomize the logical DAG view by selecting operator properties that are\ndisplayed above and below each operator.  To customize these properties   Click an operator for which you want to display additional details.  To display a detail on the top of this operator, click the Top list and\n   select a metric.  To display a detail at the bottom of this operator, click the Bottom list\n   and select a metric.    Logical Operators  This panel displays a table with detailed information about each operator such\nas its name, the associated JAVA class, the number of tuples processed, and\nthe number of tuples emitted.   Streams  This panel displays details of each stream such as the name, locality, source,\nand sinks.   Metrics Chart  This panel displays the number tuples processed and the number of bytes\nprocessed by some internal components. Since this application has not processed\nany tuples so far (no input file was provided), the green and blue lines\ncoincide with the horizontal axis:   Physical view and associated widgets  The physical tab displays the  Application Overview  and  Metrics Chart \ndiscussed above along with additional panels:  Physical Operators  and Containers . The  Physical Operators  table shows one row per physical\noperator. When partitioning is enabled, some operators can be replicated to\nachieve better resource utilization and hence better throughput so a single\nlogical operator may correspond to multiple physical operators.  Physical Operators   Containers  For each operator, a crucial piece of information is the process\n(the Java Virtual Machine) running that operator. It is also called a\ncontainer, and shown in a column with that name. Additional information about\nthe container (such as the host on which it is running) can be gleaned from the\nmatching row in the  Containers  table.   If the state of all the physical operators and containers is  ACTIVE \nand green   this is a healthy state. If the memory requirements for all the\noperators in the application exceeds the available memory in the cluster,\nyou'll see these status values changing continually from ACTIVE to PENDING.\nThis is an unhealthy state and, if it does not stabilize, your only option is\nto kill the application and reduce the memory needs or acquire more cluster\nresources.  The physical-dag-view  The  physical-dag-view  tab displays the Physical DAG widget, which\nshows all the partitioned copies of operators and their interconnections.  The metric-view  The metric-view tab displays only the  Metrics Chart  widget.", 
            "title": "DAGs and widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c4/#view-application-logs", 
            "text": "When debugging applications, we are often faced with the task of examining\nlog files. This can be cumbersome, especially in a distributed environment\nwhere logs can be scattered across multiple machines.  dtManage  simplifies\nthis task by making all relevant logs accessible from the console.  For example, to examine logs for the  FileWordCount  operator, go to the physical\ntab of the application monitoring page and check the physical operator table to\nfind the corresponding container. An example value might be 000010.  The numeric values in the  container  column are links that open a page\ncontaining a table of all physical operators running in that container.\nIn the  Container Overview  panel, you should see a blue  logs  dropdown\nbutton; click on it to see a menu containing three entries:  dt.log ,  stderr ,\nand  stdout .   All messages output using  log4j  classes will appear in  dt.log \nwhereas messages written directly to the standard error or standard\noutput streams will appear in the other two entries. Choose the entry\nyou want to view.", 
            "title": "View application logs"
        }, 
        {
            "location": "/tutorials/topnwords-c5/", 
            "text": "Visualizing the application output using dtDashboard\n\n\nThis chapter covers how to add input files to the monitored input directory and\nvisualize the output.\n\n\nWhen adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled \nFurther Explorations\n.\n\n\nStep 1: Add files to the monitored directory\n\n\nTo add the files to the monitored input directory\n\n\n\n\nLog on to the Datatorrent Console (the default username and password are\n   both \ndtadmin\n).\n\n\nOn the top navigation bar, click \nMonitor\n.\n\n\nClick TopNWordsWithQueries to see a page with four tabs: \nlogical\n,\n   \nphysical\n, \nphysical-dag-view\n, and  \nmetric-view\n.\n\n\nClick the \nlogical\n tab and make sure that the DAG is visible.\n\n\nCreate the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands:\nhdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir\n\n\n\n\n\n\n\nYou should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.\n\n\nYou can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:\n\n\nhdfs dfs -cat /tmp/test/output-dir/rfc4844.txt\n\n\n\nFor operating on these input and output directories, you may find the following\nshell aliases and functions useful:\n\n\nin=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}\n\n\n\nPut them in a file called, say, \naliases\n and read them into your shell with:\n\nsource aliases\n.\n\n\nThereafter, you can list contents of the input and output directories with\n\nls-input\n and \nls-output\n, remove all files from them with \nclean-input\n and\n\nclean-output\n, drop an input file \nfoo.txt\n into the input directory with\n\nput-file foo.txt\n and finally, retrieve the corresponding output file with\n\nget-file foo.txt\n.\n\n\nNote\n: When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.\n\n\nStep II: Visualize the results by generating dashboards\n\n\nTo generate dashboards\n\n\n\n\nPerform step I above.\n\n\nMake sure that the logical tab is selected and the \nApplication Overview\n\n  panel is visible.\n\n\n\n\nClick \nvisualize\n to see a dropdown containing previously created dashboards\n (if any), as well as the \ngenerate new dashboard\n entry.\n\n\n\n\n\n\nSelect the \ngenerate new dashboard\n entry.\n\n\nYou should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.\n\n\n\n\n\n\n\nAdd more files, one at a time, to the input directory as described in\n  step I above.\n\n\n\n\nObserve the charts changing to reflect the new data.\n\n\n\n\nYou can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.\n\n\nStep III: Add widgets\n\n\nTo derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets: \nbar chart\n,\n\npie chart\n, \nhorizontal bar chart\n, \ntable\n, and \nnote\n.\n\n\nTo add a widget\n\n\n\n\nGenerate a dashboard by following instructions of Step II above.\n\n\nClick the \nadd widget\n button below the name of the dashboard.\n\n\nIn the \nData Source\n list, select a data source for your widget.\n\n\n\n\nSelect a widget type under \nAvailable Widgets\n.\n\n\n\n\n\n\n\n\nClick \nadd widget\n.\n\n\n\n\n\n\nThe widget is added to your dashboard.\n\n\nStep IV: Configure a widget\n\n\nAfter you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.\n\n\nTo configure a widget\n\n\n\n\nTo change the size of the widget, click the border of the widget, and\n  resize it.\n\n\nTo move the widget around, click the widget, and drag it to the desired\n  position.\n\n\nTo change the title and other properties, click the \nedit\n button in the\n  top-right corner of the widget.\n    \n\n  You can now enter a new title in the \nTitle\n box or configure the rest of the\n  options in any suitable way.\n\n\nClick \nOK\n.\n\n\nTo remove a widget, click the delete button in the top-right corner.\n\n\n\n\nPerform additional tasks on dashboards\n\n\nAt any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.\n\n\nTo perform additional tasks\n\n\n\n\nEnsure that you generated a dashboard as described in Step II above and\n   select it.\n\n\nClick \nsettings\n button (next to buttons named \nadd widget\n,\n   \nauto generate\n, and \nsave settings\n), below the name of the dashboard to see the \nDashboard Settings\n dialog:\n    \n\n\nType a new name for the dashboard in the \nName of dashboard\n box.\n\n\nType a suitable description in the box below.\n\n\nMake sure that \nTopNWordsWithQueries\n is selected under \nChoose apps to\n    visualize\n.\n\n\nClick \nSave\n.\n\n\n\n\nDelete a dashboard\n\n\nYou can delete a dashboard at any time.\n\n\n\n\nLog on to the DataTorrent Console (default username and password are both\n  \ndtadmin\n)\n\n\nOn the top navigation bar, click \nVisualize\n.\n\n\n\n\nSelect a dashboard.\n\n\n\n\n\n\n\n\nClick delete.\n\n\n\n\n\n\n\n\nNote: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Visualizing with dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#visualizing-the-application-output-using-dtdashboard", 
            "text": "This chapter covers how to add input files to the monitored input directory and\nvisualize the output.  When adding files, it is important to add only one file at a time to the\nmonitored input directory; the application, as it stands, cannot handle\nsimultaneous addition of files at a time into the input directory. This\nissue is discussed in more detail in the Appendix entitled  Further Explorations .", 
            "title": "Visualizing the application output using dtDashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-1-add-files-to-the-monitored-directory", 
            "text": "To add the files to the monitored input directory   Log on to the Datatorrent Console (the default username and password are\n   both  dtadmin ).  On the top navigation bar, click  Monitor .  Click TopNWordsWithQueries to see a page with four tabs:  logical ,\n    physical ,  physical-dag-view , and   metric-view .  Click the  logical  tab and make sure that the DAG is visible.  Create the input and output directories in HDFS and drop a file into the\n   input directory by running the following commands: hdfs dfs -mkdir -p /tmp/test/input-dir\nhdfs dfs -mkdir -p /tmp/test/output-dir\nhdfs dfs -put ~/data/rfc4844.txt /tmp/test/input-dir    You should now see some numbers above and below some of the operators as the\nlines of the file are read and tuples start flowing through the DAG.  You can view the top 10 words and the frequencies for each input file by\nexamining the corresponding output file in the output directory, for example:  hdfs dfs -cat /tmp/test/output-dir/rfc4844.txt  For operating on these input and output directories, you may find the following\nshell aliases and functions useful:  in=/tmp/test/input-dir\nout=/tmp/test/output-dir\nalias ls-input=\"hdfs dfs -ls $in\"\nalias ls-output=\"hdfs dfs -ls $out\"\nalias clean-input=\"hdfs dfs -rm $in/\\*\"\nalias clean-output=\"hdfs dfs -rm $out/\\*\"\nfunction put-file ( ) {\n    hdfs dfs -put \"$1\" \"$in\"\n}\nfunction get-file ( ) {\n    hdfs dfs -get \"$out/$1\" \"$1\".out\n}  Put them in a file called, say,  aliases  and read them into your shell with: source aliases .  Thereafter, you can list contents of the input and output directories with ls-input  and  ls-output , remove all files from them with  clean-input  and clean-output , drop an input file  foo.txt  into the input directory with put-file foo.txt  and finally, retrieve the corresponding output file with get-file foo.txt .  Note : When you list files in the output directory, their sizes might show as\n0 but if you retrieve them with get-file or catenate them, the expected output\nwill be present.", 
            "title": "Step 1: Add files to the monitored directory"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-ii-visualize-the-results-by-generating-dashboards", 
            "text": "To generate dashboards   Perform step I above.  Make sure that the logical tab is selected and the  Application Overview \n  panel is visible.   Click  visualize  to see a dropdown containing previously created dashboards\n (if any), as well as the  generate new dashboard  entry.    Select the  generate new dashboard  entry.  You should now see panels with charts where one chart displays the data for\nthe current file and a second chart displays the cumulative global data\nacross all files processed so far.    Add more files, one at a time, to the input directory as described in\n  step I above.   Observe the charts changing to reflect the new data.   You can create multiple dashboards in this manner for visualizing the output\nfrom different applications or from the same application in different ways.", 
            "title": "Step II: Visualize the results by generating dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iii-add-widgets", 
            "text": "To derive more value out of application dashboards, you can add widgets to the\ndashboards. Widgets are charts in addition to the default charts that you can see on the dashboard. DataTorrent RTS Sandbox supports 5 widgets:  bar chart , pie chart ,  horizontal bar chart ,  table , and  note .  To add a widget   Generate a dashboard by following instructions of Step II above.  Click the  add widget  button below the name of the dashboard.  In the  Data Source  list, select a data source for your widget.   Select a widget type under  Available Widgets .     Click  add widget .    The widget is added to your dashboard.", 
            "title": "Step III: Add widgets"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#step-iv-configure-a-widget", 
            "text": "After you add a widget to your dashboard, you can configure it at any\ntime. Each widget has a title that appears in gray. If you hover over\nthe title, the pointer changes to a hand.  To configure a widget   To change the size of the widget, click the border of the widget, and\n  resize it.  To move the widget around, click the widget, and drag it to the desired\n  position.  To change the title and other properties, click the  edit  button in the\n  top-right corner of the widget.\n     \n  You can now enter a new title in the  Title  box or configure the rest of the\n  options in any suitable way.  Click  OK .  To remove a widget, click the delete button in the top-right corner.", 
            "title": "Step IV: Configure a widget"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#perform-additional-tasks-on-dashboards", 
            "text": "At any time, you can change the name and the description of a dashboard. You\ncan also delete dashboards.  To perform additional tasks   Ensure that you generated a dashboard as described in Step II above and\n   select it.  Click  settings  button (next to buttons named  add widget ,\n    auto generate , and  save settings ), below the name of the dashboard to see the  Dashboard Settings  dialog:\n      Type a new name for the dashboard in the  Name of dashboard  box.  Type a suitable description in the box below.  Make sure that  TopNWordsWithQueries  is selected under  Choose apps to\n    visualize .  Click  Save .", 
            "title": "Perform additional tasks on dashboards"
        }, 
        {
            "location": "/tutorials/topnwords-c5/#delete-a-dashboard", 
            "text": "You can delete a dashboard at any time.   Log on to the DataTorrent Console (default username and password are both\n   dtadmin )  On the top navigation bar, click  Visualize .   Select a dashboard.     Click delete.     Note: The delete button becomes visible only if one or more rows are selected.", 
            "title": "Delete a dashboard"
        }, 
        {
            "location": "/tutorials/topnwords-c6/", 
            "text": "Appendix\n\n\nOperators in Top N words application\n\n\nThis section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\n\n\nImplementing class\n\n\nDescription\n\n\n\n\n\n\nlineReader\n\n\nLineReader\n\n\nReads lines from input files.\n\n\n\n\n\n\nwordReader\n\n\nWordReader\n\n\nSplits a line into words.\n\n\n\n\n\n\nwindowWordCount\n\n\nWindowWordCount\n\n\nComputes word frequencies for a single window.\n\n\n\n\n\n\nfileWordCount\n\n\nFileWordCount\n\n\nMaintains per-file and global word frequencies.\n\n\n\n\n\n\nwcWriter\n\n\nWcWriter\n\n\nWrites top N words and their frequencies to output files.\n\n\n\n\n\n\nconsole\n\n\nConsoleOutputOperator\n\n\nWrites received tuples to console.\n\n\n\n\n\n\nsnapshotServerFile\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last data set for the current file, and returns it in response to queries.\n\n\n\n\n\n\nsnapshotServerGlobal\n\n\nAppDataSnapshotServerMap\n\n\nCaches the last global data set, and returns it in response to queries.\n\n\n\n\n\n\nQueryFile\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for per-file data.\n\n\n\n\n\n\nQueryGlobal\n\n\nPubSubWebSocketAppDataQuery\n\n\nReceives queries for global data.\n\n\n\n\n\n\nwsResultFile\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for per-file queries.\n\n\n\n\n\n\nwsResultGlobal\n\n\nPubSubWebSocketAppDataResult\n\n\nReturns results for global queries.\n\n\n\n\n\n\n\n\n\nWe now describe the process of wiring these operators together in the\n\npopulateDAG()\n method of the main application class\n\nApplicationWithQuerySupport\n. First, the operators are created and added to\nthe DAG via the \naddOperator\n method:\n\n\nLineReader lineReader = dag.addOperator(\nlineReader\n,new LineReader());\n\n\n\n\nThe first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.\n\n\nNext, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the \naddStream\n function, for example:\n\n\ndag.addStream(\nlines\n, lineReader.output, wordReader.input);\n...\ndag.addStream(\nWordCountsFile\n, fileWordCount.outputPerFile, snapshotServerFile.input, console.input);\n\n\n\n\nNotice that the stream from \nfileWordCount.outputPerFile\n (which consists of\nthe top N words for the current file as the file is being read) goes to\n\nsnapshotServerFile.input\n (where it will be saved to respond to queries) and to\n\nconsole.input\n (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.\n\n\nThis section provides detailed information about each operator.\n\n\nLineReader\n\n\nThis class extends \nAbstractFileInputOperator\nString\n to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.\n\n\nThe base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods: \nopenFile\n, \ncloseFile\n, \nreadEntity\n, and \nemit\n. Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the \nFileWordCount\n operator.\n\n\nWordReader\n\n\nThis operator receives lines from \nLineReader\n on the input port and emits\nwords on the output port. It has a configurable property called \nnonWordStr\n\nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property\n\ndt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr\n.\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe \nprocess\n method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.\n\n\nWindowWordCount\n\n\nThis operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when\n\nendWindow\n is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the\n\nendWindow\n call, but rather emit output tuples as each input tuple is\nprocessed.\n\n\nFileWordCount\n\n\nThis operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen \nLineReader\n reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next \nendWindow\n. The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we \ndo\n\nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing\n\nbeginWindow\n and \nendWindow\n calls by the upstream operator.\n\n\nThis operator also has three output ports: the \noutputPerFile\n port for the top\nN pairs for the current file as it is being read; the \noutputGlobal\n port for\nthe global top N pairs, and the \nfileOutput\n port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.\n\n\nFileWordCount\n also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.\n\n\nFileWordCount\n has a configurable property \ntopN\n for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name: \ndt.application.TopNWordsWithQueries.operator.fileWordCount.topN\n\n\nIn the \nendWindow\n call, both maps are passed to the \ngetTopNList\n function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.\n\n\nWordCountWriter\n\n\nThis operator extends \nAbstractFileOutputOperator\nMap\nString,Object\n, and\nsimply writes the final top N pairs to the output file. As with \nLineReader\n,\nmost of the complexity of \nWordCountWriter\n is hidden in the base class. You must\nprovide implementations for 3 methods: \nendWindow\n, \ngetFileName\n, and\n\ngetBytesForTuple\n. The first method calls the base class method \nrequestFinalize\n.\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The \ngetFileName\n\nmethod retrieves the file name from the tuple, and the \ngetBytesForTuple\n\nmethod converts the list of pairs to a string in the desired format.\n\n\nConsoleOutputOperator\n\n\nThis is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.\n\n\nAppDataSnapshotServerMap\n\n\nThis operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots \n  one for a per-file top N snapshot and one for a\nglobal snapshot.\n\n\nPubSubWebSocketAppDataQuery\n\n\nThis is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:\n\n\nws://gateway-host:port/pubsub\n\n\n\n\nwhere \ngateway-host\n and \nport\n should be replaced by appropriate values.\n\n\nA publisher sends a JSON message to the URL where the value of the \ndata\n key\nis the desired message content. The JSON might look like this:\n\n\n{\ntype\n:\npublish\n, \ntopic\n:\nfoobar\n, \ndata\n: ...}\n\n\n\n\nCorrespondingly, subscribers send messages like this to retrieve published\nmessage data:\n\n\n{\ntype\n:\nsubscribe\n, \ntopic\n:\nfoobar\n}\n\n\n\n\nTopic names need not be pre-registered anywhere but the same topic\nname (for example, \nfoobar\n in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.\n\n\nFor this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values \nTopNWordsQueryFile\n and \nTopNWordsQueryGlobal\n under the\nrespective names:\n\n\ndt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic\n\n\n\n\nPubSubWebSocketAppDataResult\n\n\nAnalogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values\n\nTopNWordsQueryFileResult\n and \nTopNWordsQueryGlobalResult\n corresponding to\nthe respective names:\n\n\ndt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic\n\n\n\n\nFurther Exploration\n\n\nIn this tutorial, the property values in the \nproperties.xml\n file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.\n\n\nAnother aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the \nFileWordCount\n operator gets an EOF on the\ncontrol port, it waits for an \nendWindow\n call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.\n\n\nDataTorrent terminology\n\n\nOperators\n\n\nOperators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.\n\n\nStreams\n\n\nA stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.\n\n\nPorts\n\n\nPorts are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.\n\n\nDirected Acyclic Graph (DAG)\n\n\nA DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.\n\n\nLogical Plan or DAG\n\n\nLogical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.\n\n\nPhysical Plan or DAG\n\n\nA physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.\n\n\nData Tuples Processed\n\n\nThis is the number of data objects processed by real-time stream processing\napplications.\n\n\nData Tuples Emitted\n\n\nThis is the number of data objects emitted after real-time stream processing\napplications complete processing operations.\n\n\nStreaming Application Manager (STRAM)\n\n\nStreaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.\n\n\nStreaming Window\n\n\nA streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.\n\n\nSliding Application Window\n\n\nSliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.\n\n\nDemo Applications\n\n\nThe real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.\n\n\nCommand-line Interface\n\n\nCommand line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.\n\n\nWeb services\n\n\nDataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#appendix", 
            "text": "", 
            "title": "Appendix"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#operators-in-top-n-words-application", 
            "text": "This section describes the operators used for building the top N\nwords application. The operators, the implementing classes and a brief description of their\nfunctionalities are described in this table.          Operator  Implementing class  Description    lineReader  LineReader  Reads lines from input files.    wordReader  WordReader  Splits a line into words.    windowWordCount  WindowWordCount  Computes word frequencies for a single window.    fileWordCount  FileWordCount  Maintains per-file and global word frequencies.    wcWriter  WcWriter  Writes top N words and their frequencies to output files.    console  ConsoleOutputOperator  Writes received tuples to console.    snapshotServerFile  AppDataSnapshotServerMap  Caches the last data set for the current file, and returns it in response to queries.    snapshotServerGlobal  AppDataSnapshotServerMap  Caches the last global data set, and returns it in response to queries.    QueryFile  PubSubWebSocketAppDataQuery  Receives queries for per-file data.    QueryGlobal  PubSubWebSocketAppDataQuery  Receives queries for global data.    wsResultFile  PubSubWebSocketAppDataResult  Returns results for per-file queries.    wsResultGlobal  PubSubWebSocketAppDataResult  Returns results for global queries.     We now describe the process of wiring these operators together in the populateDAG()  method of the main application class ApplicationWithQuerySupport . First, the operators are created and added to\nthe DAG via the  addOperator  method:  LineReader lineReader = dag.addOperator( lineReader ,new LineReader());  The first argument is a string that names this instance of the\noperator; it is the same as the value in the first column of the above\ntable and also the node name in the Logical DAG.  Next, we connect each output port of an operator with all the input ports that\nshould receive these tuples using the  addStream  function, for example:  dag.addStream( lines , lineReader.output, wordReader.input);\n...\ndag.addStream( WordCountsFile , fileWordCount.outputPerFile, snapshotServerFile.input, console.input);  Notice that the stream from  fileWordCount.outputPerFile  (which consists of\nthe top N words for the current file as the file is being read) goes to snapshotServerFile.input  (where it will be saved to respond to queries) and to console.input  (which is used for debugging). Additional sinks can be provided\nin the same call as additional terminal arguments. You can examine the rest of\nthese calls and ensure that they match the names and connections of the\nLogical DAG.  This section provides detailed information about each operator.  LineReader  This class extends  AbstractFileInputOperator String  to open a file, read its\nlines, and emit them as tuples. It has two output ports, one for the normal\noutput of tuples, and the other for the output of an EOF tuple indicating the\nend of the current input file. Ports should always be transient fields because\nthey should not be serialized and saved to the disk during checkpointing.  The base class keeps track of files already processed, files that\nshould be ignored, and files that failed part-way. Derived classes need to\noverride four methods:  openFile ,  closeFile ,  readEntity , and  emit . Of\nthese, only the third is non-trivial: if a valid line is available, it is read\nand returned. Otherwise, the end of the file must have been reached. To\nindicate this, the file name is emitted on the control port where it\nwill be read by the  FileWordCount  operator.  WordReader  This operator receives lines from  LineReader  on the input port and emits\nwords on the output port. It has a configurable property called  nonWordStr \nalong with associated public getter and setter methods. Such properties can be\ncustomized in the appropriate properties file of the application. The values of\nthe properties are automatically injected into the operator at run-time. In\nthis scenario, this string is provided the value of the property dt.application.TopNWordsWithQueries.operator.wordReader.nonWordStr .\nFor efficiency, this string is compiled into a pattern for repeated use.\nThe  process  method of the input port splits each input line into words using\nthis pattern as the separator, and emits non-empty words on the output port.  WindowWordCount  This operator receives words and emits a list of word-frequency pairs for each\nwindow. It maintains a word-frequency map for the current window, updates this\nmap for each word received, emits the whole map (if non-empty) when endWindow  is called, and clears the map in preparation for the next window.\nThis design pattern is appropriate because for normal text files, the number of\nwords received is far more than the size of the accumulated map. However, for\nsituations where data is emitted for each tuple, you should not wait till the endWindow  call, but rather emit output tuples as each input tuple is\nprocessed.  FileWordCount  This operator has two input ports, one for the per-window frequency maps it\ngets from the previous operator, and a control port to receive the file name\nwhen  LineReader  reaches the end of a file. When a file name is received on\nthe control port, it is saved and the final results for the file appear as\noutput at the next  endWindow . The reason for waiting is subtle: there is no\nguarantee of the relative order in which tuples arrive at two input ports;\nadditional input tuples from the same window can arrive at the input port\neven after the EOF was received on the control port. Note however that we  do \nhave a guarantee that tuples on the input port will arrive in exactly the same\norder in which they were emitted on the output port between the bracketing beginWindow  and  endWindow  calls by the upstream operator.  This operator also has three output ports: the  outputPerFile  port for the top\nN pairs for the current file as it is being read; the  outputGlobal  port for\nthe global top N pairs, and the  fileOutput  port for the final top N pairs for\nthe current file computed after receiving the EOF control tuple. The output\nfrom the first is sent to the per-file snapshot server, the output from\nthe second is sent to the global snapshot server, and the output from the last\nis sent to the operator that writes results to the output file.  FileWordCount  also maintains two maps for per-file and global frequency\ncounts because they track frequencies of all words seen so far. These maps\ncan get very large as more and more files are processed.  FileWordCount  has a configurable property  topN  for the number of top pairs we\nare interested in. This was configured in our properties file with a value of\n10 and the property name:  dt.application.TopNWordsWithQueries.operator.fileWordCount.topN  In the  endWindow  call, both maps are passed to the  getTopNList  function\nwhere they are flattened, sorted in descending order of frequency, stripped of\nall but the top N pairs, and returned for output. There are a couple of\nadditional fields used to cast the output into the somewhat peculiar form\nrequired by the snapshot server.  WordCountWriter  This operator extends  AbstractFileOutputOperator Map String,Object , and\nsimply writes the final top N pairs to the output file. As with  LineReader ,\nmost of the complexity of  WordCountWriter  is hidden in the base class. You must\nprovide implementations for 3 methods:  endWindow ,  getFileName , and getBytesForTuple . The first method calls the base class method  requestFinalize .\nThe output file is written periodically to temporary files\nwith a synthetic file name that includes a timestamp. These files are removed\nand the actual desired file name is restored by this call. The  getFileName \nmethod retrieves the file name from the tuple, and the  getBytesForTuple \nmethod converts the list of pairs to a string in the desired format.  ConsoleOutputOperator  This is an output operator that is a part of the Malhar library. It simply\nwrites incoming tuples to the console and is useful when debugging.  AppDataSnapshotServerMap  This operator is also part of the Malhar library and is used to store snapshots\nof data. These snapshots are used to respond to queries. For this application,\nwe use two snapshots    one for a per-file top N snapshot and one for a\nglobal snapshot.  PubSubWebSocketAppDataQuery  This is an input operator that is a part of the Malhar library. It is used to\nsend queries to an operator via the Data Torrent Gateway, which can act as a\nmessage broker for limited amounts of data using a topic-based\npublish-subscribe model. The URL to connect is typically something like:  ws://gateway-host:port/pubsub  where  gateway-host  and  port  should be replaced by appropriate values.  A publisher sends a JSON message to the URL where the value of the  data  key\nis the desired message content. The JSON might look like this:  { type : publish ,  topic : foobar ,  data : ...}  Correspondingly, subscribers send messages like this to retrieve published\nmessage data:  { type : subscribe ,  topic : foobar }  Topic names need not be pre-registered anywhere but the same topic\nname (for example,  foobar  in the example) must be used by both publisher and\nsubscriber. Additionally, if there are no subscribers when a message is\npublished, it is simply discarded.  For this tutorial, two query operators are used: one for per-file queries and\none for global queries. The topic names were configured in the properties file\nearlier with values  TopNWordsQueryFile  and  TopNWordsQueryGlobal  under the\nrespective names:  dt.application.TopNWordsWithQueries.operator.QueryFile.topic\ndt.application.TopNWordsWithQueries.operator.QueryGlobal.topic  PubSubWebSocketAppDataResult  Analogous to the previous operator, this is an output operator used to publish\nquery results to a gateway topic. You must use two of these to match the query\noperators, and configure their topics in the properties file with values TopNWordsQueryFileResult  and  TopNWordsQueryGlobalResult  corresponding to\nthe respective names:  dt.application.TopNWordsWithQueries.operator.wsResultFile.topic\ndt.application.TopNWordsWithQueries.operator.wsResultGlobal.topic", 
            "title": "Operators in Top N words application"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#further-exploration", 
            "text": "In this tutorial, the property values in the  properties.xml  file were set to\nlimit the amount of memory allocated to each operator. You can try varying\nthese values and checking the impact of such an operation on the stability and\nperformance of the application. You can also explore the largest text\nfile that the application can handle.  Another aspect to explore is fixing the current limitation of\none-file-at-a-time processing; if multiple files are dropped into the\ninput directory simultaneously, the file reader can switch from one file to the\nnext in the same window. When the  FileWordCount  operator gets an EOF on the\ncontrol port, it waits for an  endWindow  call to emit word counts so those\ncounts will be incorrect if tuples from two different files arrive in the same\nwindow. Try fixing this issue.", 
            "title": "Further Exploration"
        }, 
        {
            "location": "/tutorials/topnwords-c6/#datatorrent-terminology", 
            "text": "Operators  Operators are basic computation units that have properties and\nattributes, and are interconnected via streams to form an application.\nProperties customize the functional definition of the operator, while\nattributes customize the operational behavior. You can think of\noperators as classes for implementing the operator interface. They read\nfrom incoming streams of tuples and write to other streams.  Streams  A stream is a connector (edge) abstraction which is a fundamental building\nblock of DataTorrent RTS. A stream consists of tuples that flow from one input\nport to one or more output ports.  Ports  Ports are transient objects declared in the operator class and act connection\npoints for operators. Tuples flow in and out through ports. Input ports read\nfrom streams while output port write to streams.  Directed Acyclic Graph (DAG)  A DAG is a logical representation of real-time stream processing application.\nThe computational units within a DAG are called operators and the data-flow\nedges are called data streams.  Logical Plan or DAG  Logical Plan is the Data Object Model (DOM) that is created as operators and\nstreams are added to the DAG. It is identical to a DAG.  Physical Plan or DAG  A physical plan is the physical representation of the logical plan of the\napplication, which depicts how applications run on physical containers and\nnodes of a DataTorrent cluster.  Data Tuples Processed  This is the number of data objects processed by real-time stream processing\napplications.  Data Tuples Emitted  This is the number of data objects emitted after real-time stream processing\napplications complete processing operations.  Streaming Application Manager (STRAM)  Streaming Application Manager (STRAM) is a YARN-native, lightweight controller\nprocess. It is the process that is activated first upon application launch to\norchestrate the streaming application.  Streaming Window  A streaming window is a duration during which a set of tuples are emitted. The\ncollection of these tuples constitutes a window data set, also called as an\natomic micro-batch.  Sliding Application Window  Sliding window is computation that requires \"n\" streaming windows. After each\nstreaming window, the nth window is dropped, and the new window is added to the\ncomputation.  Demo Applications  The real-time stream processing applications which are packaged with the\nDataTorrent RTS binaries, are called demo applications. A Demo application can\nbe launched standalone, or on a Hadoop cluster.  Command-line Interface  Command line interface (CLI) is the access point for applications.\nThis is a wrapper around the web services layer, which makes the web\nservices user friendly.  Web services  DataTorrent RTS platform provides a robust webservices layer called\nDT Gateway. Currently, Hadoop provides detailed web services for\nmap-reduce jobs. The DataTorrent RTS platform leverages the same\nframework to provide a web service interface for real-time streaming\napplications.", 
            "title": "DataTorrent terminology"
        }, 
        {
            "location": "/tutorials/sales_dimensions/", 
            "text": "Sales Dimensions - Transform, Analyze and Alert\n\n\nSales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.\n\n\nA large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.\n\n\nIn order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.\n\n\nApplication setup will require following steps:\n\n\n\n\nInput\n - receive individual sales transactions\n\n\nTransform\n - convert incoming records into consumable form\n\n\nEnrich\n - provide additional information for each record by performing additional lookups\n\n\nCompute\n - perform aggregate computations on all possible key field combinations\n\n\nStore\n - store computed results for further analysis and visualizations\n\n\nAnalyze\n, Alert \n Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.\n\n\n\n\nCreate New Application\n\n\nDataTorrent platforms supports building new applications with \nGraphical Application Builder\n, which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.\n\n\nGo to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.\n\n\nThis will bring up the Application Builder interface\n\n\n\n\nAdd and Connect Operators\n\n\nFrom the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.\n\n\n\n\nJSON Sales Event Generator (Input)\n - generates synthetic sales events and emits them as JSON string bytes.\n\n\nJSON to Map Parser (Parse)\n - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.\n\n\nEnrichment (Enrich)\n - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.\n\n\nDimension Computation Map (Compute)\n -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.\n\n\nSimple App Data Dimensions Store (Store)\n - stores the computed dimensional information on HDFS in an optimized manner.\n\n\nApp Data Pub Sub Query (Query)\n - dashboard connector for visualization queries.\n\n\nApp Data Pub Sub Result (Result)\n - dashboard connector for visualization data results.\n\n\n\n\nConnect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.\n\n\n\n\nCustomize Application and Operator Settings\n\n\nBy clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:\n\n\n\n\n\n\nCopy the Sales schema below and paste the contents into the \nEvent Schema JSON\n field of \nInput\n operator, and \nConfiguration Schema JSON\n of the \nCompute\n and \nStore\n operators.\n\n\n{\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}\n\n\n\n\n\n\n\nSet the \nTopic\n property for \nQuery\n and \nResult\n operators to \nSalesDimensionsQuery\n and \nSalesDimensionsResult\n respectively.\n\n\n\n\n\n\nSelect the \nStore\n operator, and edit the \nFile Store\n property.  Set the \nBase Path\n to \nSalesDimensionsDemoStore\n value.  This sets the HDHT storage path to write dimensions computation results to the \n/user/\nusername\n/SalesDimensionsDemoStore\n on HDFS.\n\n\n\n\n\n\n\n\nClick on the stream and set the \nStream Locality\n to \nCONTAINER_LOCAL\n for all the streams between \nInput\n and \nCompute\n operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.\n\n\n\n\n\n\nLaunch Application\n\n\nOnce application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it.\n\n\n\n\nAfter launching, go to \nSales Dimensions\n application operations page in the \nMonitor\n section.\n\n\n\n\nConfirm that the application is launched successfully by looking for \nRunning\n state in the \nApplication Overview\n section, and all confirming all the operators are successfully started under \nStram Events\n section.  By navigating to \nPhysical\n view tab, and looking at \nInput\n, \nParse\n, \nEnrich\n, or \nCompute\n operators, notice that they are all deployed to the single container, thanks to the stream locality setting of \nCONTAINER_LOCAL\n we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.\n\n\n\n\nVisualize Data\n\n\nDataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see \nData Visualization\n tutorial.\n\n\nAfter application is started, a \nvisualize\n button, available in the \nApplication Overview\n section, can be used to quickly generate a new dashboard for the Sales Dimensions application.\n\n\n\n\nOnce dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Sales Dimenions"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#sales-dimensions-transform-analyze-and-alert", 
            "text": "Sales Dimensions application demonstrates multiple features of the DataTorrent platform, including ability to transform, analyze, and take actions on data in real time.  The application also demonstrates how DataTorrent platform is can be used to build scalable applications for high volume multi-dimensional computations with very low latency using existing library operators.  A large national retailer with physical stores and online sales channels is trying to gain better insights and improve decision making for their business.  By utilising real time sales data they would like to detect and forecast customer demand across multiple product categories, gage pricing and promotional effectiveness across regions, and drive additional customer loyalty with real time cross purchase promotions.  In order to achieve these goals, they need to be able to analyze large volumes of transactions in real time by computing aggregations of sales data across multiple dimensions, including retail channels, product categories, and regions.  This allows them to not only gain insights by visualizing the data for any dimension, but also make decisions and take actions on the data in real time.  Application setup will require following steps:   Input  - receive individual sales transactions  Transform  - convert incoming records into consumable form  Enrich  - provide additional information for each record by performing additional lookups  Compute  - perform aggregate computations on all possible key field combinations  Store  - store computed results for further analysis and visualizations  Analyze , Alert   Visualize - display graphs for selected combinations, perform analysis, and take actions on computed data in real time.", 
            "title": "Sales Dimensions - Transform, Analyze and Alert"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#create-new-application", 
            "text": "DataTorrent platforms supports building new applications with  Graphical Application Builder , which will be used for Sales Dimensions demo.  App Builder is an easy and intuitive way to construct your applications, which provides a great visualization of the logical operator connectivity and application data flow.  Go to App Packages section of the Console and make sure DataTorrent Dimensions Demos package is imported.  Use Import Demos button to add it, if not already present.\nClick create new application, and name the application \u201cSales Dimensions\u201d.  This will bring up the Application Builder interface", 
            "title": "Create New Application"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#add-and-connect-operators", 
            "text": "From the Operator Library section on the App Builder screen, select the following operators and drag them to the Application Canvas.  Rename them to the names in parenthesis.   JSON Sales Event Generator (Input)  - generates synthetic sales events and emits them as JSON string bytes.  JSON to Map Parser (Parse)  - transforms JSON data to Java maps, which provides a way to easily transform and analyze the sales data.  Enrichment (Enrich)  - performs category lookup based on incoming product IDs, and adds the category ID to the output maps.  Dimension Computation Map (Compute)  -  performs dimensions computations, also known as cubing, on the incoming data.  This pre-computes the sales numbers by region, product category, customer, and sales channel, and all combinations of the above.  Having these numbers available in advance, allows for viewing and taking action on any of these combinations in real time.  Simple App Data Dimensions Store (Store)  - stores the computed dimensional information on HDFS in an optimized manner.  App Data Pub Sub Query (Query)  - dashboard connector for visualization queries.  App Data Pub Sub Result (Result)  - dashboard connector for visualization data results.   Connect all the operators together by clicking on the output port of the upstream operator, dragging connection, and connecting to the input port of the downstream operator.  Use the example below for layout and connectivity reference.", 
            "title": "Add and Connect Operators"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#customize-application-and-operator-settings", 
            "text": "By clicking on the individual operators or streams connecting them, and using Operator Inspector panel on the right, edit the operator and stream settings as follows:    Copy the Sales schema below and paste the contents into the  Event Schema JSON  field of  Input  operator, and  Configuration Schema JSON  of the  Compute  and  Store  operators.  {\n  \"keys\":[{\"name\":\"channel\",\"type\":\"string\",\"enumValues\":[\"Mobile\",\"Online\",\"Store\"]},\n          {\"name\":\"region\",\"type\":\"string\",\"enumValues\":[\"Atlanta\",\"Boston\",\"Chicago\",\"Cleveland\",\"Dallas\",\"Minneapolis\",\"New York\",\"Philadelphia\",\"San Francisco\",\"St. Louis\"]},\n          {\"name\":\"product\",\"type\":\"string\",\"enumValues\":[\"Laptops\",\"Printers\",\"Routers\",\"Smart Phones\",\"Tablets\"]}],\n \"timeBuckets\":[\"1m\", \"1h\", \"1d\"],\n \"values\":\n  [{\"name\":\"sales\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"discount\",\"type\":\"double\",\"aggregators\":[\"SUM\"]},\n   {\"name\":\"tax\",\"type\":\"double\",\"aggregators\":[\"SUM\"]}],\n \"dimensions\":\n  [{\"combination\":[]},\n   {\"combination\":[\"channel\"]},\n   {\"combination\":[\"region\"]},\n   {\"combination\":[\"product\"]},\n   {\"combination\":[\"channel\",\"region\"]},\n   {\"combination\":[\"channel\",\"product\"]},\n   {\"combination\":[\"region\",\"product\"]},\n   {\"combination\":[\"channel\",\"region\",\"product\"]}]\n}    Set the  Topic  property for  Query  and  Result  operators to  SalesDimensionsQuery  and  SalesDimensionsResult  respectively.    Select the  Store  operator, and edit the  File Store  property.  Set the  Base Path  to  SalesDimensionsDemoStore  value.  This sets the HDHT storage path to write dimensions computation results to the  /user/ username /SalesDimensionsDemoStore  on HDFS.     Click on the stream and set the  Stream Locality  to  CONTAINER_LOCAL  for all the streams between  Input  and  Compute  operators.  Changing stream locality controls which container operators get deployed to, and can lead to significant performance improvements for an application.  Once set, connection will be represented by a dashed line to indicate the new locality setting.", 
            "title": "Customize Application and Operator Settings"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#launch-application", 
            "text": "Once application is constructed, and validation checks are satisfied, a launch button will become available at the top left of the Application Canvas screen.  Clicking it will bring up the application launch dialog, which allows you to further configure the application by changing its name and configuration settings prior to starting it.   After launching, go to  Sales Dimensions  application operations page in the  Monitor  section.   Confirm that the application is launched successfully by looking for  Running  state in the  Application Overview  section, and all confirming all the operators are successfully started under  Stram Events  section.  By navigating to  Physical  view tab, and looking at  Input ,  Parse ,  Enrich , or  Compute  operators, notice that they are all deployed to the single container, thanks to the stream locality setting of  CONTAINER_LOCAL  we applied earlier.  This represents one of the many performance improvement techniques available with the DataTorrent platform, in this case eliminating data serialization and networking stack overhead between a group of adjacent operators.", 
            "title": "Launch Application"
        }, 
        {
            "location": "/tutorials/sales_dimensions/#visualize-data", 
            "text": "DataTorrent includes powerful data visualization tools, which allow you to visualize streaming data from multiple sources in real time.  For additional details see  Data Visualization  tutorial.  After application is started, a  visualize  button, available in the  Application Overview  section, can be used to quickly generate a new dashboard for the Sales Dimensions application.   Once dashboard is created, additional widgets can be added to display various dimensions and combinations of the sales data.  Below is an example of multiple sales combinations displayed in real time.", 
            "title": "Visualize Data"
        }, 
        {
            "location": "/apex_development_setup/", 
            "text": "Apache Apex Development Environment Setup\n\n\nThis document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.\n\n\nMicrosoft Windows\n\n\nThere are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:\n\n\n\n\n\n\ngit\n -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows (\nhttp://git-scm.com/download/win\n for example), so download and install a client of your choice.\n\n\n\n\n\n\njava JDK\n (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.\n\n\n\n\n\n\nmaven\n -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from \nhttps://maven.apache.org/download.cgi\n.\n\n\n\n\n\n\nVirtualBox\n -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from \nhttps://www.virtualbox.org/wiki/Downloads\n. It is needed to run the Data Torrent Sandbox.\n\n\n\n\n\n\nDataTorrent Sandbox\n -- The sandbox can be downloaded from \nhttps://www.datatorrent.com/download\n. It is useful for testing simple applications since it contains Apache Hadoop and Data Torrent RTS 3.1.1 pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.\n\n\n\n\n\n\n(Optional) If you prefer to use an IDE (Integrated Development Environment) such as \nNetBeans\n, \nEclipse\n or \nIntelliJ\n, install that as well.\n\n\n\n\n\n\nAfter installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like \njava\n and \njavac\n, the directory might be something like \nC:\\\\Program Files\\\\Java\\\\jdk1.7.0\\_80\\\\bin\n; for \ngit\n it might be \nC:\\\\Program Files\\\\Git\\\\bin\n; and for maven it might be \nC:\\\\Users\\\\user\\\\Software\\\\apache-maven-3.3.3\\\\bin\n. Open a console window and enter the command:\n\n\necho %PATH%\n\n\n\nto see the value of the \nPATH\n variable and verify that the above directories are present. If not, you can change its value clicking on the button at \nControl Panel\n \n \nAdvanced System Settings\n \n \nAdvanced tab\n \n \nEnvironment Variables\n.\n\n\nNow run the following commands and ensure that the output is something similar to that shown in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\nOutput\n\n\n\n\n\n\njavac -version\n\n\njavac 1.7.0_80\n\n\n\n\n\n\njava -version\n\n\njava version \n1.7.0_80\n\n\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\n\n\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n\n\n\n\n\ngit --version\n\n\ngit version 2.6.1.windows.1\n\n\n\n\n\n\nmvn --version\n\n\nApache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)\n\n\nMaven home: C:\\Users\\ram\\Software\\apache-maven-3.3.3\\bin\\..\n\n\nJava version: 1.7.0_80, vendor: Oracle Corporation\n\n\nJava home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre\n\n\nDefault locale: en_US, platform encoding: Cp1252\n\n\nOS name: \nwindows 8\n, version: \n6.2\n, arch: \namd64\n, family: \nwindows\n\n\n\n\n\n\n\n\n\nTo install the sandbox, first download it from \nhttps://www.datatorrent.com/download\n and import the downloaded file into VirtualBox. Once the import completes, you can select it and click the  Start button to start the sandbox.\n\n\nThe sandbox is configured with 6GB RAM; if your development machine has 16GB or more, you can increase the sandbox RAM to 8GB or more using the VirtualBox console. This will yield better performance and support larger applications. Additionally, you can change the network adapter from \nNAT\n to \nBridged Adapter\n; this will allow you to login to the sandbox from your host machine using an \nssh\n tool like \nPuTTY\n and also to transfer files to and from the host using \npscp\n on Windows. Of course all such configuration must be done when when the sandbox is not running.\n\n\nYou can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g. \njdk\n, \ngit\n, \nmaven\n) are pre-installed and also the package files created by your project are directly available to the Data Torrent tools such as  \ndtManage\n and \ndtcli\n. The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.\n\n\nYou can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example, \nnewapp.cmd\n and run it:\n\n\n@echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases ^\n  -DarchetypeGroupId=com.datatorrent ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.1.1 ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal\n\n\n\nThe caret (^) at the end of some lines indicates that a continuation line follows. When you run this file, the properties will be displayed and you will be prompted with \nY: :\n; just press \nEnter\n to complete the project generation.\n\n\nThis command file also exists in the Data Torrent \nexamples\n repository which you can check out with:\n\n\ngit clone https://github.com/DataTorrent/examples\n\n\n\nYou will find the script under \nexamples\\tutorials\\topnwords\\scripts\\newapp.cmd\n.\n\n\nYou can also, if you prefer, use an IDE to generate the project as described in Section 3 of \nApplication Packages\n but use the archetype version 3.1.1 instead of 3.0.0.\n\n\nWhen the run completes successfully, you should see a new directory named \nmyapexapp\n containing a maven project for building a basic Apache Apex application. It includes 3 source files:\nApplication.java\n,  \nRandomNumberGenerator.java\n and \nApplicationTest.java\n. You can now build the application by stepping into the new directory and running the appropriate maven command:\n\n\ncd myapexapp\nmvn clean package -DskipTests\n\n\n\nThe build should create the application package file \nmyapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa\n. This file can then be uploaded to the Data Torrent GUI tool on the sandbox (called \ndtManage\n) and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string  \nhello world:\n.  If you built this package on the host, you can transfer it to the sandbox using the \npscp\n tool bundled with \nPuTTY\n mentioned earlier.\n\n\nIf you want to checkout the Apache Apex source repositories and build them, you can do so by running the script \nbuild-apex.cmd\n located in the same place in the examples repository described above. The source repositories contain more substantial demo applications and the associated source code. Alternatively, if you do not want to use the script, you can follow these simple manual steps:\n\n\n\n\n\n\nCheck out the source code repositories:\n\n\ngit clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar\n\n\n\n\n\n\n\nSwitch to the appropriate release branch and build each repository:\n\n\npushd incubator-apex-core\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\n\n\n\n\n\n\n\nThe \ninstall\n argument to the \nmvn\n command installs resources from each project to your local maven repository (typically \n.m2/repository\n under your home directory), and \nnot\n to the system directories, so Administrator privileges are not required. The  \n-DskipTests\n argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.\n\n\nAfter the build completes, you should see the demo application package files in the target directory under each demo subdirectory in \nincubator-apex-malhar\\demos\\\n.\n\n\nLinux\n\n\nMost of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.\n\n\nThe pre-requisites (such as \ngit\n, \nmaven\n, etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is: \necho $PATH\n).\n\n\nThe maven archetype command is the same except that continuation lines use a backslash (\n\\\n) instead of caret (\n^\n); the script for it is available in the same location and is named \nnewapp\n (without the \n.cmd\n extension). The script to checkout and build the Apache Apex repositories is named \nbuild-apex\n.", 
            "title": "Apex Development Setup"
        }, 
        {
            "location": "/apex_development_setup/#apache-apex-development-environment-setup", 
            "text": "This document discusses the steps needed for setting up a development environment for creating applications that run on the Apache Apex or the DataTorrent RTS streaming platform.", 
            "title": "Apache Apex Development Environment Setup"
        }, 
        {
            "location": "/apex_development_setup/#microsoft-windows", 
            "text": "There are a few tools that will be helpful when developing Apache Apex applications, some required and some optional:    git  -- A revision control system (version 1.7.1 or later). There are multiple git clients available for Windows ( http://git-scm.com/download/win  for example), so download and install a client of your choice.    java JDK  (not JRE). Includes the Java Runtime Environment as well as the Java compiler and a variety of tools (version 1.7.0_79 or later). Can be downloaded from the Oracle website.    maven  -- Apache Maven is a build system for Java projects (version 3.0.5 or later). It can be downloaded from  https://maven.apache.org/download.cgi .    VirtualBox  -- Oracle VirtualBox is a virtual machine manager (version 4.3 or later) and can be downloaded from  https://www.virtualbox.org/wiki/Downloads . It is needed to run the Data Torrent Sandbox.    DataTorrent Sandbox  -- The sandbox can be downloaded from  https://www.datatorrent.com/download . It is useful for testing simple applications since it contains Apache Hadoop and Data Torrent RTS 3.1.1 pre-installed with a time-limited Enterprise License. If you already installed the RTS Enterprise Edition (evaluation or production license) on a cluster, you can use that setup for deployment and testing instead of the sandbox.    (Optional) If you prefer to use an IDE (Integrated Development Environment) such as  NetBeans ,  Eclipse  or  IntelliJ , install that as well.    After installing these tools, make sure that the directories containing the executable files are in your PATH environment; for example, for the JDK executables like  java  and  javac , the directory might be something like  C:\\\\Program Files\\\\Java\\\\jdk1.7.0\\_80\\\\bin ; for  git  it might be  C:\\\\Program Files\\\\Git\\\\bin ; and for maven it might be  C:\\\\Users\\\\user\\\\Software\\\\apache-maven-3.3.3\\\\bin . Open a console window and enter the command:  echo %PATH%  to see the value of the  PATH  variable and verify that the above directories are present. If not, you can change its value clicking on the button at  Control Panel     Advanced System Settings     Advanced tab     Environment Variables .  Now run the following commands and ensure that the output is something similar to that shown in the table below:         Command  Output    javac -version  javac 1.7.0_80    java -version  java version  1.7.0_80  Java(TM) SE Runtime Environment (build 1.7.0_80-b15)  Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)    git --version  git version 2.6.1.windows.1    mvn --version  Apache Maven 3.3.3 (7994120775791599e205a5524ec3e0dfe41d4a06; 2015-04-22T06:57:37-05:00)  Maven home: C:\\Users\\ram\\Software\\apache-maven-3.3.3\\bin\\..  Java version: 1.7.0_80, vendor: Oracle Corporation  Java home: C:\\Program Files\\Java\\jdk1.7.0_80\\jre  Default locale: en_US, platform encoding: Cp1252  OS name:  windows 8 , version:  6.2 , arch:  amd64 , family:  windows     To install the sandbox, first download it from  https://www.datatorrent.com/download  and import the downloaded file into VirtualBox. Once the import completes, you can select it and click the  Start button to start the sandbox.  The sandbox is configured with 6GB RAM; if your development machine has 16GB or more, you can increase the sandbox RAM to 8GB or more using the VirtualBox console. This will yield better performance and support larger applications. Additionally, you can change the network adapter from  NAT  to  Bridged Adapter ; this will allow you to login to the sandbox from your host machine using an  ssh  tool like  PuTTY  and also to transfer files to and from the host using  pscp  on Windows. Of course all such configuration must be done when when the sandbox is not running.  You can choose to develop either directly on the sandbox or on your development machine. The advantage of the former is that most of the tools (e.g.  jdk ,  git ,  maven ) are pre-installed and also the package files created by your project are directly available to the Data Torrent tools such as   dtManage  and  dtcli . The disadvantage is that the sandbox is a memory-limited environment so running a memory-hungry tool like a Java IDE on it may starve other applications of memory.  You can now use the maven archetype to create a basic Apache Apex project as follows: Put these lines in a Windows command file called, for example,  newapp.cmd  and run it:  @echo off\n@rem Script for creating a new application\nsetlocal\nmvn archetype:generate ^\n-DarchetypeRepository=https://www.datatorrent.com/maven/content/repositories/releases ^\n  -DarchetypeGroupId=com.datatorrent ^\n  -DarchetypeArtifactId=apex-app-archetype ^\n  -DarchetypeVersion=3.1.1 ^\n  -DgroupId=com.example ^\n  -Dpackage=com.example.myapexapp ^\n  -DartifactId=myapexapp ^\n  -Dversion=1.0-SNAPSHOT\nendlocal  The caret (^) at the end of some lines indicates that a continuation line follows. When you run this file, the properties will be displayed and you will be prompted with  Y: : ; just press  Enter  to complete the project generation.  This command file also exists in the Data Torrent  examples  repository which you can check out with:  git clone https://github.com/DataTorrent/examples  You will find the script under  examples\\tutorials\\topnwords\\scripts\\newapp.cmd .  You can also, if you prefer, use an IDE to generate the project as described in Section 3 of  Application Packages  but use the archetype version 3.1.1 instead of 3.0.0.  When the run completes successfully, you should see a new directory named  myapexapp  containing a maven project for building a basic Apache Apex application. It includes 3 source files: Application.java ,   RandomNumberGenerator.java  and  ApplicationTest.java . You can now build the application by stepping into the new directory and running the appropriate maven command:  cd myapexapp\nmvn clean package -DskipTests  The build should create the application package file  myapexapp\\target\\myapexapp-1.0-SNAPSHOT.apa . This file can then be uploaded to the Data Torrent GUI tool on the sandbox (called  dtManage ) and launched  from there. It generates a stream of random numbers and prints them out, each prefixed by the string   hello world: .  If you built this package on the host, you can transfer it to the sandbox using the  pscp  tool bundled with  PuTTY  mentioned earlier.  If you want to checkout the Apache Apex source repositories and build them, you can do so by running the script  build-apex.cmd  located in the same place in the examples repository described above. The source repositories contain more substantial demo applications and the associated source code. Alternatively, if you do not want to use the script, you can follow these simple manual steps:    Check out the source code repositories:  git clone https://github.com/apache/incubator-apex-core\ngit clone https://github.com/apache/incubator-apex-malhar    Switch to the appropriate release branch and build each repository:  pushd incubator-apex-core\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd\npushd incubator-apex-malhar\ngit checkout release-3.1\nmvn clean install -DskipTests\npopd    The  install  argument to the  mvn  command installs resources from each project to your local maven repository (typically  .m2/repository  under your home directory), and  not  to the system directories, so Administrator privileges are not required. The   -DskipTests  argument skips running unit tests since they take a long time. If this is a first-time installation, it might take several minutes to complete because maven will download a number of associated plugins.  After the build completes, you should see the demo application package files in the target directory under each demo subdirectory in  incubator-apex-malhar\\demos\\ .", 
            "title": "Microsoft Windows"
        }, 
        {
            "location": "/apex_development_setup/#linux", 
            "text": "Most of the instructions for Linux (and other Unix-like systems) are similar to those for Windows described above, so we will just note the differences.  The pre-requisites (such as  git ,  maven , etc.) are the same as for Windows described above; please run the commands in the table and ensure that appropriate versions are present in your PATH environment variable (the command to display that variable is:  echo $PATH ).  The maven archetype command is the same except that continuation lines use a backslash ( \\ ) instead of caret ( ^ ); the script for it is available in the same location and is named  newapp  (without the  .cmd  extension). The script to checkout and build the Apache Apex repositories is named  build-apex .", 
            "title": "Linux"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Applications"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/application_packages/", 
            "text": "Apache Apex Application Packages\n\n\nAn Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the App Package)\n\n\nApache Apex 3.0.0 or later (for launching the App Package in your cluster)\n\n\n\n\nCreating Your First Apex App Package\n\n\nYou can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.\n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:\n\n\n$cd mydtapp; mvn package\n\n\n\nThis builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:\n\n\n -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n\n\n\n\nThe \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as shown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.2.0-incubating (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.\n\n\nWriting Your Own App Package\n\n\nPlease refer to the \nCreating Apps\n on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to \nOperator Development Guide\n), project dependencies, default and required configuration properties, pre-set configurations and other metadata.\n\n\nAdding (and removing) project dependencies\n\n\nUnder the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:\n\n\n  \ndependencies\n\n    \n!-- add your dependencies here --\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\nmalhar-library\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \n!--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      --\n\n      \n!--\n      \nexclusions\n\n        \nexclusion\n\n          \ngroupId\n*\n/groupId\n\n          \nartifactId\n*\n/artifactId\n\n        \n/exclusion\n\n      \n/exclusions\n\n      --\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\norg.apache.apex\n/groupId\n\n      \nartifactId\napex-engine\n/artifactId\n\n      \nversion\n${apex.version}\n/version\n\n      \nscope\nprovided\n/scope\n\n    \n/dependency\n\n    \ndependency\n\n      \ngroupId\njunit\n/groupId\n\n      \nartifactId\njunit\n/artifactId\n\n      \nversion\n4.10\n/version\n\n      \nscope\ntest\n/scope\n\n    \n/dependency\n\n  \n/dependencies\n\n\n\n\n\nBy default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.\n\n\nIn the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.\n\n\nNote that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:\n\n\n\n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]\n\n\n\n\n\nThis is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.\n\n\nApplication Configuration\n\n\nA configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nApplication attributes\n\n\nApplication attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter\n\ndt.attr.\nattribute\n. The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and \nattribute\n\nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.\n\n\n  \nproperty\n\n     \nname\ndt.attr.STREAMING_WINDOW_SIZE_MILLIS\n/name\n\n     \nvalue\n1000\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.\n\n\nOperator attributes\n\n\nOperator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter\n\ndt.operator.\noperator-name\n.attr.\nattribute\n. The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified, \noperator-name\n denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and\n\nattribute\n is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10\n\n\nproperty\n\n  \nname\ndt.operator.input.attr.APPLICATION_WINDOW_COUNT\n/name\n\n  \nvalue\n10\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.\n\n\nOperator properties\n\n\nOperators can be configured using operator specific properties. The\nproperties can be specified using the parameter\n\ndt.operator.\noperator-name\n.prop.\nproperty-name\n. The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and\n\nproperty-name\n specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.\n\n\n  \nproperty\n\n    \nname\ndt.operator.redis.prop.host\n/name\n\n    \nvalue\n127.0.0.1\n/value\n\n  \n/property\n\n\n\n\n\nThe name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.\n\n\nPort attributes\n\n\nPort attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter \ndt.operator.\noperator-name\n.inputport.\nport-name\n.attr.\nattribute\n\nfor input port and \ndt.operator.\noperator-name\n.outputport.\nport-name\n.attr.\nattribute\n\nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.\n\n\nproperty\n\n  \nname\ndt.operator.range.inputport.input.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.\n\n\nThe attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.\n\n\nStream properties\n\n\nStreams can be configured using stream properties. The properties can be\nspecified using the parameter\n\ndt.stream.\nstream-name\n.prop.\nproperty-name\n  The constant \u201cstream\u201d\nspecifies that it is a stream, \nstream-name\n specifies the name of the\nstream and \nproperty-name\n the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.\n\n\n  \nproperty\n\n    \nname\ndt.stream.stream1.prop.locality\n/name\n\n    \nvalue\nCONTAINER_LOCAL\n/value\n\n  \n/property\n\n\n\n\n\nThe property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.\n\n\nAlong with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format\n\nfull-application-class-name\n.\nparam-name\n.\n The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.\n\n\nWildcards\n\n\nWildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows\n\n\nproperty\n\n  \nname\ndt.operator.range.port.*.attr.QUEUE_CAPACITY\n/name\n\n  \nvalue\n4000\n/value\n\n\n/property\n\n\n\n\n\nThe wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.\n\n\nAdding configuration properties\n\n\nIt is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:\n\n\n?xml version=\n1.0\n?\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome_name_1\n/name\n\n  \n/property\n\n  \nproperty\n\n    \nname\nsome_name_2\n/name\n\n    \nvalue\nsome_default_value\n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\n\nThe name of an application-specific property takes the form of:\n\n\ndt.operator.{opName}.prop.{propName}\n\n\nThe first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:\n\n\n    dt.attr.APPLICATION_NAME\n\n\n\nThere are also other properties that can be set.  For details on\nproperties, refer to the \nOperation and Installation Guide\n.\n\n\nIn this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.\n\n\nAdding pre-set configurations\n\n\nAt build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under \nsrc/site/conf/\nconf\n.xml\nin your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.\n\n\nApplication-specific properties file\n\n\nYou can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:\n\n\nproperties.xml: Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.\n\n\nProperties source precedence\n\n\nIf properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:\n\n\n\n\nLaunch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)\n\n\nLaunch time specified configuration file in file system (using -conf\n    option in CLI)\n\n\nLaunch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)\n\n\nConfiguration from \\$HOME/.dt/dt-site.xml\n\n\nApplication defaults within the package as\n    META-INF/properties-{appname}.xml\n\n\nPackage defaults as META-INF/properties.xml\n\n\ndt-site.xml in local DT installation\n\n\ndt-site.xml stored in HDFS\n\n\n\n\nOther meta-data\n\n\nIn a Apex App Package project, the pom.xml file contains a\nsection that looks like:\n\n\nproperties\n\n  \napex.version\n3.2.0-incubating\n/apex.version\n\n  \napex.apppackage.classpath\\\nlib*.jar\n/apex.apppackage.classpath\n\n\n/properties\n\n\n\n\n\napex.version is the Apache Apex version that are to be used\nwith this Application Package.\n\n\napex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.\n\n\nLogging configuration\n\n\nJust like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:\n\n\n log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n\n\n\n\n\nThe root logger\u2019s level is set to WARN and the output is set to the console (stdout).\n\n\nNote that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.\n\n\nZip Structure of Application Package\n\n\nApache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.\n\n\nThere are four top level directories in an Application Package:\n\n\n\n\n\"app\" contains the jar files of the DAG code and any custom operators.\n\n\n\"lib\" contains all dependency jars\n\n\n\"conf\" contains all the pre-set configuration XML files.\n\n\n\"META-INF\" contains the MANIFEST.MF file and the properties.xml file.\n\n\n\u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.\n\n\n\n\nManaging Application Packages Through DT Gateway\n\n\nThe DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.\n\n\nStoring an Application Package\n\n\nYou can store your Application Packages through DT Gateway using this\nREST call:\n\n\n POST /ws/v2/appPackages\n\n\n\n\nThe payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:\n\n\n$ curl -XPOST -T \napp-package-file\n http://localhost:9090/ws/v2/appPackages\n\n\n\n\nGetting Meta Information on Application Packages\n\n\nYou can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}\n\n\n\n\nThe parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this call.\n\n\nGET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}\n\n\n\n\nGetting List of Pre-Set Configurations in Application Package\n\n\nYou can get a list of pre-set configurations within the Application\nPackage using this call.\n\n\nGET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs\n\n\n\n\nYou can also get the content of a specific pre-set configuration within\nthe Application Package.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nChanging Pre-Set Configurations in Application Package\n\n\nYou can create or replace pre-set configurations within the Application\nPackage\n\n\n PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nThe payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.\n\n\n DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}\n\n\n\n\nRetrieving an Application Package\n\n\nYou can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.\n\n\n GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download\n\n\n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\nPOST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}\n\n\n\n\nThe config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:\n\n\n {\nproperty-name\n:\nproperty-value\n, ... }\n\n\n\n\nHere is an example of launching an application through curl:\n\n\n $ curl -XPOST -d'{\ndt.operator.console.prop.stringFormat\n:\nxyz %s\n}'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app\n lications/MyFirstApplication/launch\n\n\n\n\nPlease refer to the \nGateway API reference\n for the complete specification of the REST API.\n\n\nExamining and Launching Application Packages Through Apex CLI\n\n\nIf you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (dtcli).  Please refer to the \nGateway API\n\nto see samples for these commands.\n\n\nGetting Application Package Meta Information\n\n\nYou can get the meta information about the Application Package using\nthis Apex CLI command.\n\n\n dt\n get-app-package-info \napp-package-file\n\n\n\n\n\nGetting Available Operators In Application Package\n\n\nYou can get the list of available operators in the Application Package\nusing this command.\n\n\n dt\n get-app-package-operators \napp-package-file\n \npackage-prefix\n\n [parent-class]\n\n\n\n\nGetting Properties of Operators in Application Package\n\n\nYou can get the list of properties of any operator in the Application\nPackage using this command.\n\n\ndt\n get-app-package-operator-properties \n \n\n\nLaunching an Application Package\n\n\nYou can launch an application within an Application Package.\n\n\ndt\n launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package] \napp-package-file\n\n [matching-app-name]\n\n\n\n\nNote that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Application Packages"
        }, 
        {
            "location": "/application_packages/#apache-apex-application-packages", 
            "text": "An Apache Apex Application Package is a zip file that contains all the\nnecessary files to launch an application in Apache Apex. It is the\nstandard way for assembling and sharing an Apache Apex application.", 
            "title": "Apache Apex Application Packages"
        }, 
        {
            "location": "/application_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the App Package)  Apache Apex 3.0.0 or later (for launching the App Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/application_packages/#creating-your-first-apex-app-package", 
            "text": "You can create an Apex Application Package using your Linux command\nline, or using your favorite IDE.", 
            "title": "Creating Your First Apex App Package"
        }, 
        {
            "location": "/application_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create\nan Apex application project using Maven by running the following\ncommand.  Replace \"com.example\", \"mydtapp\" and \"1.0-SNAPSHOT\" with the\nappropriate values (make sure this is all on one line):  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-app-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtapp -DartifactId=mydtapp \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtapp\". Open it with your favorite\nIDE (e.g. NetBeans, Eclipse, IntelliJ IDEA). In the project, there is a\nsample DAG that generates a number of tuples with a random number and\nprints out \"hello world\" and the random number in the tuples.  The code\nthat builds the DAG is in\nsrc/main/java/com/example/mydtapp/Application.java, and the code that\nruns the unit test for the DAG is in\nsrc/test/java/com/example/mydtapp/ApplicationTest.java. Try it out by\nrunning the following command:  $cd mydtapp; mvn package  This builds the App Package runs the unit test of the DAG.  You should\nbe getting test output similar to this:   -------------------------------------------------------\n  TESTS\n -------------------------------------------------------\n\n Running com.example.mydtapp.ApplicationTest\n hello world: 0.8015370953286478\n hello world: 0.9785359225545481\n hello world: 0.6322611586644047\n hello world: 0.8460953663451775\n hello world: 0.5719372906929072\n hello world: 0.6361174312337172\n hello world: 0.14873007534816318\n hello world: 0.8866986277418261\n hello world: 0.6346526809866057\n hello world: 0.48587295703904465\n hello world: 0.6436832429676687\n\n ...\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.863\n sec\n\n Results :\n\n Tests run: 1, Failures: 0, Errors: 0, Skipped: 0  The \"mvn package\" command creates the App Package file in target\ndirectory as target/mydtapp-1.0-SNAPSHOT.apa. You will be able to use\nthat App Package file to launch this sample application in your actual\nApex installation.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/application_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as shown below.   Group ID: org.apache.apex\nArtifact ID: apex-app-archetype\nVersion: 3.2.0-incubating (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apache Apex App Package\nproject, with a default unit test.  You can run the unit test, make code\nchanges or make dependency changes within your IDE.  The procedure for\nother IDEs, like Eclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/application_packages/#writing-your-own-app-package", 
            "text": "Please refer to the  Creating Apps  on the basics on how to write an Apache Apex application.  In your AppPackage project, you can add custom operators (refer to  Operator Development Guide ), project dependencies, default and required configuration properties, pre-set configurations and other metadata.", 
            "title": "Writing Your Own App Package"
        }, 
        {
            "location": "/application_packages/#adding-and-removing-project-dependencies", 
            "text": "Under the project, you can add project dependencies in pom.xml, or do it\nthrough your IDE.  Here\u2019s the section that describes the dependencies in\nthe default pom.xml:     dependencies \n     !-- add your dependencies here -- \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId malhar-library /artifactId \n       version ${apex.version} /version \n       !--\n           If you know your application do not need the transitive dependencies that are pulled in by malhar-library,\n           Uncomment the following to reduce the size of your app package.\n      -- \n       !--\n       exclusions \n         exclusion \n           groupId * /groupId \n           artifactId * /artifactId \n         /exclusion \n       /exclusions \n      -- \n     /dependency \n     dependency \n       groupId org.apache.apex /groupId \n       artifactId apex-engine /artifactId \n       version ${apex.version} /version \n       scope provided /scope \n     /dependency \n     dependency \n       groupId junit /groupId \n       artifactId junit /artifactId \n       version 4.10 /version \n       scope test /scope \n     /dependency \n   /dependencies   By default, as shown above, the default dependencies include\nmalhar-library in compile scope, dt-engine in provided scope, and junit\nin test scope.  Do not remove these three dependencies since they are\nnecessary for any Apex application.  You can, however, exclude\ntransitive dependencies from malhar-library to reduce the size of your\nApp Package, provided that none of the operators in malhar-library that\nneed the transitive dependencies will be used in your application.  In the sample application, it is safe to remove the transitive\ndependencies from malhar-library, by uncommenting the \"exclusions\"\nsection.  It will reduce the size of the sample App Package from 8MB to\n700KB.  Note that if we exclude *, in some versions of Maven, you may get\nwarnings similar to the following:  \n [WARNING] 'dependencies.dependency.exclusions.exclusion.groupId' for\n org.apache.apex:malhar-library:jar with value '*' does not match a\n valid id pattern.\n\n [WARNING]\n [WARNING] It is highly recommended to fix these problems because they\n threaten the stability of your build.\n [WARNING]\n [WARNING] For this reason, future Maven versions might no longer support\n building such malformed projects.\n [WARNING]  This is a bug in early versions of Maven 3.  The dependency exclusion is\nstill valid and it is safe to ignore these warnings.", 
            "title": "Adding (and removing) project dependencies"
        }, 
        {
            "location": "/application_packages/#application-configuration", 
            "text": "A configuration file can be used to configure an application.  Different\nkinds of configuration parameters can be specified. They are application\nattributes, operator attributes and properties, port attributes, stream\nproperties and application specific properties. They are all specified\nas name value pairs, in XML format, like the following.  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n     value some_default_value /value \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration", 
            "title": "Application Configuration"
        }, 
        {
            "location": "/application_packages/#application-attributes", 
            "text": "Application attributes are used to specify the platform behavior for the\napplication. They can be specified using the parameter dt.attr. attribute . The prefix \u201cdt\u201d is a constant, \u201cattr\u201d is a\nconstant denoting an attribute is being specified and  attribute \nspecifies the name of the attribute. Below is an example snippet setting\nthe streaming windows size of the application to be 1000 milliseconds.     property \n      name dt.attr.STREAMING_WINDOW_SIZE_MILLIS /name \n      value 1000 /value \n   /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.DAGContext and the different attributes can\nbe specified in the format described above.", 
            "title": "Application attributes"
        }, 
        {
            "location": "/application_packages/#operator-attributes", 
            "text": "Operator attributes are used to specify the platform behavior for the\noperator. They can be specified using the parameter dt.operator. operator-name .attr. attribute . The prefix \u201cdt\u201d is a\nconstant, \u201coperator\u201d is a constant denoting that an operator is being\nspecified,  operator-name  denotes the name of the operator, \u201cattr\u201d is\nthe constant denoting that an attribute is being specified and attribute  is the name of the attribute. The operator name is the\nsame name that is specified when the operator is added to the DAG using\nthe addOperator method. An example illustrating the specification is\nshown below. It specifies the number of streaming windows for one\napplication window of an operator named \u201cinput\u201d to be 10  property \n   name dt.operator.input.attr.APPLICATION_WINDOW_COUNT /name \n   value 10 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.OperatorContext and the different attributes\ncan be specified in the format described above.", 
            "title": "Operator attributes"
        }, 
        {
            "location": "/application_packages/#operator-properties", 
            "text": "Operators can be configured using operator specific properties. The\nproperties can be specified using the parameter dt.operator. operator-name .prop. property-name . The difference\nbetween this and the operator attribute specification described above is\nthat the keyword \u201cprop\u201d is used to denote that it is a property and property-name  specifies the property name.  An example illustrating\nthis is specified below. It specifies the property \u201chostname\u201d of the\nredis server for a \u201credis\u201d output operator.     property \n     name dt.operator.redis.prop.host /name \n     value 127.0.0.1 /value \n   /property   The name tag specifies the property and the value specifies the property\nvalue. The property name is converted to a setter method which is called\non the actual operator. The method name is composed by appending the\nword \u201cset\u201d and the property name with the first character of the name\ncapitalized. In the above example the setter method would become\nsetHost. The method is called using JAVA reflection and the property\nvalue is passed as an argument. In the above example the method setHost\nwill be called on the \u201credis\u201d operator with \u201c127.0.0.1\u201d as the argument.", 
            "title": "Operator properties"
        }, 
        {
            "location": "/application_packages/#port-attributes", 
            "text": "Port attributes are used to specify the platform behavior for input and\noutput ports. They can be specified using the parameter  dt.operator. operator-name .inputport. port-name .attr. attribute \nfor input port and  dt.operator. operator-name .outputport. port-name .attr. attribute \nfor output port. The keyword \u201cinputport\u201d is used to denote an input port\nand \u201coutputport\u201d to denote an output port. The rest of the specification\nfollows the conventions described in other specifications above. An\nexample illustrating this is specified below. It specifies the queue\ncapacity for an input port named \u201cinput\u201d of an operator named \u201crange\u201d to\nbe 4k.  property \n   name dt.operator.range.inputport.input.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The name tag specifies the attribute and value tag specifies the\nattribute value. The name of the attribute is a JAVA constant name\nidentifying the attribute. The constants are defined in\ncom.datatorrent.api.Context.PortContext and the different attributes can\nbe specified in the format described above.  The attributes for an output port can also be specified in a similar way\nas described above with a change that keyword \u201coutputport\u201d is used\ninstead of \u201cintputport\u201d. A generic keyword \u201cport\u201d can be used to specify\neither an input or an output port. It is useful in the wildcard\nspecification described below.", 
            "title": "Port attributes"
        }, 
        {
            "location": "/application_packages/#stream-properties", 
            "text": "Streams can be configured using stream properties. The properties can be\nspecified using the parameter dt.stream. stream-name .prop. property-name   The constant \u201cstream\u201d\nspecifies that it is a stream,  stream-name  specifies the name of the\nstream and  property-name  the name of the property. The name of the\nstream is the same name that is passed when the stream is added to the\nDAG using the addStream method. An example illustrating the\nspecification is shown below. It sets the locality of the stream named\n\u201cstream1\u201d to container local indicating that the operators the stream is\nconnecting be run in the same container.     property \n     name dt.stream.stream1.prop.locality /name \n     value CONTAINER_LOCAL /value \n   /property   The property name is converted into a set method on the stream in the\nsame way as described in operator properties section above. In this case\nthe method would be setLocality and it will be called in the stream\n\u201cstream1\u201d with the value as the argument.  Along with the above system defined parameters, the applications can\ndefine their own specific parameters they can be specified in the\nconfiguration file. The only condition is that the names of these\nparameters don\u2019t conflict with the system defined parameters or similar\napplication parameters defined by other applications. To this end, it is\nrecommended that the application parameters have the format full-application-class-name . param-name .  The\nfull-application-class-name is the full JAVA class name of the\napplication including the package path and param-name is the name of the\nparameter within the application. The application will still have to\nstill read the parameter in using the configuration API of the\nconfiguration object that is passed in populateDAG.", 
            "title": "Stream properties"
        }, 
        {
            "location": "/application_packages/#wildcards", 
            "text": "Wildcards and regular expressions can be used in place of names to\nspecify a group for applications, operators, ports or streams. For\nexample, to specify an attribute for all ports of an operator it can be\ndone as follows  property \n   name dt.operator.range.port.*.attr.QUEUE_CAPACITY /name \n   value 4000 /value  /property   The wildcard \u201c*\u201d was used instead of the name of the port. Wildcard can\nalso be used for operator name, stream name or application name. Regular\nexpressions can also be used for names to specify attributes or\nproperties for a specific set.", 
            "title": "Wildcards"
        }, 
        {
            "location": "/application_packages/#adding-configuration-properties", 
            "text": "It is common for applications to require configuration parameters to\nrun.  For example, the address and port of the database, the location of\na file for ingestion, etc.  You can specify them in\nsrc/main/resources/META-INF/properties.xml under the App Package\nproject. The properties.xml may look like:  ?xml version= 1.0 ?  configuration \n   property \n     name some_name_1 /name \n   /property \n   property \n     name some_name_2 /name \n     value some_default_value /value \n   /property  /configuration   The name of an application-specific property takes the form of:  dt.operator.{opName}.prop.{propName}  The first represents the property with name propName of operator opName.\n Or you can set the application name at run time by setting this\nproperty:      dt.attr.APPLICATION_NAME  There are also other properties that can be set.  For details on\nproperties, refer to the  Operation and Installation Guide .  In this example, property some_name_1 is a required property which\nmust be set at launch time, or it must be set by a pre-set configuration\n(see next section).  Property some_name_2 is a property that is\nassigned with value some_default_value unless it is overridden at\nlaunch time.", 
            "title": "Adding configuration properties"
        }, 
        {
            "location": "/application_packages/#adding-pre-set-configurations", 
            "text": "At build time, you can add pre-set configurations to the App Package by\nadding configuration XML files under  src/site/conf/ conf .xml in your\nproject.  You can then specify which configuration to use at launch\ntime.  The configuration XML is of the same format of the properties.xml\nfile.", 
            "title": "Adding pre-set configurations"
        }, 
        {
            "location": "/application_packages/#application-specific-properties-file", 
            "text": "You can also specify properties.xml per application in the application\npackage.  Just create a file with the name properties-{appName}.xml and\nit will be picked up when you launch the application with the specified\nname within the application package.  In short:  properties.xml: Properties that are global to the Configuration\nPackage  properties-{appName}.xml: Properties that are specific when launching\nan application with the specified appName.", 
            "title": "Application-specific properties file"
        }, 
        {
            "location": "/application_packages/#properties-source-precedence", 
            "text": "If properties with the same key appear in multiple sources (e.g. from\napp package default configuration as META-INF/properties.xml, from app\npackage configuration in the conf directory, from launch time defines,\netc), the precedence of sources, from highest to lowest, is as follows:   Launch time defines (using -D option in CLI, or the POST payload\n    with the Gateway REST API\u2019s launch call)  Launch time specified configuration file in file system (using -conf\n    option in CLI)  Launch time specified package configuration (using -apconf option in\n    CLI or the conf={confname} with Gateway REST API\u2019s launch call)  Configuration from \\$HOME/.dt/dt-site.xml  Application defaults within the package as\n    META-INF/properties-{appname}.xml  Package defaults as META-INF/properties.xml  dt-site.xml in local DT installation  dt-site.xml stored in HDFS", 
            "title": "Properties source precedence"
        }, 
        {
            "location": "/application_packages/#other-meta-data", 
            "text": "In a Apex App Package project, the pom.xml file contains a\nsection that looks like:  properties \n   apex.version 3.2.0-incubating /apex.version \n   apex.apppackage.classpath\\ lib*.jar /apex.apppackage.classpath  /properties   apex.version is the Apache Apex version that are to be used\nwith this Application Package.  apex.apppackage.classpath is the classpath that is used when\nlaunching the application in the Application Package.  The default is\nlib/*.jar, where lib is where all the dependency jars are kept within\nthe Application Package.  One reason to change this field is when your\nApplication Package needs the classpath in a specific order.", 
            "title": "Other meta-data"
        }, 
        {
            "location": "/application_packages/#logging-configuration", 
            "text": "Just like other Java projects, you can change the logging configuration\nby having your log4j.properties under src/main/resources.  For example,\nif you have the following in src/main/resources/log4j.properties:   log4j.rootLogger=WARN,CONSOLE\n log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\n log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\n log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] %-5p\n %c{2} %M - %m%n  The root logger\u2019s level is set to WARN and the output is set to the console (stdout).  Note that by default from project created from the maven archetype,\nthere is already a log4j.properties file under src/test/resources and\nthat file is only used for the unit test.", 
            "title": "Logging configuration"
        }, 
        {
            "location": "/application_packages/#zip-structure-of-application-package", 
            "text": "Apache Apex Application Package files are zip files.  You can examine the content of any Application Package by using unzip -t on your Linux command line.  There are four top level directories in an Application Package:   \"app\" contains the jar files of the DAG code and any custom operators.  \"lib\" contains all dependency jars  \"conf\" contains all the pre-set configuration XML files.  \"META-INF\" contains the MANIFEST.MF file and the properties.xml file.  \u201cresources\u201d contains other files that are to be served by the Gateway on behalf of the app package.", 
            "title": "Zip Structure of Application Package"
        }, 
        {
            "location": "/application_packages/#managing-application-packages-through-dt-gateway", 
            "text": "The DT Gateway provides storing and retrieving Application Packages to\nand from your distributed file system, e.g. HDFS.", 
            "title": "Managing Application Packages Through DT Gateway"
        }, 
        {
            "location": "/application_packages/#storing-an-application-package", 
            "text": "You can store your Application Packages through DT Gateway using this\nREST call:   POST /ws/v2/appPackages  The payload is the raw content of your Application Package.  For\nexample, you can issue this request using curl on your Linux command\nline like this, assuming your DT Gateway is accepting requests at\nlocalhost:9090:  $ curl -XPOST -T  app-package-file  http://localhost:9090/ws/v2/appPackages", 
            "title": "Storing an Application Package"
        }, 
        {
            "location": "/application_packages/#getting-meta-information-on-application-packages", 
            "text": "You can get the meta information on Application Packages stored through\nDT Gateway using this call.  The information includes the logical plan\nof each application within the Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}", 
            "title": "Getting Meta Information on Application Packages"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package", 
            "text": "You can get the list of available operators in the Application Package\nusing this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators?parent={parent}  The parent parameter is optional.  If given, parent should be the fully\nqualified class name.  It will only return operators that derive from\nthat class or interface. For example, if parent is\ncom.datatorrent.api.InputOperator, this call will only return input\noperators provided by the Application Package.", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this call.  GET  /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/operators/{className}", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#getting-list-of-pre-set-configurations-in-application-package", 
            "text": "You can get a list of pre-set configurations within the Application\nPackage using this call.  GET /ws/v2/appPackages/{owner}/{pkgName}/{packageVersion}/configs  You can also get the content of a specific pre-set configuration within\nthe Application Package.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Getting List of Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#changing-pre-set-configurations-in-application-package", 
            "text": "You can create or replace pre-set configurations within the Application\nPackage   PUT   /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}  The payload of this PUT call is the XML file that represents the pre-set configuration.  The Content-Type of the payload is \"application/xml\" and you can delete a pre-set configuration within the Application Package.   DELETE /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/configs/{configName}", 
            "title": "Changing Pre-Set Configurations in Application Package"
        }, 
        {
            "location": "/application_packages/#retrieving-an-application-package", 
            "text": "You can download the Application Package file.  This Application Package\nis not necessarily the same file as the one that was originally uploaded\nsince the pre-set configurations may have been modified.   GET /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/download", 
            "title": "Retrieving an Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package", 
            "text": "You can launch an application within an Application Package.  POST /ws/v2/appPackages/{owner}/{pkgName}/{pkgVersion}/applications/{appName}/launch?config={configName}  The config parameter is optional.  If given, it must be one of the\npre-set configuration within the given Application Package.  The\nContent-Type of the payload of the POST request is \"application/json\"\nand should contain the properties to be launched with the application.\n It is of the form:   { property-name : property-value , ... }  Here is an example of launching an application through curl:   $ curl -XPOST -d'{ dt.operator.console.prop.stringFormat : xyz %s }'\n http://localhost:9090/ws/v2/appPackages/dtadmin/mydtapp/1.0-SNAPSHOT/app\n lications/MyFirstApplication/launch  Please refer to the  Gateway API reference  for the complete specification of the REST API.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/application_packages/#examining-and-launching-application-packages-through-apex-cli", 
            "text": "If you are working with Application Packages in the local filesystem and\ndo not want to deal with dtGateway, you can use the Apex Command Line Interface (dtcli).  Please refer to the  Gateway API \nto see samples for these commands.", 
            "title": "Examining and Launching Application Packages Through Apex CLI"
        }, 
        {
            "location": "/application_packages/#getting-application-package-meta-information", 
            "text": "You can get the meta information about the Application Package using\nthis Apex CLI command.   dt  get-app-package-info  app-package-file", 
            "title": "Getting Application Package Meta Information"
        }, 
        {
            "location": "/application_packages/#getting-available-operators-in-application-package_1", 
            "text": "You can get the list of available operators in the Application Package\nusing this command.   dt  get-app-package-operators  app-package-file   package-prefix \n [parent-class]", 
            "title": "Getting Available Operators In Application Package"
        }, 
        {
            "location": "/application_packages/#getting-properties-of-operators-in-application-package_1", 
            "text": "You can get the list of properties of any operator in the Application\nPackage using this command.  dt  get-app-package-operator-properties", 
            "title": "Getting Properties of Operators in Application Package"
        }, 
        {
            "location": "/application_packages/#launching-an-application-package_1", 
            "text": "You can launch an application within an Application Package.  dt  launch [-D property-name=property-value, ...] [-conf config-name]\n [-apconf config-file-within-app-package]  app-package-file \n [matching-app-name]  Note that -conf expects a configuration file in the file system, while -apconf expects a configuration file within the app package.", 
            "title": "Launching an Application Package"
        }, 
        {
            "location": "/configuration_packages/", 
            "text": "Apache Apex Configuration Packages\n\n\nAn Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an\n\nApplication Package\n using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.\n\n\nRequirements\n\n\nYou will need have the following installed:\n\n\n\n\nApache Maven 3.0 or later (for assembling the Config Package)\n\n\nApex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)\n\n\n\n\nCreating Your First Configuration Package\n\n\nYou can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.  \n\n\nUsing Command Line\n\n\nFirst, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:\n\n\n$ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT\n\n\n\nThis creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:\n\n\n$ mvn package                                                         \n\n\n\n\nThe \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.\n\n\nUsing IDE\n\n\nAlternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.\n\n\n\n\nThen fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.\n\n\n\n\nGroup ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.2.0-incubating (or any later version)\n\n\nPress Next and fill out the rest of the required information. For\nexample:\n\n\n\n\nClick Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.\n\n\nAssembling your own configuration package\n\n\nInside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:\n\n\n./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml\n\n\n\npom.xml\n\n\nExample:\n\n\n  \ngroupId\ncom.example\n/groupId\n\n  \nversion\n1.0.0\n/version\n\n  \nartifactId\nmydtconf\n/artifactId\n\n  \npackaging\njar\n/packaging\n\n  \n!-- change these to the appropriate values --\n\n  \nname\nMy DataTorrent Application Configuration\n/name\n\n  \ndescription\nMy DataTorrent Application Configuration Description\n/description\n\n  \nproperties\n\n    \ndatatorrent.apppackage.name\nmydtapp\n/datatorrent.apppackage.name\n\n    \ndatatorrent.apppackage.minversion\n1.0.0\n/datatorrent.apppackage.minversion\n\n   \ndatatorrent.apppackage.maxversion\n1.9999.9999\n/datatorrent.apppackage.maxversion\n\n    \ndatatorrent.appconf.classpath\nclasspath/*\n/datatorrent.appconf.classpath\n\n    \ndatatorrent.appconf.files\nfiles/*\n/datatorrent.appconf.files\n\n  \n/properties\n \n\n\n\n\n\nIn pom.xml, you can change the following keys to your desired values\n\n\n\n\ngroupId\n\n\nversion\n\n\nartifactId\n\n\nname\n\n\ndescription\n\n\n\n\nYou can also change the values of \n\n\n\n\ndatatorrent.apppackage.name\n\n\ndatatorrent.apppackage.minversion\n\n\ndatatorrent.apppackage.maxversion\n\n\n\n\nto reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.\n\n\n./src/main/resources/classpath\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.\n\n\n./src/main/resources/files\n\n\nPlace any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.\n\n\nProperties XML file\n\n\nA properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.  \n\n\nExample:\n\n\nconfiguration\n\n  \nproperty\n\n    \nname\nsome-property-name\n/name\n\n    \nvalue\nsome-property-value\n/value\n\n  \n/property\n\n   ...\n\n/configuration\n\n\n\n\n\nNames of properties XML file:\n\n\n\n\nproperties.xml:\n Properties that are global to the Configuration\nPackage\n\n\nproperties-{appName}.xml:\n Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.\n\n\n\n\nAfter you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.\n\n\nZip structure of configuration package\n\n\nApex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:\n\n\nMETA-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files} \n\n\n\n\nLaunching with CLI\n\n\n-conf\n option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:\n\n\ndt\\\n launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar\n\n\n\nThis command expects both the application package and the configuration package to be in the local file system.\n\n\nRelated REST API\n\n\nPOST /ws/v2/configPackages\n\n\nPayload: Raw content of configuration package zip\n\n\nFunction: Creates or replace a configuration package zip file in HDFS\n\n\nCurl example:\n\n\n$ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages\n\n\n\nGET /ws/v2/configPackages?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName. \n\n\nGET /ws/v2/configPackages/\nuser\n?appPackageName=...\nappPackageVersion=...\n\n\nAll query parameters are optional\n\n\nFunction: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n\n\nFunction: Returns the information of the specified configuration package\n\n\nGET /ws/v2/configPackages/\nuser\n/\nname\n/download\n\n\nFunction: Returns the raw config package file\n\n\nCurl example:\n\n\n$ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \\\n DTConfig-xyz.jar\n$ unzip -t DTConfig-xyz.jar\n\n\n\n\nPOST /ws/v2/appPackages/\nuser\n/\napp-pkg-name\n/\napp-pkg-version\n/applications/{app-name}/launch?configPackage=\nuser\n/\nconfpkgname\n\n\nFunction: Launches the app package with the specified configuration package stored in HDFS.\n\n\nCurl example:\n\n\n$ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#apache-apex-configuration-packages", 
            "text": "An Apache Apex Application Configuration Package is a zip file that contains\nconfiguration files and additional files to be launched with an Application Package  using \nDTCLI or REST API.  This guide assumes the reader\u2019s familiarity of\nApplication Package.  Please read the Application Package document to\nget yourself familiar with the concept first if you have not done so.", 
            "title": "Apache Apex Configuration Packages"
        }, 
        {
            "location": "/configuration_packages/#requirements", 
            "text": "You will need have the following installed:   Apache Maven 3.0 or later (for assembling the Config Package)  Apex 3.0.0 or later (for launching the App Package with the Config\n    Package in your cluster)", 
            "title": "Requirements"
        }, 
        {
            "location": "/configuration_packages/#creating-your-first-configuration-package", 
            "text": "You can create a Configuration Package using your Linux command line, or\nusing your favorite IDE.", 
            "title": "Creating Your First Configuration Package"
        }, 
        {
            "location": "/configuration_packages/#using-command-line", 
            "text": "First, change to the directory where you put your projects, and create a\nDT configuration project using Maven by running the following command.\n Replace \"com.example\", \"mydtconfig\" and \"1.0-SNAPSHOT\" with the\nappropriate values:  $ mvn archetype:generate \\\n -DarchetypeGroupId=org.apache.apex \\\n -DarchetypeArtifactId=apex-conf-archetype -DarchetypeVersion=3.2.0-incubating \\\n -DgroupId=com.example -Dpackage=com.example.mydtconfig -DartifactId=mydtconfig \\\n -Dversion=1.0-SNAPSHOT  This creates a Maven project named \"mydtconfig\". Open it with your\nfavorite IDE (e.g. NetBeans, Eclipse, IntelliJ IDEA).  Try it out by\nrunning the following command:  $ mvn package                                                           The \"mvn package\" command creates the Config Package file in target\ndirectory as target/mydtconfig.apc. You will be able to use that\nConfiguration Package file to launch an Apache Apex application.", 
            "title": "Using Command Line"
        }, 
        {
            "location": "/configuration_packages/#using-ide", 
            "text": "Alternatively, you can do the above steps all within your IDE.  For\nexample, in NetBeans, select File -> New Project.  Then choose \u201cMaven\u201d\nand \u201cProject from Archetype\u201d in the dialog box, as shown.   Then fill the Group ID, Artifact ID, Version and Repository entries as\nshown below.   Group ID: org.apache.apex\nArtifact ID: apex-conf-archetype\nVersion: 3.2.0-incubating (or any later version)  Press Next and fill out the rest of the required information. For\nexample:   Click Finish, and now you have created your own Apex\nConfiguration Package project.  The procedure for other IDEs, like\nEclipse or IntelliJ, is similar.", 
            "title": "Using IDE"
        }, 
        {
            "location": "/configuration_packages/#assembling-your-own-configuration-package", 
            "text": "Inside the project created by the archetype, these are the files that\nyou should know about when assembling your own configuration package:  ./pom.xml\n./src/main/resources/classpath\n./src/main/resources/files\n./src/main/resources/META-INF/properties.xml\n./src/main/resources/META-INF/properties-{appname}.xml", 
            "title": "Assembling your own configuration package"
        }, 
        {
            "location": "/configuration_packages/#pomxml", 
            "text": "Example:     groupId com.example /groupId \n   version 1.0.0 /version \n   artifactId mydtconf /artifactId \n   packaging jar /packaging \n   !-- change these to the appropriate values -- \n   name My DataTorrent Application Configuration /name \n   description My DataTorrent Application Configuration Description /description \n   properties \n     datatorrent.apppackage.name mydtapp /datatorrent.apppackage.name \n     datatorrent.apppackage.minversion 1.0.0 /datatorrent.apppackage.minversion \n    datatorrent.apppackage.maxversion 1.9999.9999 /datatorrent.apppackage.maxversion \n     datatorrent.appconf.classpath classpath/* /datatorrent.appconf.classpath \n     datatorrent.appconf.files files/* /datatorrent.appconf.files \n   /properties    In pom.xml, you can change the following keys to your desired values   groupId  version  artifactId  name  description   You can also change the values of    datatorrent.apppackage.name  datatorrent.apppackage.minversion  datatorrent.apppackage.maxversion   to reflect what app packages should be used with this configuration package.  Apex will use this information to check whether a\nconfiguration package is compatible with the application package when you issue a launch command.", 
            "title": "pom.xml"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesclasspath", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application and included in the\nclasspath of the application.  Example of such files are Java properties\nfiles and jar files.", 
            "title": "./src/main/resources/classpath"
        }, 
        {
            "location": "/configuration_packages/#srcmainresourcesfiles", 
            "text": "Place any file in this directory that you\u2019d like to be copied to the\ncompute machines when launching an application but not included in the\nclasspath of the application.", 
            "title": "./src/main/resources/files"
        }, 
        {
            "location": "/configuration_packages/#properties-xml-file", 
            "text": "A properties xml file consists of a set of key-value pairs.  The set of\nkey-value pairs specifies the configuration options the application\nshould be launched with.    Example:  configuration \n   property \n     name some-property-name /name \n     value some-property-value /value \n   /property \n   ... /configuration   Names of properties XML file:   properties.xml:  Properties that are global to the Configuration\nPackage  properties-{appName}.xml:  Properties that are specific when launching\nan application with the specified appName within the Application\nPackage.   After you are done with the above, remember to do mvn package to\ngenerate a new configuration package, which will be located in the\ntarget directory in your project.", 
            "title": "Properties XML file"
        }, 
        {
            "location": "/configuration_packages/#zip-structure-of-configuration-package", 
            "text": "Apex Application Configuration Package files are zip files.  You\ncan examine the content of any Application Configuration Package by\nusing unzip -t on your Linux command line.  The structure of the zip\nfile is as follow:  META-INF\n  MANIFEST.MF\n  properties.xml\n  properties-{appname}.xml\nclasspath\n  {classpath files}\nfiles\n  {files}", 
            "title": "Zip structure of configuration package"
        }, 
        {
            "location": "/configuration_packages/#launching-with-cli", 
            "text": "-conf  option of the launch command in CLI supports specifying configuration package in the local filesystem.  Example:  dt\\  launch DTApp-mydtapp-1.0.0.jar -conf DTConfig-mydtconfig-1.0.0.jar  This command expects both the application package and the configuration package to be in the local file system.", 
            "title": "Launching with CLI"
        }, 
        {
            "location": "/configuration_packages/#related-rest-api", 
            "text": "POST /ws/v2/configPackages  Payload: Raw content of configuration package zip  Function: Creates or replace a configuration package zip file in HDFS  Curl example:  $ curl -XPOST -T DTConfig-{name}.jar http://{yourhost:port}/ws/v2/configPackages  GET /ws/v2/configPackages?appPackageName=... appPackageVersion=...  All query parameters are optional  Function: Returns the configuration packages that the user is authorized to use and that are compatible with the specified appPackageName, appPackageVersion and appName.   GET /ws/v2/configPackages/ user ?appPackageName=... appPackageVersion=...  All query parameters are optional  Function: Returns the configuration packages under the specified user and that are compatible with the specified appPackageName, appPackageVersion and appName.  GET /ws/v2/configPackages/ user / name  Function: Returns the information of the specified configuration package  GET /ws/v2/configPackages/ user / name /download  Function: Returns the raw config package file  Curl example:  $ curl http://{yourhost:port}/ws/v2/configPackages/{user}/{name}/download \\  DTConfig-xyz.jar\n$ unzip -t DTConfig-xyz.jar  POST /ws/v2/appPackages/ user / app-pkg-name / app-pkg-version /applications/{app-name}/launch?configPackage= user / confpkgname  Function: Launches the app package with the specified configuration package stored in HDFS.  Curl example:  $ curl -XPOST -d \u2019{}\u2019 http://{yourhost:port}/ws/v2/appPackages/{user}/{app-pkg-name}/{app-pkg-version}/applications/{app-name}/launch?configPackage={user}/{confpkgname}", 
            "title": "Related REST API"
        }, 
        {
            "location": "/operator_development/", 
            "text": "Operator Development Guide\n\n\nOperators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).\n\n\nIn this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes\n\n\n\n\nApache Apex Operators\n\u00a0- Introduction to operator terminology and concepts.\n\n\nWriting Custom Operators\n\u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.\n\n\nOperator Reference\n - Details of operator internals, lifecycle, and best practices and optimizations.\n\n\n\n\n\n\nApache Apex Operators \n\n\nOperators - \u201cWhat\u201d in a nutshell\n\n\nOperators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.\n\n\n\n\nOperators - \u201cHow\u201d in a nutshell\n\n\nAn Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.\n\n\nTypes of Operators\n\n\nAn operator works on one tuple at a time. These tuples may be supplied\nby some other operator in the application or by some external source,\nlike a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to some other operator, or may be\nstored into some external system. Based on the functions that the\nOperator performs, we have the following types of operators:\n\n\n\n\nInput Adapter\n - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.\n\n\nGeneric Operator\n - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.\n\n\nOutput Adapter\n - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.\n\n\n\n\nNote: There can be multiple operators of all types in an application\nDAG.\n\n\nPositioning of Operators in the DAG\n\n\nWe may refer to operators depending on their positioning with respect to\none another. For any operator opr, we have the following types of\noperators.\n\n\n\n\nUpstream operators\n - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.\n\n\nDownstream operators\n - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.\n\n\n\n\nNote that there are no cycles formed in the application\u00a0DAG.\n\n\n\n\nPorts\n\n\nThe operators in the DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. The ports again fall into two types.\n\n\n\n\nInput Port\n - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.\n\n\nOutput port\n - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.\n\n\n\n\nLooking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.\n\n\n\n\n\n\nWorking of an Operator\n\n\nAn operator passes through various stages during its lifetime. Each\nstage is an API call that the streaming application master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.\n\n\n\n\n\n\nThe \nsetup()\n call initializes the operator and prepares itself to\n    start processing tuples.\n\n\nThe \nbeginWindow()\n call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.\n\n\nThe \nprocess()\n call belongs to the \nInputPort\n and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.\n\n\nThe \nemitTuples()\n is the counterpart of \nprocess()\n call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.\n\n\nThe \nendWindow()\n call marks the end of the window and allows for any\n    processing to be done after the window ends.\n\n\nThe \nteardown()\n call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.\n\n\n\n\nWriting Custom Operators \n\n\nAbout this tutorial\n\n\nThis tutorial will guide the user towards developing a operator from\nscratch. It will include all aspects of writing an operator including\ndesign, code as well as unit tests.\n\n\nIntroduction\n\n\nIn this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.\n\n\nDesign\n\n\nDesign of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.\n\n\nFunctionality\n\n\nWe can define the scope of operator functionality using the following\ntasks:\n\n\n\n\nParse the input tuple to identify the words in the tuple\n\n\nIdentify the stop-words in the tuple by looking up the stop-word\n    file as configured\n\n\nFor each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts\n\n\n\n\nLet\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.\n\n\n\n\nHumpty dumpty sat on a wall\n\n\nHumpty dumpty had a great fall\n\n\n\n\nInitially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:\n\n\nhumpty - 1\ndumpty - 1\nsat - 1\nwall - 1\n\n\n\n\nNote that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.\n\n\nSimilarly, after the second tuple is processed, the counts that must be\nemitted are:\n\n\nhumpty - 2\ndumpty - 2\ngreat - 1\nfall - 1\n\n\n\n\nAgain, we ignore the words \n\u201chad\u201d\n and \n\u201ca\u201d\n since these are stop-words.\n\n\nNote that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.\n\n\nInputs\n\n\nAs seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:\n\n\n\n\nInput stream whose tuple type is String\n\n\nInput HDFS file path, pointing to a file containing stop-words\n\n\n\n\nOnly one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.\n\n\n\n\nOutputs\n\n\nWe can define the output for this operator in multiple ways.\n\n\n\n\nThe operator may send out the set of counts for which the counts\n    have changed after processing each tuple.\n\n\nSome applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.\n\n\n\n\nLet us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter\n\n\u201csendPerTuple\u201d\n. The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).\n\n\nThe type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a \n key, value \n\u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.\n\n\n\n\nConfiguration\n\n\nWe have the following configuration parameters:\n\n\n\n\nstopWordFilePath\n\u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.\n\n\nsendPerTuple\n\u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.\n\n\n\n\nCode\n\n\nThe source code for the tutorial can be found here:\n\n\nhttps://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial\n\n\nOperator Reference \n\n\nThe Operator Class\n\n\nThe operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:\n\n\n-   setup(OperatorContext context)\n-   beginWindow(long windowId)\n-   endWindow()\n-   tearDown()\n\n\n\n\nIn order to simplify the creation of an operator, the Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the \nGetting\nStarted\n\u00a0section and the\n\nReference\n\u00a0section for details on these.\n\n\nWe extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.\n\n\npublic class WordCountOperator extends BaseOperator\n{\n}\n\n\n\n\nClass (Operator) properties\n\n\nWe define the following class variables:\n\n\n\n\nsendPerTuple\n\u00a0- Needed for configuring the frequency of output from\n    the operator\n\n\n\n\nprivate boolean sendPerTuple = true; // default\n\n\n\n\n\n\nstopWordFilePath\n\u00a0- Stores the path to the stop words file on HDFS\n\n\n\n\nprivate String stopWordFilePath;\u00a0// no default\n\n\n\n\n\n\nstopWords\n\u00a0- Stores the stop words read from the configured file\n\n\n\n\nprivate transient String[] stopWords;\n\n\n\n\n\n\nglobalCounts\n\u00a0- A Map which stores the counts for all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpointing state\n    and can be recovered in event of a crash.\n\n\n\n\nprivate Map\nString, Long\n globalCounts;\n\n\n\n\n\n\nupdatedCounts\n\u00a0- A map which stores the counts for only the most\n    recent tuple(s). Whether to store the most recent or the recent\n    window worth of tuples will be determined by the configuration\n    parameter sendPerTuple.\n\n\n\n\nprivate transient Map\nString, Long\n updatedCounts;\n\n\n\n\n\n\ninput\n - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.\n\n\n\n\npublic transient DefaultInputPort\nString\n input = new \u00a0 \u00a0\nDefaultInputPort\nString\n()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};\n\n\n\n\n\n\noutput - The output port for the operator. The type of this port is\n    Entry \n String, Long \n, which means the operator will emit \n word,\n    count \n pairs for the updated counts.\n\n\n\n\npublic transient DefaultOutputPort \nEntry\nString, Long\n output = new\nDefaultOutputPort\nEntry\nString,Long\n();\n\n\n\n\nThe Constructor\n\n\nThe constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. In case\nof Word Count\u00a0operator, we initialize the globalCounts variable in the\nconstructor.\n\n\nglobalCounts = Maps.newHashMap();\n\n\n\n\nSetup call\n\n\nThe setup method is called only once during the lifetime of an operator.\nThe purpose of the setup call is to allow the operator to set itself up\nfor processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence it is essential that such objects\nmust be initialized in the setup call. In case of operator failure, the\noperator will be redeployed, most likely on a different container. In\nthis case, it is the setup method which will be called by the Apache\nApex engine to allow the operator to prepare for execution in the new\ncontainer.\n\n\nWe perform the following tasks as part of the setup call:\n\n\n\n\nRead the stop-word list from HDFS and store it in the\n    stopWords\u00a0array\n\n\nInitialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    This is a transient variable, hence the value of this variable will\n    be lost in case of operator failure.\n\n\n\n\nBegin Window call\n\n\nThe begin window call signals the start of an application window. In\ncase of Word Count Operator, if the sendPerTuple\u00a0is set to false, it\nmeans that we are expecting updated counts for the most recent window of\ndata. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.\n\n\nProcess Tuple call\n\n\nThe processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.\n\n\nEnd Window call\n\n\nThis call signals the end of an application window. In case of Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.\n\n\nTeardown call\n\n\nThis method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. In case of our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.\n\n\nTesting your Operator\n\n\nTesting an operator after development is essential to ensure that he\nrequired functionality is indeed correctly implemented. As part of\ntesting our operator, we test the following two facets:\n\n\n\n\nTest output of the operator after processing a single tuple\n\n\nTest output of the operator after processing of a window of tuples\n\n\n\n\nThe unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.\n\n\n\n\nInvoke constructor; non-transients initialized.\n\n\nCopy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Operators"
        }, 
        {
            "location": "/operator_development/#operator-development-guide", 
            "text": "Operators are basic building blocks of an application built to run on\nApache Apex\u00a0platform. An application may consist of one or more\noperators each of which define some logical operation to be done on the\ntuples arriving at the operator. These operators are connected together\nusing streams forming a Directed Acyclic Graph (DAG). In other words, a streaming\napplication is represented by a DAG that consists of operations (called operators) and\ndata flow (called streams).  In this document we will discuss details on how an operator works and\nits internals. This document is intended to serve the following purposes   Apache Apex Operators \u00a0- Introduction to operator terminology and concepts.  Writing Custom Operators \u00a0- Designing, coding and testing new operators from scratch.  Includes code examples.  Operator Reference  - Details of operator internals, lifecycle, and best practices and optimizations.", 
            "title": "Operator Development Guide"
        }, 
        {
            "location": "/operator_development/#apache-apex-operators", 
            "text": "", 
            "title": "Apache Apex Operators "
        }, 
        {
            "location": "/operator_development/#operators-what-in-a-nutshell", 
            "text": "Operators are independent units of logical operations which can\ncontribute in executing the business logic of a use case. For example,\nin an ETL workflow, a filtering operation can be represented by a single\noperator. This filtering operator will be responsible for doing just one\ntask in the ETL pipeline, i.e. filter incoming tuples. Operators do not\nimpose any restrictions on what can or cannot be done as part of a\noperator. An operator may as well contain the entire business logic.\nHowever, it is recommended, that the operators are light weight\nindependent tasks, in\norder to take advantage of the distributed framework that Apache Apex\nprovides.\u00a0The structure of a streaming application shares resemblance\nwith the way CPU pipelining works. CPU pipelining breaks down the\ncomputation engine into different stages viz. instruction fetch,\ninstruction decode, etc. so that each of them can perform their task on\ndifferent instructions\nparallely. Similarly,\nApache Apex APIs allow the user to break down their tasks into different\nstages so that all of the tasks can be executed on different tuples\nparallely.", 
            "title": "Operators - \u201cWhat\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#operators-how-in-a-nutshell", 
            "text": "An Apache Apex application runs as a YARN application. Hence, each of\nthe operators that the application DAG contains, runs in one of the\ncontainers provisioned by YARN.\u00a0Further, Apache Apex exposes APIs to\nallow the user to request bundling multiple operators in a single node,\na single container or even a single thread. We shall look at these calls\nin the reference sections [cite reference sections]. For now, consider\nan operator as some piece of code that runs on some machine of a YARN\ncluster.", 
            "title": "Operators - \u201cHow\u201d in a nutshell"
        }, 
        {
            "location": "/operator_development/#types-of-operators", 
            "text": "An operator works on one tuple at a time. These tuples may be supplied\nby some other operator in the application or by some external source,\nlike a database or a message bus. Similarly, after the tuples are\nprocessed, these may be passed on to some other operator, or may be\nstored into some external system. Based on the functions that the\nOperator performs, we have the following types of operators:   Input Adapter  - This is one of the starting points in\n    the\u00a0application DAG and is responsible for getting tuples from an\n    external system. At the same time, such data may also be generated\n    by the operator itself, without interacting with the outside\n    world.\u00a0These input tuples will form the initial universe of\n    data\u00a0that the application works on.  Generic Operator  - This type of operator accepts input tuples from\n    the previous operators and passes\u00a0them on to the following operators\n    in the DAG.  Output Adapter  - This is one of the ending points in the application\n    DAG and is responsible for writing the data out to some external\n    system.   Note: There can be multiple operators of all types in an application\nDAG.", 
            "title": "Types of Operators"
        }, 
        {
            "location": "/operator_development/#positioning-of-operators-in-the-dag", 
            "text": "We may refer to operators depending on their positioning with respect to\none another. For any operator opr, we have the following types of\noperators.   Upstream operators  - These are the operators from which there is a\n    directed path to opr\u00a0in the application DAG.  Downstream operators  - These are the operators to which there is a\n    directed path from opr\u00a0in the application DAG.   Note that there are no cycles formed in the application\u00a0DAG.", 
            "title": "Positioning of Operators in the DAG"
        }, 
        {
            "location": "/operator_development/#ports", 
            "text": "The operators in the DAG are connected together via directed flows\ncalled streams. Each\u00a0stream\u00a0has end-points located on the operators\ncalled ports. The ports again fall into two types.   Input Port  - This is a\u00a0port through which an operator accepts input\n    tuples\u00a0from an upstream operator.  Output port  - This is a\u00a0port through which an operator passes on the\n    processed data to downstream operators.   Looking at the number of input ports, an Input Adapter is an operator\nwith no input ports, a Generic operator has both input and output ports,\nwhile an Output Adapter has no output ports. At the same time, note that\nan operator may act as an Input Adapter while at the same time have an\ninput port. In such cases, the operator is getting data from two\ndifferent sources, viz.\u00a0the input stream from the input port and an\nexternal source.", 
            "title": "Ports"
        }, 
        {
            "location": "/operator_development/#working-of-an-operator", 
            "text": "An operator passes through various stages during its lifetime. Each\nstage is an API call that the streaming application master makes for an\noperator. \u00a0The following figure illustrates the stages through which an\noperator passes.    The  setup()  call initializes the operator and prepares itself to\n    start processing tuples.  The  beginWindow()  call marks the beginning\u00a0of an\u00a0application window\n    and allows for any processing to be done before a window starts.  The  process()  call belongs to the  InputPort  and gets triggered when\n    any tuple arrives at the Input port of the operator. This call is\n    specific only to Generic and Output adapters, since Input Adapters\n    do not have an input port. This is made for all the tuples at the\n    input port until the end window marker tuple is received on the\n    input port.  The  emitTuples()  is the counterpart of  process()  call for Input\n    Adapters.\n    This call is used by Input adapters to emit any tuples that are\n    fetched from the external systems, or generated by the operator.\n    This method is called continuously until the pre-configured window\n    time is elapsed, at which the end window marker tuple is sent out on\n    the output port.  The  endWindow()  call marks the end of the window and allows for any\n    processing to be done after the window ends.  The  teardown()  call is used for gracefully shutting down the\n    operator and releasing any resources held by the operator.", 
            "title": "Working of an Operator"
        }, 
        {
            "location": "/operator_development/#writing-custom-operators", 
            "text": "", 
            "title": "Writing Custom Operators "
        }, 
        {
            "location": "/operator_development/#about-this-tutorial", 
            "text": "This tutorial will guide the user towards developing a operator from\nscratch. It will include all aspects of writing an operator including\ndesign, code as well as unit tests.", 
            "title": "About this tutorial"
        }, 
        {
            "location": "/operator_development/#introduction", 
            "text": "In this tutorial, we will design and write, from scratch, an operator\ncalled Word Count. This operator will accept tuples of type String,\ncount the number of occurrences for each word appearing in the tuple and\nsend out the updated counts for all the words encountered in the tuple.\nFurther, the operator will also accept a file path on HDFS which will\ncontain the stop-words which need to be ignored when counting\noccurrences.", 
            "title": "Introduction"
        }, 
        {
            "location": "/operator_development/#design", 
            "text": "Design of the operator must be finalized before starting to write an\noperator. Many aspects including the functionality, the data sources,\nthe types involved etc. need to be first finalized before writing the\noperator. Let us dive into each of these while considering the Word\nCount\u00a0operator.  Functionality  We can define the scope of operator functionality using the following\ntasks:   Parse the input tuple to identify the words in the tuple  Identify the stop-words in the tuple by looking up the stop-word\n    file as configured  For each non-stop-word in the tuple, count the occurrences in that\n    tuple and add it to a global counts   Let\u2019s consider an example. Suppose we have the following tuples flow\ninto the Word Count operator.   Humpty dumpty sat on a wall  Humpty dumpty had a great fall   Initially counts for all words\u00a0is 0. Once the first tuple is processed,\nthe counts that must be emitted are:  humpty - 1\ndumpty - 1\nsat - 1\nwall - 1  Note that we are ignoring the stop-words, \u201con\u201d and \u201ca\u201d in this case.\nAlso note that as a rule, we\u2019ll ignore the case of the words when\ncounting occurrences.  Similarly, after the second tuple is processed, the counts that must be\nemitted are:  humpty - 2\ndumpty - 2\ngreat - 1\nfall - 1  Again, we ignore the words  \u201chad\u201d  and  \u201ca\u201d  since these are stop-words.  Note that the most recent count for any word is correct count for that\nword. In other words, any new output for a word, invalidated all the\nprevious counts for that word.  Inputs  As seen from the example\u00a0above, the following\u00a0inputs are expected for\nthe operator:   Input stream whose tuple type is String  Input HDFS file path, pointing to a file containing stop-words   Only one input port is needed. The stop-word file will be small enough\nto be read completely in a single read. In addition this will be a one\ntime activity for the lifetime of the operator. This does not need a\nseparate input port.   Outputs  We can define the output for this operator in multiple ways.   The operator may send out the set of counts for which the counts\n    have changed after processing each tuple.  Some applications might not need an update after every tuple, but\n    only after\u00a0a certain time duration.   Let us try and implement both these options depending on the\nconfiguration. Let us define a\u00a0boolean configuration parameter \u201csendPerTuple\u201d . The value of this parameter will indicate whether the\nupdated counts for words need to be emitted after processing each\ntuple\u00a0(true)\u00a0or after a certain time duration (false).  The type of information the operator will be sending out on the output\nport is the same for all the cases. This will be a   key, value  \u00a0pair,\nwhere the key\u00a0is the word while, the value is the latest count for that\nword. This means we just need one output port on which this information\nwill go out.", 
            "title": "Design"
        }, 
        {
            "location": "/operator_development/#configuration", 
            "text": "We have the following configuration parameters:   stopWordFilePath \u00a0- This parameter will store the path to the stop\n    word file on HDFS as configured by the user.  sendPerTuple \u00a0- This parameter decides whether we send out the\n    updated counts after processing each tuple or at the end of a\n    window. When set to true, the operator will send out the updated\n    counts after each tuple, else it will send at the end of\n    each\u00a0window.", 
            "title": "Configuration"
        }, 
        {
            "location": "/operator_development/#code", 
            "text": "The source code for the tutorial can be found here:  https://github.com/DataTorrent/examples/tree/master/tutorials/operatorTutorial", 
            "title": "Code"
        }, 
        {
            "location": "/operator_development/#operator-reference", 
            "text": "The Operator Class  The operator will exist physically as a class which implements the\nOperator\u00a0interface. This interface will require implementations for the\nfollowing method calls:  -   setup(OperatorContext context)\n-   beginWindow(long windowId)\n-   endWindow()\n-   tearDown()  In order to simplify the creation of an operator, the Apache\u00a0Apex\nlibrary also provides a base class \u201cBaseOperator\u201d which has empty\nimplementations for these methods. Please refer to the  Getting\nStarted \u00a0section and the Reference \u00a0section for details on these.  We extend the class \u201cBaseOperator\u201d to create our own operator\n\u201cWordCountOperator\u201d.  public class WordCountOperator extends BaseOperator\n{\n}  Class (Operator) properties  We define the following class variables:   sendPerTuple \u00a0- Needed for configuring the frequency of output from\n    the operator   private boolean sendPerTuple = true; // default   stopWordFilePath \u00a0- Stores the path to the stop words file on HDFS   private String stopWordFilePath;\u00a0// no default   stopWords \u00a0- Stores the stop words read from the configured file   private transient String[] stopWords;   globalCounts \u00a0- A Map which stores the counts for all the words\n    encountered so far. Note that this variable is non transient, which\n    means that this variable is saved as part of the checkpointing state\n    and can be recovered in event of a crash.   private Map String, Long  globalCounts;   updatedCounts \u00a0- A map which stores the counts for only the most\n    recent tuple(s). Whether to store the most recent or the recent\n    window worth of tuples will be determined by the configuration\n    parameter sendPerTuple.   private transient Map String, Long  updatedCounts;   input  - The input port for the operator. The type of this input port\n    is String\u00a0which means it will only accept tuples of type String. The\n    definition of an input port requires implementation of a method\n    called process(String tuple), which should\u00a0have the processing logic\n    for the input tuple which \u00a0arrives at this input port. We delegate\n    this task to another method called processTuple(String tuple). This\n    helps in keeping the operator classes extensible by overriding the\n    processing logic for the input tuples.   public transient DefaultInputPort String  input = new \u00a0 \u00a0\nDefaultInputPort String ()\n{\n\u00a0\u00a0\u00a0\u00a0@Override\n\u00a0\u00a0\u00a0\u00a0public void process(String tuple)\n\u00a0\u00a0\u00a0\u00a0{\n    \u00a0\u00a0\u00a0\u00a0processTuple(tuple);\n\u00a0\u00a0\u00a0\u00a0}\n};   output - The output port for the operator. The type of this port is\n    Entry   String, Long  , which means the operator will emit   word,\n    count   pairs for the updated counts.   public transient DefaultOutputPort  Entry String, Long  output = new\nDefaultOutputPort Entry String,Long ();  The Constructor  The constructor is the place where we initialize the non-transient data\nstructures,\u00a0since\nconstructor is called just once per activation of an operator. In case\nof Word Count\u00a0operator, we initialize the globalCounts variable in the\nconstructor.  globalCounts = Maps.newHashMap();  Setup call  The setup method is called only once during the lifetime of an operator.\nThe purpose of the setup call is to allow the operator to set itself up\nfor processing incoming streams. Transient objects in the operator are\nnot serialized and checkpointed. Hence it is essential that such objects\nmust be initialized in the setup call. In case of operator failure, the\noperator will be redeployed, most likely on a different container. In\nthis case, it is the setup method which will be called by the Apache\nApex engine to allow the operator to prepare for execution in the new\ncontainer.  We perform the following tasks as part of the setup call:   Read the stop-word list from HDFS and store it in the\n    stopWords\u00a0array  Initialize updatedCounts\u00a0variable. This will store the updated\n    counts for words in most recent tuples processed by the operator.\n    This is a transient variable, hence the value of this variable will\n    be lost in case of operator failure.   Begin Window call  The begin window call signals the start of an application window. In\ncase of Word Count Operator, if the sendPerTuple\u00a0is set to false, it\nmeans that we are expecting updated counts for the most recent window of\ndata. Hence, we clear the updatedCounts\u00a0variable in the begin window\ncall and start accumulating the counts till the end window call.  Process Tuple call  The processTuple\u00a0method is called by the process\u00a0method of the input\nport, input. This method defines the processing logic for the current\ntuple that is received at the input port. As part of this method, we\nidentify the words in the current tuple and update the globalCounts\u00a0and\nthe updatedCounts\u00a0variables. In addition, if the sendPerTuple\u00a0variable\nis set to true, we also emit the words\u00a0and corresponding counts in\nupdatedCounts\u00a0to the output port. Note\u00a0that in this case (sendPerTuple =\ntrue), we clear the updatedCounts\u00a0variable in every call to\nprocessTuple.  End Window call  This call signals the end of an application window. In case of Word\nCount Operator, we emit the updatedCounts\u00a0to the output port if the\nsendPerTuple\u00a0flag is set to false.  Teardown call  This method allows the operator to gracefully shut down itself after\nreleasing the resources that it has acquired. In case of our operator,\nwe call the shutDown\u00a0method which shuts down the operator along with any\ndownstream operators.", 
            "title": "Operator Reference "
        }, 
        {
            "location": "/operator_development/#testing-your-operator", 
            "text": "Testing an operator after development is essential to ensure that he\nrequired functionality is indeed correctly implemented. As part of\ntesting our operator, we test the following two facets:   Test output of the operator after processing a single tuple  Test output of the operator after processing of a window of tuples   The unit tests for the WordCount operator are available in the class\nWordCountOperatorTest.java. We simulate the behavior of the engine by\nusing the test utilities provided by Apache Apex libraries. We simulate\nthe setup, beginWindow, process\u00a0method of the input port and\nendWindow\u00a0calls and compare the output received at the simulated output\nports.   Invoke constructor; non-transients initialized.  Copy state from checkpoint -- initialized values from step 1 are\nreplaced.", 
            "title": "Testing your Operator"
        }, 
        {
            "location": "/operators/io/file_splitter/", 
            "text": "File Splitter\n\n\nThis is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits. \n\n\nWhy is it needed?\n\n\nIt is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.\n\n\nClass Diagram\n\n\n\n\nAbstractFileSplitter\n\n\nThe abstract implementation defines the logic of processing \nFileInfo\n. This comprises the following tasks -  \n\n\n\n\n\n\nbuilding \nFileMetadata\n per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.\n\n\n\n\n\n\ncreating \nBlockMetadataIterator\n from \nFileMetadata\n. The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.\n\n\n\n\n\n\nretrieving \nBlockMetadata.FileBlockMetadata\n from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by \nblocksThreshold\n setting which by default is 1.  \n\n\n\n\n\n\nThe main utility method that performs all the above tasks is the \nprocess()\n method. Concrete implementations can invoke this method whenever they have data to process.\n\n\nPorts\n\n\nDeclares only output ports on which file metadata and block metadata are emitted.\n\n\n\n\nfilesMetadataOutput: metadata for each file is emitted on this port. \n\n\nblocksMetadataOutput: metadata for each block is emitted on this port. \n\n\n\n\nprocess()\n method\n\n\nWhen process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the \nblocksThreshold\n is reached or there are no more new files.\n\n\n  protected void process()\n  {\n    if (blockMetadataIterator != null \n blockCount \n blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount \n blocksThreshold \n (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }\n\n\n\n\nAbstract methods\n\n\n\n\n\n\nFileInfo getFileInfo()\n: called from within the \nprocess()\n and provides the next file to process.\n\n\n\n\n\n\nlong getDefaultBlockSize()\n: provides the block size which is used when user hasn't configured the size.\n\n\n\n\n\n\nFileStatus getFileStatus(Path path)\n: provides the \norg.apache.hadoop.fs.FileStatus\n instance for a path.   \n\n\n\n\n\n\nConfiguration\n\n\n\n\nblockSize\n: size of a block.\n\n\nblocksThreshold\n: threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.\n\n\n\n\nFileSplitterBase\n\n\nSimple operator that receives tuples of type \nFileInfo\n on its \ninput\n port. \nFileInfo\n contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.\n\n\n\nThe upstream operator emits tuples of type \nFileInfo\n on its output port which is connected to splitter input port. The downstream receives tuples of type \nBlockMetadata.FileBlockMetadata\n from the splitter's block metadata output port.\n\n\npublic class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator(\nInput\n, new JMSInput());\n    FileSplitterBase splitter = dag.addOperator(\nSplitter\n, new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator(\nBlockReader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nfile-info\n, input.output, splitter.input);\n    dag.addStream(\nblock-metadata\n, splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator\nAbstractFileSplitter.FileInfo\n\n  {\n\n    public final transient DefaultOutputPort\nAbstractFileSplitter.FileInfo\n output = new DefaultOutputPort\n();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}\n\n\n\n\nPorts\n\n\nDeclares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.\n\n\n\n\ninput: non optional port on which tuples of type \nFileInfo\n are received.\n\n\n\n\nConfiguration\n\n\n\n\nfile\n: path of the file from which the filesystem is inferred. FileSplitter creates an instance of \norg.apache.hadoop.fs.FileSystem\n which is why this path is needed.  \n\n\n\n\nFileSystem.newInstance(new Path(file).toUri(), new Configuration());\n\n\n\n\nThe fs instance is then used to fetch the default block size and \norg.apache.hadoop.fs.FileStatus\n for each file path.\n\n\nFileSplitterInput\n\n\nThis is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by \nTimeBasedDirectoryScanner\n. The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.\n\n\n\n\nExample application\n\n\nThis is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.\n\n\n\n\nSplitter is the input operator here that sends block metadata to the downstream BlockReader.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nInput\n, new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator(\nBlock Reader\n, new FSSliceReader());\n    ...\n    dag.addStream(\nblock-metadata\n, input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }\n\n\n\n\n\nPorts\n\n\nSince it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.\n\n\nConfiguration\n\n\n\n\nscanner\n: the component that scans directories asynchronously. It is of type \ncom.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner\n. The basic implementation of TimeBasedDirectoryScanner can be customized by users.  \n\n\n\n\na. \nfiles\n: comma separated list of directories to scan.  \n\n\nb. \nrecursive\n: flag that controls whether the directories should be scanned recursively.  \n\n\nc. \nscanIntervalMillis\n: interval specified in milliseconds after which another scan iteration is triggered.  \n\n\nd. \nfilePatternRegularExp\n: regular expression for accepted file names.  \n\n\ne. \ntrigger\n: a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2. \nidempotentStorageManager\n: by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of \ncom.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager\n.\n\n\nHandling of split records\n\n\nSplitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.\n\n\nWe have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers- \nAbstractFSLineReader\n and \nAbstractFSReadAheadLineReader\n can be found here \nAbstractFSBlockReader\n.", 
            "title": "File splitter"
        }, 
        {
            "location": "/operators/io/file_splitter/#file-splitter", 
            "text": "This is a simple operator whose main function is to split a file virtually and create metadata describing the files and the splits.", 
            "title": "File Splitter"
        }, 
        {
            "location": "/operators/io/file_splitter/#why-is-it-needed", 
            "text": "It is a common operation to read a file and parse it. This operation can be parallelized by having multiple partitions of such operators and each partition operating on different files. However, at times when a file is large then a single partition reading it can become a bottleneck.\nIn these cases, throughput can be increased if instances of the partitioned operator can read and parse non-overlapping sets of file blocks. This is where file splitter comes in handy. It creates metadata of blocks of file which serves as tasks handed out to downstream operator partitions. \nThe downstream partitions can read/parse the block without the need of interacting with other partitions.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/io/file_splitter/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/io/file_splitter/#abstractfilesplitter", 
            "text": "The abstract implementation defines the logic of processing  FileInfo . This comprises the following tasks -      building  FileMetadata  per file and emitting it. This metadata contains the file information such as filepath, no. of blocks in it, length of the file, all the block ids, etc.    creating  BlockMetadataIterator  from  FileMetadata . The iterator lazy-loads the block metadata when needed. We use an iterator because the no. of blocks in a file can be huge if the block size is small and loading all of them at once in memory may cause out of memory errors.    retrieving  BlockMetadata.FileBlockMetadata  from the block metadata iterator and emitting it. The FileBlockMetadata contains the block id, start offset of the block, length of file in the block, etc. The number of block metadata emitted per window are controlled by  blocksThreshold  setting which by default is 1.      The main utility method that performs all the above tasks is the  process()  method. Concrete implementations can invoke this method whenever they have data to process.  Ports  Declares only output ports on which file metadata and block metadata are emitted.   filesMetadataOutput: metadata for each file is emitted on this port.   blocksMetadataOutput: metadata for each block is emitted on this port.    process()  method  When process() is invoked, any pending blocks from the current file are emitted on the 'blocksMetadataOutput' port. If the threshold for blocks per window is still not met then a new input file is processed - corresponding metadata is emitted on 'filesMetadataOutput' and more of its blocks are emitted. This operation is repeated until the  blocksThreshold  is reached or there are no more new files.    protected void process()\n  {\n    if (blockMetadataIterator != null   blockCount   blocksThreshold) {\n      emitBlockMetadata();\n    }\n\n    FileInfo fileInfo;\n    while (blockCount   blocksThreshold   (fileInfo = getFileInfo()) != null) {\n      if (!processFileInfo(fileInfo)) {\n        break;\n      }\n    }\n  }  Abstract methods    FileInfo getFileInfo() : called from within the  process()  and provides the next file to process.    long getDefaultBlockSize() : provides the block size which is used when user hasn't configured the size.    FileStatus getFileStatus(Path path) : provides the  org.apache.hadoop.fs.FileStatus  instance for a path.       Configuration   blockSize : size of a block.  blocksThreshold : threshold on the number of blocks emitted by file splitter every window. This setting is used for throttling the work for downstream operators.", 
            "title": "AbstractFileSplitter"
        }, 
        {
            "location": "/operators/io/file_splitter/#filesplitterbase", 
            "text": "Simple operator that receives tuples of type  FileInfo  on its  input  port.  FileInfo  contains the information (currently just the file path) about the file which this operator uses to create file metadata and block metadata.  Example application  This is a simple sub-dag that demonstrates how FileSplitterBase can be plugged into an application.  The upstream operator emits tuples of type  FileInfo  on its output port which is connected to splitter input port. The downstream receives tuples of type  BlockMetadata.FileBlockMetadata  from the splitter's block metadata output port.  public class ApplicationWithBaseSplitter implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    JMSInput input = dag.addOperator( Input , new JMSInput());\n    FileSplitterBase splitter = dag.addOperator( Splitter , new FileSplitterBase());\n    FSSliceReader blockReader = dag.addOperator( BlockReader , new FSSliceReader());\n    ...\n    dag.addStream( file-info , input.output, splitter.input);\n    dag.addStream( block-metadata , splitter.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    ...\n  }\n\n  public static class JMSInput extends AbstractJMSInputOperator AbstractFileSplitter.FileInfo \n  {\n\n    public final transient DefaultOutputPort AbstractFileSplitter.FileInfo  output = new DefaultOutputPort ();\n\n    @Override\n    protected AbstractFileSplitter.FileInfo convert(Message message) throws JMSException\n    {\n      //assuming the message is a text message containing the absolute path of the file.\n      return new AbstractFileSplitter.FileInfo(null, ((TextMessage)message).getText());\n    }\n\n    @Override\n    protected void emit(AbstractFileSplitter.FileInfo payload)\n    {\n      output.emit(payload);\n    }\n  }\n}  Ports  Declares an input port on which it receives tuples from the upstream operator. Output ports are inherited from AbstractFileSplitter.   input: non optional port on which tuples of type  FileInfo  are received.   Configuration   file : path of the file from which the filesystem is inferred. FileSplitter creates an instance of  org.apache.hadoop.fs.FileSystem  which is why this path is needed.     FileSystem.newInstance(new Path(file).toUri(), new Configuration());  The fs instance is then used to fetch the default block size and  org.apache.hadoop.fs.FileStatus  for each file path.", 
            "title": "FileSplitterBase"
        }, 
        {
            "location": "/operators/io/file_splitter/#filesplitterinput", 
            "text": "This is an input operator that discovers files itself. The scanning of the directories for new files is asynchronous which is handled by  TimeBasedDirectoryScanner . The function of TimeBasedDirectoryScanner is to periodically scan specified directories and find files which were newly added or modified. The interaction between the operator and the scanner is depicted in the diagram below.   Example application  This is a simple sub-dag that demonstrates how FileSplitterInput can be plugged into an application.   Splitter is the input operator here that sends block metadata to the downstream BlockReader.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator( Input , new FileSplitterInput());\n    FSSliceReader reader = dag.addOperator( Block Reader , new FSSliceReader());\n    ...\n    dag.addStream( block-metadata , input.blocksMetadataOutput, reader.blocksMetadataInput);\n    ...\n  }  Ports  Since it is an input operator there are no input ports and output ports are inherited from AbstractFileSplitter.  Configuration   scanner : the component that scans directories asynchronously. It is of type  com.datatorrent.lib.io.fs.FileSplitter.TimeBasedDirectoryScanner . The basic implementation of TimeBasedDirectoryScanner can be customized by users.     a.  files : comma separated list of directories to scan.    b.  recursive : flag that controls whether the directories should be scanned recursively.    c.  scanIntervalMillis : interval specified in milliseconds after which another scan iteration is triggered.    d.  filePatternRegularExp : regular expression for accepted file names.    e.  trigger : a flag that triggers a scan iteration instantly. If the scanner thread is idling then it will initiate a scan immediately otherwise if a scan is in progress, then the new iteration will be triggered immediately after the completion of current one.\n2.  idempotentStorageManager : by default FileSplitterInput is idempotent. \nIdempotency ensures that the operator will process the same set of files/blocks in a window if it has seen that window previously, i.e., before a failure. For example, let's say the operator completed window 10 and failed somewhere between window 11. If the operator gets restored at window 10 then it will process the same file/block again in window 10 which it did in the previous run before the failure. Idempotency is important but comes with higher cost because at the end of each window the operator needs to persist some state with respect to that window. Therefore, if one doesn't care about idempotency then they can set this property to be an instance of  com.datatorrent.lib.io.IdempotentStorageManager.NoopIdempotentStorageManager .", 
            "title": "FileSplitterInput"
        }, 
        {
            "location": "/operators/io/file_splitter/#handling-of-split-records", 
            "text": "Splitting of files to create tasks for downstream operator needs to be a simple operation that doesn't consume a lot of resources and is fast. This is why the file splitter doesn't open files to read. The downside of that is if the file contains records then a record may split across adjacent blocks. Handling of this is left to the downstream operator.  We have created Block readers in Apex-malhar library that handle line splits efficiently. The 2 line readers-  AbstractFSLineReader  and  AbstractFSReadAheadLineReader  can be found here  AbstractFSBlockReader .", 
            "title": "Handling of split records"
        }, 
        {
            "location": "/operators/io/block_reader/", 
            "text": "Block Reader\n\n\nThis is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block. \n\n\nWhy is it needed?\n\n\nA Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see \nAbstractFileInputOperator\n) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.\n\n\nClass Diagram\n\n\n\n\nAbstractBlockReader\n\n\nThis is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.\n\n\n\n\nPorts\n\n\n\n\n\n\nblocksMetadataInput: input port on which block metadata are received.\n\n\n\n\n\n\nblocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.\n\n\n\n\n\n\nmessages: output port on which tuples of type \ncom.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord\n are emitted. This class encapsulates a \nrecord\n and the \nblockId\n of the corresponding block.\n\n\n\n\n\n\nreaderContext\n\n\nThis is one of the most important fields in the block reader. It is of type \ncom.datatorrent.lib.io.block.ReaderContext\n and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.\n\n\nOnce the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking \nreaderContext.initialize(stream, blockMetadata, consecutiveBlock);\n. Initialize method is where any implementation of \nReaderContext\n can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.\n\n\nOnce the initialization is done, \nreaderContext.next()\n is called repeatedly until it returns \nnull\n. It is left to the \nReaderContext\n implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples- \nLineReaderContext\n and \nReadAheadLineReaderContext\n). In other cases when there isn't a possibility of split record (example- \nFixedBytesReaderContext\n), it returns \nnull\n immediately when the block boundary is reached. The return type of \nreaderContext.next()\n is of type \ncom.datatorrent.lib.io.block.ReaderContext.Entity\n which is just a wrapper for a \nbyte[]\n that represents the record and total bytes used in fetching the record.\n\n\nAbstract methods\n\n\n\n\n\n\nSTREAM setupStream(B block)\n: creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.\n\n\n\n\n\n\nR convertToRecord(byte[] bytes)\n: this converts the array of bytes into the actual instance of record type.\n\n\n\n\n\n\nAuto-scalability\n\n\nBlock reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the \nblocksMetadataInput\n port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the \npartitioner and stats-listener\n.\n\n\nConfiguration\n\n\n\n\nmaxReaders\n: when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.\n\n\nminReaders\n: when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.\n\n\ncollectStats\n: this enables or disables auto-scaling. When it is set to \ntrue\n the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.\n\n\nintervalMillis\n: when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.\n\n\n\n\n AbstractFSBlockReader\n\n\nThis abstract implementation deals with files. Different types of file systems that are implementations of \norg.apache.hadoop.fs.FileSystem\n are supported. The user can override \ngetFSInstance()\n method to create an instance of a specific \nFileSystem\n. By default, filesystem instance is created from the filesytem URI that comes from the default hadoop configuration.\n\n\nprotected FileSystem getFSInstance() throws IOException\n{\n  return FileSystem.newInstance(configuration);\n}\n\n\n\n\nIt uses this filesystem instance to setup a stream of type \norg.apache.hadoop.fs.FSDataInputStream\n to read the block.\n\n\n@Override\nprotected FSDataInputStream setupStream(BlockMetadata.FileBlockMetadata block) throws IOException\n{\n  return fs.open(new Path(block.getFilePath()));\n}\n\n\n\n\nAll the ports and configurations are derived from the super class. It doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n method which is delegated to concrete sub-classes.\n\n\nExample Application\n\n\nThis simple dag demonstrates how any concrete implementation of \nAbstractFSBlockReader\n can be plugged into an application. \n\n\n\n\nIn the above application, file splitter creates block metadata for files which are sent to block reader. Partitions of the block reader parses the file blocks for records which are filtered, transformed and then persisted to a file (created per block). Therefore block reader is parallel partitioned with the 2 downstream operators - filter/converter and record output operator. The code which implements this dag is below.\n\n\npublic class ExampleApplication implements StreamingApplication\n{\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    FileSplitterInput input = dag.addOperator(\nFile-splitter\n, new FileSplitterInput());\n    //any concrete implementation of AbstractFSBlockReader based on the use-case can be added here.\n    LineReader blockReader = dag.addOperator(\nBlock-reader\n, new LineReader());\n    Filter filter = dag.addOperator(\nFilter\n, new Filter());\n    RecordOutputOperator recordOutputOperator = dag.addOperator(\nRecord-writer\n, new RecordOutputOperator());\n\n    dag.addStream(\nfile-block metadata\n, input.blocksMetadataOutput, blockReader.blocksMetadataInput);\n    dag.addStream(\nrecords\n, blockReader.messages, filter.input);\n    dag.addStream(\nfiltered-records\n, filter.output, recordOutputOperator.input);\n  }\n\n  /**\n   * Concrete implementation of {@link AbstractFSBlockReader} for which a record is a line in the file.\n   */\n  public static class LineReader extends AbstractFSBlockReader.AbstractFSReadAheadLineReader\nString\n\n  {\n\n    @Override\n    protected String convertToRecord(byte[] bytes)\n    {\n      return new String(bytes);\n    }\n  }\n\n  /**\n   * Considers any line starting with a '.' as invalid. Emits the valid records.\n   */\n  public static class Filter extends BaseOperator\n  {\n    public final transient DefaultOutputPort\nAbstractBlockReader.ReaderRecord\nString\n output = new DefaultOutputPort\n();\n    public final transient DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n input = new DefaultInputPort\nAbstractBlockReader.ReaderRecord\nString\n()\n    {\n      @Override\n      public void process(AbstractBlockReader.ReaderRecord\nString\n stringRecord)\n      {\n        //filter records and transform\n        //if the string starts with a '.' ignore the string.\n        if (!StringUtils.startsWith(stringRecord.getRecord(), \n.\n)) {\n          output.emit(stringRecord);\n        }\n      }\n    };\n  }\n\n  /**\n   * Persists the valid records to corresponding block files.\n   */\n  public static class RecordOutputOperator extends AbstractFileOutputOperator\nAbstractBlockReader.ReaderRecord\nString\n\n  {\n    @Override\n    protected String getFileName(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return Long.toHexString(tuple.getBlockId());\n    }\n\n    @Override\n    protected byte[] getBytesForTuple(AbstractBlockReader.ReaderRecord\nString\n tuple)\n    {\n      return tuple.getRecord().getBytes();\n    }\n  }\n}\n\n\n\n\nConfiguration to parallel partition block reader with its downstream operators.\n\n\n  \nproperty\n\n    \nname\ndt.operator.Filter.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.operator.Record-writer.port.input.attr.PARTITION_PARALLEL\n/name\n\n    \nvalue\ntrue\n/value\n\n  \n/property\n\n\n\n\n\nAbstractFSReadAheadLineReader\n\n\nThis extension of \nAbstractFSBlockReader\n parses lines from a block and binds the \nreaderContext\n field to an instance of \nReaderContext.ReadAheadLineReaderContext\n.\n\n\nIt is abstract because it doesn't provide an implementation of \nconvertToRecord(byte[] bytes)\n since the user may want to convert the bytes that make a line into some other type. \n\n\nReadAheadLineReaderContext\n\n\nIn order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.\n\n\nThis is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.\n\n\nAbstractFSLineReader\n\n\nSimilar to \nAbstractFSReadAheadLineReader\n, even this parses lines from a block. However, it binds the \nreaderContext\n field to an instance of \nReaderContext.LineReaderContext\n.\n\n\nLineReaderContext\n\n\nThis handles the line split differently from \nReadAheadLineReaderContext\n. It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.\n\n\nWhen the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.\n\n\nIf the validations of completeness fails for a line then \nconvertToRecord(byte[] bytes)\n should return null.\n\n\nFSSliceReader\n\n\nA concrete extension of \nAbstractFSBlockReader\n that reads fixed-size \nbyte[]\n from a block and emits the byte array wrapped in \ncom.datatorrent.netlet.util.Slice\n.\n\n\nThis operator binds the \nreaderContext\n to an instance of \nReaderContext.FixedBytesReaderContext\n.\n\n\nFixedBytesReaderContext\n\n\nThis implementation of \nReaderContext\n never reads beyond a block boundary which can result in the last \nbyte[]\n of a block to be of a shorter length than the rest of the records.\n\n\nConfiguration\n\n\nreaderContext.length\n: length of each record. By default, this is initialized to the default hdfs block size.\n\n\nPartitioner and StatsListener\n\n\nThe logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute - \nPARTITIONER\n) as well as a StatsListener. This is because the \n\nAbstractBlockReader\n implements both the \ncom.datatorrent.api.Partitioner\n and \ncom.datatorrent.api.StatsListener\n interfaces and provides an implementation of \ndefinePartitions(...)\n and \nprocessStats(...)\n which make it auto-scalable.\n\n\nprocessStats \n\n\nThe application master invokes \nResponse processStats(BatchedOperatorStats stats)\n method on the logical instance with the stats (\ntuplesProcessedPSMA\n, \ntuplesEmittedPSMA\n, \nlatencyMA\n, etc.) of each partition. The data which this operator is interested in is the \nqueueSize\n of the input port \nblocksMetadataInput\n.\n\n\nUsually the \nqueueSize\n of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with \n@DataQueueSize\n. In this case \nAbstractBlockReader\n itself is the \nStatsListener\n which is why it is annotated with \n@DataQueueSize\n.\n\n\nThe logical instance caches the queue size per partition and at regular intervals (configured by \nintervalMillis\n) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.\n\n\n\n\nThe goal of this logic is to create as many partitions within bounds (see \nmaxReaders\n and \nminReaders\n above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.\n\n\ndefinePartitions\n\n\nBased on the \nrepartitionRequired\n field of the \nResponse\n object which is returned by \nprocessStats\n method, the application master invokes \n\n\nCollection\nPartition\nAbstractBlockReader\n...\n definePartitions(Collection\nPartition\nAbstractBlockReader\n...\n partitions, PartitioningContext context)\n\n\n\n\non the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created. \n\n\nPlease note auto-scaling can be disabled by setting \ncollectStats\n to \nfalse\n. If the use-case requires only static partitioning, then that can be achieved by setting \nStatelessPartitioner\n as the operator attribute- \nPARTITIONER\n on the block reader.", 
            "title": "Block reader"
        }, 
        {
            "location": "/operators/io/block_reader/#block-reader", 
            "text": "This is a scalable operator that reads and parses blocks of data sources into records. A data source can be a file or a message bus that contains records and a block defines a chunk of data in the source by specifying the block offset and the length of the source belonging to the block.", 
            "title": "Block Reader"
        }, 
        {
            "location": "/operators/io/block_reader/#why-is-it-needed", 
            "text": "A Block Reader is needed to parallelize reading and parsing of a single data source, for example a file. Simple parallelism of reading data sources can be achieved by multiple partitions reading different source of same type (for files see  AbstractFileInputOperator ) but Block Reader partitions can read blocks of same source in parallel and parse them for records ensuring that no record is duplicated or missed.", 
            "title": "Why is it needed?"
        }, 
        {
            "location": "/operators/io/block_reader/#class-diagram", 
            "text": "", 
            "title": "Class Diagram"
        }, 
        {
            "location": "/operators/io/block_reader/#abstractblockreader", 
            "text": "This is the abstract implementation that serves as the base for different types of data sources. It defines how a block metadata is processed. The flow diagram below describes the processing of a block metadata.   Ports    blocksMetadataInput: input port on which block metadata are received.    blocksMetadataOutput: output port on which block metadata are emitted if the port is connected. This port is useful when a downstream operator that receives records from block reader may also be interested to know the details of the corresponding blocks.    messages: output port on which tuples of type  com.datatorrent.lib.io.block.AbstractBlockReader.ReaderRecord  are emitted. This class encapsulates a  record  and the  blockId  of the corresponding block.    readerContext  This is one of the most important fields in the block reader. It is of type  com.datatorrent.lib.io.block.ReaderContext  and is responsible for fetching bytes that make a record. It also lets the reader know how many total bytes were consumed which may not be equal to the total bytes in a record because consumed bytes also include bytes for the record delimiter which may not be a part of the actual record.  Once the reader creates an input stream for the block (or uses the previous opened stream if the current block is successor of the previous block) it initializes the reader context by invoking  readerContext.initialize(stream, blockMetadata, consecutiveBlock); . Initialize method is where any implementation of  ReaderContext  can perform all the operations which have to be executed just before reading the block or create states which are used during the lifetime of reading the block.  Once the initialization is done,  readerContext.next()  is called repeatedly until it returns  null . It is left to the  ReaderContext  implementations to decide when a block is completely processed. In cases when a record is split across adjacent blocks, reader context may decide to read ahead of the current block boundary to completely fetch the split record (examples-  LineReaderContext  and  ReadAheadLineReaderContext ). In other cases when there isn't a possibility of split record (example-  FixedBytesReaderContext ), it returns  null  immediately when the block boundary is reached. The return type of  readerContext.next()  is of type  com.datatorrent.lib.io.block.ReaderContext.Entity  which is just a wrapper for a  byte[]  that represents the record and total bytes used in fetching the record.  Abstract methods    STREAM setupStream(B block) : creating a stream for a block is dependent on the type of source which is not known to AbstractBlockReader. Sub-classes which deal with a specific data source provide this implementation.    R convertToRecord(byte[] bytes) : this converts the array of bytes into the actual instance of record type.    Auto-scalability  Block reader can auto-scale, that is, depending on the backlog (total number of all the blocks which are waiting in the  blocksMetadataInput  port queue of all partitions) it can create more partitions or reduce them. Details are discussed in the last section which covers the  partitioner and stats-listener .  Configuration   maxReaders : when auto-scaling is enabled, this controls the maximum number of block reader partitions that can be created.  minReaders : when auto-scaling is enabled, this controls the minimum number of block reader partitions that should always exist.  collectStats : this enables or disables auto-scaling. When it is set to  true  the stats (number of blocks in the queue) are collected and this triggers partitioning; otherwise auto-scaling is disabled.  intervalMillis : when auto-scaling is enabled, this specifies the interval at which the reader will trigger the logic of computing the backlog and auto-scale.", 
            "title": "AbstractBlockReader"
        }, 
        {
            "location": "/operators/io/block_reader/#abstractfsreadaheadlinereader", 
            "text": "This extension of  AbstractFSBlockReader  parses lines from a block and binds the  readerContext  field to an instance of  ReaderContext.ReadAheadLineReaderContext .  It is abstract because it doesn't provide an implementation of  convertToRecord(byte[] bytes)  since the user may want to convert the bytes that make a line into some other type.   ReadAheadLineReaderContext  In order to handle a line split across adjacent blocks, ReadAheadLineReaderContext always reads beyond the block boundary and ignores the bytes till the first end-of-line character of all the blocks except the first block of the file. This ensures that no line is missed or incomplete.  This is one of the most common ways of handling a split record. It doesn't require any further information to decide if a line is complete. However, the cost of this consistent way to handle a line split is that it always reads from the next block.", 
            "title": "AbstractFSReadAheadLineReader"
        }, 
        {
            "location": "/operators/io/block_reader/#abstractfslinereader", 
            "text": "Similar to  AbstractFSReadAheadLineReader , even this parses lines from a block. However, it binds the  readerContext  field to an instance of  ReaderContext.LineReaderContext .  LineReaderContext  This handles the line split differently from  ReadAheadLineReaderContext . It doesn't always read from the next block. If the end of the last line is aligned with the block boundary then it stops processing the block. It does read from the next block when the boundaries are not aligned, that is, last line extends beyond the block boundary. The result of this is an inconsistency in reading the next block.  When the boundary of the last line of the previous block was aligned with its block, then the first line of the current block is a valid line. However, in the other case the bytes from the block start offset to the first end-of-line character should be ignored. Therefore, this means that any record formed by this reader context has to be validated. For example, if the lines are of fixed size then size of each record can be validated or if each line begins with a special field then that knowledge can be used to check if a record is complete.  If the validations of completeness fails for a line then  convertToRecord(byte[] bytes)  should return null.", 
            "title": "AbstractFSLineReader"
        }, 
        {
            "location": "/operators/io/block_reader/#fsslicereader", 
            "text": "A concrete extension of  AbstractFSBlockReader  that reads fixed-size  byte[]  from a block and emits the byte array wrapped in  com.datatorrent.netlet.util.Slice .  This operator binds the  readerContext  to an instance of  ReaderContext.FixedBytesReaderContext .  FixedBytesReaderContext  This implementation of  ReaderContext  never reads beyond a block boundary which can result in the last  byte[]  of a block to be of a shorter length than the rest of the records.  Configuration  readerContext.length : length of each record. By default, this is initialized to the default hdfs block size.", 
            "title": "FSSliceReader"
        }, 
        {
            "location": "/operators/io/block_reader/#partitioner-and-statslistener", 
            "text": "The logical instance of the block reader acts as the Partitioner (unless a custom partitioner is set using the operator attribute -  PARTITIONER ) as well as a StatsListener. This is because the  AbstractBlockReader  implements both the  com.datatorrent.api.Partitioner  and  com.datatorrent.api.StatsListener  interfaces and provides an implementation of  definePartitions(...)  and  processStats(...)  which make it auto-scalable.  processStats   The application master invokes  Response processStats(BatchedOperatorStats stats)  method on the logical instance with the stats ( tuplesProcessedPSMA ,  tuplesEmittedPSMA ,  latencyMA , etc.) of each partition. The data which this operator is interested in is the  queueSize  of the input port  blocksMetadataInput .  Usually the  queueSize  of an input port gives the count of waiting control tuples plus data tuples. However, if a stats listener is interested only in the count of data tuples then that can be expressed by annotating the class with  @DataQueueSize . In this case  AbstractBlockReader  itself is the  StatsListener  which is why it is annotated with  @DataQueueSize .  The logical instance caches the queue size per partition and at regular intervals (configured by  intervalMillis ) sums these values to find the total backlog which is then used to decide whether re-partitioning is needed. The flow-diagram below describes this logic.   The goal of this logic is to create as many partitions within bounds (see  maxReaders  and  minReaders  above) to quickly reduce this backlog or if the backlog is small then remove any idle partitions.  definePartitions  Based on the  repartitionRequired  field of the  Response  object which is returned by  processStats  method, the application master invokes   Collection Partition AbstractBlockReader ...  definePartitions(Collection Partition AbstractBlockReader ...  partitions, PartitioningContext context)  on the logical instance which is also the partitioner instance. The implementation calculates the difference between required partitions and the existing count of partitions. If this difference is negative, then equivalent number of partitions are removed otherwise new partitions are created.   Please note auto-scaling can be disabled by setting  collectStats  to  false . If the use-case requires only static partitioning, then that can be achieved by setting  StatelessPartitioner  as the operator attribute-  PARTITIONER  on the block reader.", 
            "title": "Partitioner and StatsListener"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "File Input, S3, NFS"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "File Output"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Deduper"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "HDHT"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "DimensionsStore"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "DimenstionComputation"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "KafkaInput"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "KafkaOutput"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Solace"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "JMS"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "HBase"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtgateway_api/", 
            "text": "DataTorrent dtGateway API v2 Specification\n\n\nREST API\n\n\nReturn codes\n\n\n\n\n200\n: OK\n\n\n400\n: The request is not in the format that the server expects\n\n\n404\n: The resource is not found\n\n\n500\n: Something is wrong on the server side\n\n\n\n\nREST URI Specification\n\n\nGET /ws/v2/about\n\n\nFunction:\n\n\nReturn:\n\n\n{\n    \nbuildVersion\n: \n{buildVersion}\n,\n    \nbuildDate\n: \n{date and time}\n,\n    \nbuildRevision\n: \n{revision}\n,\n    \nbuildUser\n: \n{user}\n,\n    \nversion\n: \n{version}\n,\n    \ngatewayUser\n: \n{user}\n,\n    \njavaVersion\n: \n{java_version}\n,\n    \nhadoopLocation\n: \n{hadoop_location}\n,\n    \njvmName\n: \n{pid}@{hostname}\n,\n    \nconfigDirectory\n: \n{configDir}\n,\n    \nhostname\n: \n{hostname}\n,\n    \nhadoopIsSecurityEnabled\n: \n{true/false}\n\n}\n\n\n\n\nGET /ws/v2/cluster/metrics\n\n\nFunction: List metrics that are relevant to the entire cluster\n\n\nReturn:\n\n\n{\n    \naverageAge\n: \n{average running application age in milliseconds}\n,\n    \ncpuPercentage\n: \n{cpuPercentage}\n,\n    \ncurrentMemoryAllocatedMB\n: \n{currentMemoryAllocatedMB}\n,\n    \nmaxMemoryAllocatedMB\n: \n{maxMemoryAllocatedMB}\n,\n    \nnumAppsFailed\n: \n{numAppsFailed}\n,\n    \nnumAppsFinished\n: \n{numAppsFinished}\n,\n    \nnumAppsKilled\n: \n{numAppsKilled}\n,\n    \nnumAppsPending\n: \n{numAppsPending}\n,\n    \nnumAppsRunning\n: \n{numAppsRunning}\n,\n    \nnumAppsSubmitted\n: \n{numAppsSubmitted}\n,\n    \nnumContainers\n: \n{numContainers}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications[?states={STATE_FILTER}\nname={NAME_FILTER}\nuser={USER_FILTER]\n\n\nFunction: List IDs of all streaming applications\n\n\nReturn:\n\n\n{\n    \napps\n: [\n        {\n            \ndiagnostics\n: \n{diagnostics}\n,\n            \nelapsedTime\n: \n{elapsedTime}\n,\n            \nfinalStatus\n: \n{finalStatus}\n,\n            \nfinishedTime\n: \n{finishedTime}\n,\n            \nid\n: \n{appId}\n,\n            \nname\n: \n{name}\n,\n            \nqueue\n: \n{queue}\n,\n            \nstartedTime\n: \n{startedTime}\n,\n            \nstate\n: \n{state}\n,\n            \ntrackingUrl\n: \n{trackingUrl}\n,\n            \nuser\n: \n{user}\n\n        },  \n        \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}\n\n\nFunction: Get the information for the specified application\n\n\nReturn:\n\n\n{\n    \nid\n: \n{appid}\n,\n    \nname\n: \n{name}\n,\n    \nstate\n: \n{state}\n,\n    \ntrackingUrl\n: \n{tracking url}\n,\n    \nfinalStatus\n: {finalStatus},\n    \nappPath\n: \n{appPath}\n,\n    \ngatewayAddress\n: \n{gatewayAddress}\n,\n    \nelapsedTime\n: \n{elapsedTime}\n,\n    \nstartedTime\n: \n{startTime}\n,\n    \nuser\n: \n{user}\n,\n    \nversion\n: \n{stram version}\n,\n    \nremainingLicensedMB\n: \n{remainingLicensedMB}\n,\n    \nallocatedMB\n: \n{allocatedMB}\n,\n    \ngatewayConnected\n: \ntrue/false\n,\n    \nconnectedToThisGateway\n: \ntrue/false\n,\n    \nattributes\n: {\n           \n{attributeName}\n: \n{attributeValue}\n, \n           \n{attributeName-n}\n: \n{attributeValue-n}\n, \n    },\n    \nstats\n: {\n        \nallocatedContainers\n: \n{allocatedContainer}\n,\n        \ntotalMemoryAllocated\n: \n{totalMemoryAllocated}\n,\n        \nlatency\n: \n{overall latency}\n,\n        \ncriticalPath\n: \n{list of operator id that represents the critical path}\n,\n        \nfailedContainers\n: \n{failedContainers}\n,\n        \nnumOperators\n: \n{numOperators}\n,\n        \nplannedContainers\n: \n{plannedContainers}\n,\n        \ncurrentWindowId\n: \n{min of operators:currentWindowId}\n,\n        \nrecoveryWindowId\n: \n{min of operators:recoveryWindowId}\n,\n        \ntuplesProcessedPSMA\n: \n{sum of operators:tuplesProcessedPSMA}\n,\n        \ntotalTuplesProcessed\n:\n{sum of operators:totalTuplesProcessed}\n,\n        \ntuplesEmittedPSMA\n:\n{sum of operators:tuplesEmittedPSMA}\n,\n        \ntotalTuplesEmitted\n:\n{sum of operators:totalTuplesEmitted}\n,\n        \ntotalBufferServerReadBytesPSMA\n: \n{totalBufferServerReadBytesPSMA}\n,\n        \ntotalBufferServerWriteBytesPSMA\n: \n{totalBufferServerWriteBytesPSMA}\n\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan\n\n\nFunction: Return the physical plan for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nisUnifier\n: true/false\n        },\n         \u2026\n     ],\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators\n\n\nFunction: Return list of operators for the given application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainer\n: \n{containerId}\n,\n            \ncounters\n: {\n                \n{counterName}\n: \n{counterValue}\n, \n                ...\n             },\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \nports\n: [\n                {\n                    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n                    \nname\n: \n{name}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n                    \ntype\n: \ninput/output\n,\n                    \nrecordingStartTime\n: \n{recordingStartTime}\n\n                },\n                ...\n            ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: \n{status}\n,\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nlogicalName\n: \n{logicalName}\n,\n            \nunifierClass\n: \n{unifierClass}\n\n        },\n         \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/streams\n\n\nFunction: Return physical streams\n\n\nReturn:\n\n\n{\n     \nstreams\n: [        \n        {\n            \nlogicalName\n: \n{logicalName}\n,\n            \nsinks\n: [\n                {\n                    \noperatorId\n: \n{operatorId}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorId\n: \n{operatorId}\n,\n                \nportName\n: \n{portName}\n\n            },\n            \nlocality\n: \n{locality}\n\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}\n\n\nFunction: Return information of the given operator for the given application\n\n\nReturn:\n\n\n{\n    \nclassName\n: \n{className}\n,\n    \ncontainer\n: \n{containerId}\n,\n    \ncounters\n: {\n      \n{counterName}: \n{counterValue}\n, ...            \n    }\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \ncurrentWindowId\n: \n{currentWindowId}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \nports\n: [\n       {\n          \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,\n          \nname\n: \n{name}\n,\n          \ntotalTuples\n: \n{totalTuples}\n,\n          \ntuplesPSMA\n: \n{tuplesPSMA}\n,\n          \ntype\n: \ninput/output\n\n       }, ...\n    ],\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,\n    \nname\n: \n{name}\n,\n    \nrecordingStartTime\n: \n{recordingStartTime}\n,\n    \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n    \nstatus\n: \n{status}\n,\n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory\n\n\nFunction: Return container deploy history of this operator\nSince: 1.0.6\n\n\nReturn:\n\n\n{\n   \ncontainers\n: [  \n        {  \n            \ncontainer\n: \n{containerId}\n,   \n            \nstartTime\n: \n{startTime}\n  \n        }, ...  \n    ],   \n    \nname\n: \n{operatorName}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports\n\n\nFunction: Get the information of all ports of the given operator of the\ngiven application\n\n\nReturn:\n\n\n{  \n    \nports\n: [\n        {  \n            \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n            \nname\n: \n{name}\n,\n            \nrecordingStartTime\n: \n{recordingStartTime}\n,  \n            \ntotalTuples\n: \n{totalTuples}\n,   \n            \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n            \ntype\n: \noutput\n  \n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}\n\n\nFunction: Get the information of a specified port\n\n\nReturn:\n\n\n{  \n    \nbufferServerBytesPSMA\n: \n{bufferServerBytesPSMA}\n,   \n    \nname\n: \n{name}\n,   \n    \ntotalTuples\n: \n{totalTuples}\n,   \n    \ntuplesPSMA\n: \n{tuplesPSMA}\n,   \n    \ntype\n: \n{type}\n  \n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses[?parent={parent}\nq={searchTerm}\npackagePrefixes={comma-separated-package-prefixes}]\n\n\nFunction: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        { \nname\n:\n{className}\n },\n       \u2026\n     ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}\n\n\nFunction: Get the description of the given operator class\n\n\nReturn:\n\n\n{\n    \ninputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n          ...\n    ],\n    \noutputPorts\n: [\n        {\n            \nname\n: \n{name}\n,\n            \noptional\n: {boolean}\n        },\n        \u2026\n    ],\n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/shutdown\n\n\nFunction: Shut down the application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/kill\n\n\nFunction: Kill the given application\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start\n\n\nFunction: Start recording on operator\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop\n\n\nFunction: Stop recording on operator\n\n\nPayload: none\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start\n\n\nFunction: Start recording on port\n\n\nPayload (optional):\n\n\n{\n   \nnumWindows\n: {number of windows to record}  (if not given, the\nrecording goes on forever)\n}\n\n\n\n\nReturns:\n\n\n{\n    \nid\n: \n{recordingId}\n,\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop\n\n\nFunction: Stop recording on port\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]\n\n\nFunction: Return the list of containers for this application\n\n\nReturn:\n\n\n{\n    \ncontainers\n: [\n        {\n            \nhost\n: \n{host}\n,\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n            \nmemoryMBFree\n: \n{memoryMBFree}\n,\n            \nnumOperators\n: \n{numOperators}\n,\n            \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n            \nstate\n: \n{state}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}\n\n\nFunction: Return the information of the specified container\n\n\nReturn:\n\n\n{\n    \nhost\n: \n{host}\n,\n    \nid\n: \n{id}\n,\n    \njvmName\n: \n{jvmName}\n,\n    \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n    \nmemoryMBAllocated\n: \n{memoryMBAllocated}\n,\n    \nmemoryMBFree\n: \n{memoryMBFree}\n,\n    \nnumOperators\n: \n{numOperators}\n,\n    \ncontainerLogsUrl\n: \n{containerLogsUrl}\n,\n    \nstate\n: \n{state}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs\n\n\nFunction: Return the container log list\n\n\nReturn:\n\n\n{\n    \nlogs\n: [\n        {\n            \nlength\n: \n{log length}\n,\n            \nname\n: \n{logName}\n\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos}\nend={endPos}\ngrep={regexp}\nincludeOffset={true/false}]\n\n\nFunction: Return the raw log\n\n\nReturn: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):\n\n\n{\n    \nlines\n: [\n        { \nbyteOffset\n:\n{byteOffset}\n, \nline\n: \n{line}\n }, \u2026\n     ]\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill\n\n\nFunction: Kill this container\n\n\nPayload: none\n\n\nGET /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Return the logical plan of this application\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n      {\n        \nname\n: \n{name}\n,\n        \nattributes\n: {attributeMap},\n        \nclass\n: \n{class}\n,\n        \nports\n: {\n           [\n            {\n                \nname\n: \n{name}\n,\n                \nattributes\n: {attributeMap},\n                \ntype\n: \ninput/output\n\n            }, ...\n           ]\n         },\n         \nproperties\n: {\n            \nclass\n: \n{class}\n\n         }\n      }, ...\n    ],\n    \nstreams\n: [\n        {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/attributes\n\n\nFunction: Return the application attributes\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators\n\n\nFunction: Return the list of info of the logical operator\n\n\nReturn:\n\n\n{\n    \noperators\n: [\n        {\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n\n\nFunction: Return the info of the logical operator\n\n\nReturn:\n\n\n{\n            \nclassName\n: \n{className}\n,\n            \ncontainerIds\n: [ \n{containerid}\n, \u2026 ],\n            \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n            \ncurrentWindowId\n: \n{currentWindowId}\n,\n            \nfailureCount\n: \n{failureCount}\n,\n            \nhosts\n: [ \n{host}\n, \u2026 ],\n            \nlastHeartbeat\n: \n{lastHeartbeat}\n,\n            \nlatencyMA\n: \n{latencyMA}\n,\n            \nname\n: \n{name}\n,\n            \npartitions\n: [ \n{operatorid}\n, \u2026 ],\n            \nrecoveryWindowId\n: \n{recoveryWindowId}\n,\n            \nstatus\n: {\n                \n{state}\n: \n{number}\n, ...\n            },\n            \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n            \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n            \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n            \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n            \nunifiers\n: [ \n{operatorid}\n, \u2026 ],\n            \ncounters\n: {\n                 \n{counterName}: {\n                    \navg\n: \u2026, \nmax\n: \u2026, \nmin\n: \u2026, \nsum\n: ...\n                 }\n            }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Return the properties of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties\n\n\nFunction: Set the properties of the logical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Return the properties of the physical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties\n\n\nFunction: Set the properties of the physical operator\nPayload:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes\n\n\nFunction: Get the attributes of the logical operator\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes\n\n\nFunction:  Get the attributes of the port\n\n\nReturn:\n\n\n{\n    \n{name}\n: value, ...\n}\n\n\n\n\nPOST /ws/v2/applications/{appid}/logicalPlan\n\n\nFunction: Change logical plan of this application\nPayload:\n\n\n{\n    \nrequests\n: [\n        {\n            \nrequestType\n: \nAddStreamSinkRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nCreateOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \noperatorFQCN\n: \n{operatorFQCN}\n,\n        },\n        {\n            \nrequestType\n: \nCreateStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n            \nsourceOperatorName\n: \n{sourceOperatorName}\n,\n            \nsourceOperatorPortName\n: \n{sourceOperatorPortName}\n\n            \nsinkOperatorName\n: \n{sinkOperatorName}\n,\n            \nsinkOperatorPortName\n: \n{sinkOperatorPortName}\n\n        },\n        {\n            \nrequestType\n: \nRemoveOperatorRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n        },\n        {\n            \nrequestType\n: \nRemoveStreamRequest\n,\n            \nstreamName\n: \n{streamName}\n,\n        },\n        {\n            \nrequestType\n: \nSetOperatorPropertyRequest\n,\n            \noperatorName\n: \n{operatorName}\n,\n            \npropertyName\n: \n{propertyName}\n,\n            \npropertyValue\n: \n{propertyValue}\n\n        },\n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta\n\n\nFunction: Return the meta information about the statistics stored for\nthis operator\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \noperatorName\n: \n{operatorName}\n,\n    \noperatorIds\n: [ {opid}, \u2026 ],\n    \nstartTime\n: \n{startTime}\n,\n    \nendTime\n: \n{endTime}\n,\n    \ncount\n: \n{count}\n,\n    \nended\n: \n{boolean}\n\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the statistics stored for this logical operator\n\n\n{\n    \noperatorStats\n: [\n        {\n            \noperatorId\n: \n{operatorId}\n,\n            \ntimestamp\n: \n{timestamp}\n,\n            \nstats\n: {\n                \ncontainer\n: \ncontainerId\n,\n                \nhost\n: \nhost\n,\n                \ntotalTuplesProcessed\n, \n{totalTuplesProcessed}\n,\n                \ntotalTuplesEmitted\n, \n{totalTuplesEmitted}\n,\n                \ntuplesProcessedPSMA\n, \n{tuplesProcessedPSMA}\n,\n                \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n                \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n                \nlatencyMA\n: \n{latencyMA}\n,\n                \nports\n: [ {\n                    \nname\n: \n{name}\n,\n                    \ntype\n:\n{input/output}\n,\n                    \ntotalTuples\n: \n{totalTuples}\n,\n                    \ntuplesPSMA\n, \n{tuplesPSMA}\n,\n                    \nbufferServerBytesPSMA\n, \n{bufferServerBytesPSMA}\n\n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta\n\n\nFunction: Return the meta information about the container statistics\n\n\n{\n    \nappId\n: \n{appId}\n,\n    \ncontainers\n: {\n        \n{containerId}\n: {\n            \nid\n: \n{id}\n,\n            \njvmName\n: \n{jvmName}\n,\n            \nhost\n: \n{host}\n,\n            \nmemoryMBAllocated\n, \n{memoryMBAllocated}\n\n        },\n        \u2026\n    },\n    \nstartTime\n: \n{startTime}\n\n    \nendTime\n: \n{endTime}\n\n    \ncount\n: \n{count}\n\n    \nended\n: {boolean}\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime}\nendTime={endTime}\n\n\nFunction: Return the container statistics stored for this application\n\n\n{\n    \ncontainerStats\n: [\n        {\n            \ncontainerId\n: \n{containerId}\n\n            \ntimestamp\n: \n{timestamp}\n\n            \nstats\n: {\n                \nnumOperators\n: \n{numOperators}\n,\n            }\n        }, ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/recordings\n\n\nFunction: Get the list of all recordings for this application\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [{\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings\n\n\nFunction: Get the list of recordings on this operator\n\n\nReturn:\n\n\n{\n    \nrecordings\n: [ {\n        \nid\n: \n{id}\n,\n        \nstartTime\n: \n{startTime}\n,\n        \nappId\n: \n{appId}\n,\n        \noperatorId\n: \n{operatorId}\n,\n        \ncontainerId\n: \n{containerId}\n,\n        \ntotalTuples\n: \n{totalTuples}\n,\n        \nports\n: [ {\n            \nname\n: \n{portName}\n,\n            \nstreamName\n: \n{streamName}\n,\n            \ntype\n: \n{type}\n,\n            \nid\n: \n{index}\n,\n            \ntupleCount\n: \n{tupleCount}\n\n        } \u2026 ],\n        \nended\n: {boolean},\n        \nwindowIdRanges\n: [ {\n            \nlow\n: \n{lowId}\n,\n            \nhigh\n: \n{highId}\n\n        } \u2026 ],\n        \nproperties\n: {\n            \nname\n: \nvalue\n, ...\n        }\n    }, ...]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Get the information about the recording\n\n\nReturn:\n\n\n{\n    \nid\n: \n{id}\n,\n    \nstartTime\n: \n{startTime}\n,\n    \nappId\n: \n{appId}\n,\n    \noperatorId\n: \n{operatorId}\n,\n    \ncontainerId\n: \n{containerId}\n,\n    \ntotalTuples\n: \n{totalTuples}\n,\n    \nports\n: [ {\n       \nname\n: \n{portName}\n,\n       \nstreamName\n: \n{streamName}\n,\n       \ntype\n: \n{type}\n,\n       \nid\n: \n{index}\n,\n       \ntupleCount\n: \n{tupleCount}\n\n     } \u2026 ],\n    \nended\n: {boolean},\n    \nwindowIdRanges\n: [ {\n       \nlow\n: \n{lowId}\n,\n       \nhigh\n: \n{highId}\n\n     } \u2026 ],\n    \nproperties\n: {\n       \nname\n: \nvalue\n, ...\n     }\n}\n\n\n\n\nDELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}\n\n\nFunction: Deletes the specified recording\n\n\nSince: 1.0.4\n\n\nGET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples\n\n\nQuery Parameters:\n\n\noffset\nstartWindow\nlimit\nports\nexecuteEmptyWindow\n\n\n\nFunction: Get the tuples\n\n\nReturn:\n\n\n{\n    \nstartOffset\n: \n{startOffset}\n,\n    \ntuples\n: [ {\n        \nwindowId\n: \n{windowId}\n,\n        \ntuples\n: [ {\n            \nportId\n: \n{portId}\n,\n            \ndata\n: \n{tupleData}\n\n        }, \u2026 ]\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/events?from={fromTime}\nto={toTime}\noffset={offset}\nlimit={limit}\n\n\nFunction: Get the events\n\n\nReturn:\n\n\n{\n    \nevents\n: [ {\n           \nid\n: \n{id}\n,\n        \ntimestamp\n: \n{timestamp}\n,\n        \ntype\n: \n{type}\n,\n        \ndata\n: {\n            \nname\n: \nvalue\n, \u2026\n        }\n    }, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/user\n\n\nFunction: Get the user profile information, list of roles and list of\npermissions given the user\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/profile/settings\n\n\nFunction: Get the current user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}\n\n\nFunction: Get the specified user's settings\n\n\nReturn:\n\n\n{\n    \n{key}\n: {value}, ...\n}\n\n\n\n\nGET /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Get the specified user's setting key\n\n\nReturn:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nPUT /ws/v2/profile/settings/{user}/{key}\n\n\nFunction: Set the specified user's setting key\nPayload:\n\n\n{\n    \nvalue\n: {value}\n}\n\n\n\n\nGET /ws/v2/auth/roles\n\n\nFunction: Get the list of roles the system has\n\n\nReturn:\n\n\n{\n    \nroles\n: [\n       {\n         \nname\n: \n{role1}\n,\n         \npermissions\n: [ \n{permission1}\n, \u2026 ]\n       }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/auth/roles/{role}\n\n\nFunction: Get the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/auth/roles/{role}\n\n\nFunction: create or edit the list of permissions given the role\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ \n{permissions1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/auth/restoreDefaultRoles\n\n\nFunction: Restores default roles\n\n\nDELETE /ws/v2/auth/roles/{role}\n\n\nFunction: delete the given role\n\n\nGET /ws/v2/auth/permissions\n\n\nFunction: Get the list of possible permissions\n\n\nReturn:\n\n\n{\n    \npermissions\n: [ {\n       \nname\n: \n{permissionName}\n,\n       \nadminOnly\n: true/false\n    }, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/applications/{appid}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Set the permissions details for this application\n\n\nPayload:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{name}/permissions\n\n\nFunction: Get the permissions details for this application\n\n\nReturn:\n\n\n{\n    \nreadOnly\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    },\n    \nreadWrite\n: {\n        \nroles\n: [ \nrole1\n, \u2026 ],\n        \nusers\n: [ \nuser1\n, \u2026 ],\n        \neveryone\n: true/false\n    }\n}\n\n\n\n\nPOST /ws/v2/licenses\n\n\nFunction: Add a license to the registry or generate an eval license\n\n\nPayload: The license file content, if payload is empty, it will try to generate an eval license and return the info\n\n\nReturn:\n\n\n{\n  \nid\n: \n{licenseId}\n,\n  \nexpireTime\n: {unixTimeMillis},\n  \nnodesAllowed\n: {nodesAllowed},\n  \nmemoryMBAllowed\n: {memoryMBAllowed},\n  \ncontextType\n: \n{contextType}\n,\n  \ntype\n: \n{type}\n,\n  \nfeatures\n: [ \n{feature1}\n, \u2026 ]\n}\n\n\n\n\nGET /ws/v2/licenses/current\n\n\nFunction: Get info on the current license\n\n\n{\n      \nid\n: \n{licenseId}\n,\n      \nexpireTime\n: {unixTimeMillis},\n      \nnodesAllowed\n: {nodesAllowed},\n      \nnodesUsed\n: {nodesUsed},\n      \nmemoryMBAllowed\n: {memoryMBAllowed},\n      \nmemoryMBUsed\n: {memoryMBUsed},\n      \ncontextType\n: \n{community|standard|enterprise}\n,\n      \ntype\n: \n{evaluation|non_production|production}\n\n      \nfeatures\n: [ \n{feature1}\n, \u2026 ], // for community, empty array\n      \ncurrent\n: true/false\n}\n\n\n\n\nGET /ws/v2/config/installMode\n\n\nFunction: returns the install mode\n\n\n{\n  \ninstallMode\n: \n{evaluation|community|app}\n,\n  \nappPackageName\n: \n{optionalAppPackageName}\n,\n  \nappPackageVersion\n: \n{optionalAppPackageVersion}\n\n}\n\n\n\n\nGET /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction: returns the download type\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nPUT /ws/v2/config/properties/dt.phoneHome.enable\n\n\nFunction:\n\n\n{\n  \nvalue\n: \ntrue/false\n\n}\n\n\n\n\nFeature List:  \n\n\n\n\nSYSTEM_APPS\n\n\nSYSTEM_ALERTS\n\n\nAPP_DATA_DASHBOARDS\n\n\nRUNTIME_DAG_CHANGE\n\n\nRUNTIME_PROPERTY_CHANGE\n\n\nAPP_CONTAINER_LOGS\n\n\nLOGGING_LEVELS\n\n\nAPP_DATA_TRACKER\n\n\nJAAS_LDAP_AUTH\n\n\nAPP_BUILDER\n\n\n\n\nGET /ws/v2/config/properties\n\n\nFunction: Returns list of properties from dt-site.xml.\n\n\nReturn:\n\n\n{\n    \n{name}\n: {\n        \nvalue\n: \n{PROPERTY_VALUE}\n,\n        \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n    }\n\n}\n\n\n\n\nGET /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Returns single property from dt-site.xml, specify by name\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nPOST /ws/v2/config/properties\n\n\nFunction: Overwrites all specified properties in dt-site.xml\n\n\nPayload:\n\n\n{\n    \nproperties\n: [\n        {\n            \nname\n: \n{name}\n\n            \nvalue\n: \n{PROPERTY_VALUE}\n,\n            \nlocal\n: true/false,\n                    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n        }, \u2026\n    ]\n}\n\n\n\n\nPUT /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Overwrites or creates new property in dt-site.xml\nPayload:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n    \nlocal\n: true/false,\n    \ndescription\n: \n{PROPERTY_DESCRIPTION}\n\n}\n\n\n\n\nDELETE /ws/v2/config/properties/{PROPERTY_NAME}\n\n\nFunction: Deletes a property from dt-site.xml. This may have to be\nrestricted to custom properties?\n\n\nGET /ws/v2/config/hadoopExecutable\n\n\nFunction: Returns the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nPUT /ws/v2/config/hadoopExecutable\n\n\nFunction: Sets the hadoop executable\n\n\nReturn:\n\n\n{\n    \nvalue\n: \n{PROPERTY_VALUE}\n,\n}\n\n\n\n\nGET /ws/v2/config/issues\n\n\nFunction: Returns list of potential issues with environment\n\n\nReturn:\n\n\n{\n    \nissues\n: [\n        {\n            \nkey\n: \n{issueKey}\n,\n            \npropertyName\n: \n{PROPERTY_NAME}\n,\n            \ndescription\n: \n{ISSUE_DESCRIPTION}\n,\n            \nseverity\n: \nerror\n|\nwarning\n\n        },\n        {...},\n        {...}\n    ]    \n}\n\n\n\n\nGET /ws/v2/config/ipAddresses\n\n\nFunction: Returns list of ip addresses the gateway can listen to\n\n\nReturn:\n\n\n{\n    \nipAddresses\n: [\n      \n1.2.3.4\n, ...\n    ]    \n}\n\n\n\n\nPOST /ws/v2/config/restart\n\n\nFunction: Restarts the gateway\n\n\nPayload: none\n\n\nGET /proxy/rm/v1/\u2026\n\n\nPOST /proxy/rm/v1/\u2026\n\n\nFunction: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.\n\n\nGET /proxy/stram/v2/...\n\n\nPOST /proxy/stram/v2/\u2026\n\n\nPUT /proxy/stram/v2/\u2026\n\n\nDELETE /proxy/stram/v2/\u2026\n\n\nFunction: Proxy calls to Stram Web Services.\n\n\nPOST /ws/v2/applications/{appid}/loggers\n\n\nFunction: Set the logger levels of packages/classes.\n\n\nPayload:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers\n\n\nFunction: Gets the logger levels of packages/classes.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nlogLevel\n: value,\n            \ntarget\n: value\n        }, \n        ...\n    ]\n}\n\n\n\n\nGET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"\n\n\nFunction: searches for all classes that match the pattern.\n\n\nReturn:\n\n\n{\n    \nloggers\n : [\n        {\n            \nname\n : \n{fully qualified class name}\n,\n            \nlevel\n: \n{logger level}\n\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of appPackages the user can view in the system\n\n\n{\n    \nappPackages\n: [\n        {\n                 \nappPackageName\n: \n{appPackageName}\n,\n                 \nappPackageVersion\n: \n{appPackageVersion}\n,\n            \nmodificationTime\n: \n{modificationTime}\n,\n            \nowner\n: \n{owner}\n,\n        }, ...\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages?merge={replace|fail|ours|theirs}\n\n\nSince: 1.0.4\n\n\nFunction: Uploads an appPackage file, merge with existing app package if exists. Default is replace.\n\n\nPayload: the raw zip file\n\n\nReturn: The information of the app package\n\n\nGET /ws/v2/appPackages/{owner}/{name}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of versions of appPackages with the given name in the system owned by the specified user\n\n\n{\n    \nversions\n: [\n        \n1.0-SNAPSHOT\n\n    ]\n}\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the appPackage\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download\n\n\nSince: 1.0.4\n\n\nFunction: Downloads the appPackage zip file\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta information of the app package\n\n\nReturns:\n\n\n{\n    \nappPackageName\n: \n{appPackageName}\n,\n    \nappPackageVersion\n: \n{appPackageVersion}\n,\n    \nmodificationTime\n:  \n{modificationTime}\n,\n    \nowner\n: \n{owner}\n,\n    ...\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs\n\n\nSince: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:\n\n\n{\n    \nconfigs\n: [\n        \nmy-app-conf1.xml\n\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the properties XML of the specified config\n\n\nReturns:\n\n\nconfiguration\n\n        \nproperty\n\n                \nname\n...\n/name\n\n                \nvalue\n...\n/value\n\n        \n/property\n\n        \u2026\n\n/configuration\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Creates or replaces the specified config with the property parameters specified payload\n\n\nPayload: configuration in XML\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}\n\n\nSince: 1.0.4\n\n\nFunction: Deletes the specified config\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications\n\n\nSince: 1.0.4\n\n\nFunction: Gets the list of applications in the appPackage\n\n\nReturns:\n\n\n{\n    \napplications\n: [\n        {\n            \ndag\n: {dag in json format},\n            \nfile\n: \n{fileName}\n,\n            \nname\n: \n{name}\n,\n            \ntype\n: \n{type}\n,\n            \nerror\n: \n{error}\n,\n            \nfileContent\n: {originalFileContentForJSONTypeApp}\n        }\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}\n\n\nSince: 1.0.4\n\n\nFunction: Gets the meta data for that application\n\n\nReturns:\n\n\n{\n    \nfile\n: \n{fileName}\n,\n    \nname\n: \n{name}\n,\n    \ntype\n: \n{json/class/properties}\n,\n    \nerror\n: \n{error}\n\n    \ndag\n: {\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n         }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n    },\n    \nfileContent\n: {originalFileContentForJSONTypeApp}\n}\n\n\n\n\nPOST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge\n\n\nFunction: Merge the configuration, json apps, and resources files from the version specified from the payload to the specified app package in the url, without overwriting any existing file in the specified app package\n\n\nPayload:\n\n\n{\n \nversion\n: \n{versionToMergeFrom}\n\n}\n\n\n\n\nPOST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName}\noriginalAppId={originalAppId}\nqueue={queueName}]\n\n\nSince: 1.0.4\n\n\nFunction: Launches the application with the given configuration specified in the POST payload\n\n\nPayload:\n\n\n{\n    \n{propertyName}\n : \n{propertyValue}\n, ...\n}\n\n\n\n\nReturn:\n\n\n{\n    \nappId\n: \n{appId}\n\n}\n\n\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}\n\n\nSince: 1.0.4\n\n\nFunction: Get the properties of the operator given the classname in the jar\n\n\n{  \n    \nproperties\n: [  \n        {\n          \nname\n:\n{className}\n,\n          \ncanGet\n: {canGet},\n          \ncanSet\n: {canSet},\n          \ntype\n:\n{type}\n,\n          \ndescription\n:\n{description}\n,\n          \nproperties\n: ...\n        },\n       \u2026\n     ]\n}\n\n\n\n\nPUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]\n\n\nFunction: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app ackage\n\n\nPayload:\n\n\n{\n        \ndisplayName\n: \n{displayName}\n,\n        \ndescription\n: \n{description}\n,\n        \noperators\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nattributes\n:  {\n                \n{attributeKey}\n: \n{attributeValue}\n, ...\n            },\n            \nclass\n: \n{class}\n,\n            \nports\n: [\n                  {\n                    \nname\n: \n{name}\n,\n                    \nattributes\n:  {\n                       \n{attributeKey}\n: \n{attributeValue}\n, ...\n                     },\n                  }, ...\n            ],\n            \nproperties\n: {\n               \n{propertyName}\n: \n{propertyValue}\n\n            }\n          }, ...\n        ],\n        \nstreams\n: [\n          {\n            \nname\n: \n{name}\n,\n            \nlocality\n: \n{locality}\n,\n            \nsinks\n: [\n                {\n                    \noperatorName\n: \n{operatorName}\n,\n                    \nportName\n: \n{portName}\n\n                }, ...\n            ],\n            \nsource\n: {\n                \noperatorName\n: \n{operatorName}\n,\n                \nportName\n: \n{portName}\n\n            }\n          }, ...\n        ]\n}\n\n\n\n\nReturn:\n\n\n{\n        \nerror\n: \n{error}\n\n}\n\n\n\n\nAvailable port attributes to set: \n\n\n\n\nAUTO_RECORD\n\n\nIS_OUTPUT_UNIFIED\n\n\nPARTITION_PARALLEL\n\n\nQUEUE_CAPACITY\n\n\nSPIN_MILLIS\n\n\nSTREAM_CODEC\n\n\nUNIFIER_LIMIT\n\n\n\n\nAvailable locality options to set: \n\n\n\n\nTHREAD_LOCAL\n\n\nCONTAINER_LOCAL\n\n\nNODE_LOCAL\n\n\nRACK_LOCAL\n\n\n\n\nDELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}\n\n\nSince: 1.0.5\n\n\nFunction: Deletes non-jar based application in the app package\n\n\nGET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators\n\n\nSince: 1.0.5\n\n\nFunction: Get the classes of operators from specified app package.\n\n\nReturn:\n\n\n{  \n    \noperatorClasses\n: [  \n        {\n            \nname\n:\n{fullyQualifiedClassName}\n, \n            \ntitle\n: \n{title}\n,\n            \nshortDesc\n: \n{description}\n,\n            \nlongDesc\n: \n{description}\n,\n            \ncategory\n: \n{categoryName}\n,\n            \ndoclink\n: \n{doc url}\n,\n            \ntags\n: [ \n{tag}\n, \n{tag}\n, \u2026 ],\n            \ninputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ]\n            \noutputPorts\n: [\n                {\n                    \nname\n: \n{portName}\n,\n                    \ntype\n: \n{tupleType}\n,\n                    \noptional\n: true/false  \n                }, \u2026\n            ],\n            \nproperties\n: [  \n                {\n                    \nname\n:\n{propertyName}\n,\n                    \ncanGet\n: {canGet},\n                    \ncanSet\n: {canSet},\n                    \ntype\n:\n{type}\n,\n                    \ndescription\n:\n{description}\n,\n                    \nproperties\n: ...\n                }, \u2026\n            ],\n            \ndefaultValue\n: {\n                \n{propertyName}\n: [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}\n\n\n\n\nGET /ws/v2/appPackages/import\n\n\nFunction: List the importable app packages on Gateway's local file\nsystem\n\n\nReturn:\n\n\n{\n    \nappPackages: [\n        {\n            \nfile\n: \n{file}\n,\n            \nname\n: \n{name}\n,\n            \ndisplayName\n: \n{displayName}\n,\n            \nversion\n: \n{version}\n,\n            \ndescription\n: \n{description}\n\n        }\n    ]\n}\n\n\n\n\nPOST /ws/v2/appPackages/import\n\n\nFunction: Import app package from Gateway's local file system\n\n\nPayload:\n\n\n{\n        \nfiles\n: [\n{file}\n, \u2026 ]\n}\n\n\n\n\nPUT /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Creates or replaces the specified system alert. The condition has access to an object in its scope called \n_topic\n. An example alert might take the form of the following:\n\n\n_topic[\"applications.application_1400294100000_0001\"].allocatedContainers \n 5\n\n\n\nPayload:\n\n\n{\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n        \ntimeThresholdMillis\n:\n{time}\n\n}\n\n\n\n\nDELETE /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Deletes the specified system alert\n\n\nGET /ws/v2/systemAlerts/alerts?inAlert={true/false}\n\n\nFunction: Gets the created alerts\n\n\nReturn:\n\n\n{\n    \nalerts\n: [{\n        \nname\n: \n{alertName}\n,\n        \ncondition\n:\n{condition in javascript}\n,\n        \nemail\n:\n{email}\n,\n        \ntimeThresholdMillis\n:\n{time}\n,\n        \nalertStatus\n: {\n            \nisInAlert\n:{true/false}\n            \ninTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }\n    }, \u2026  ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/alerts/{name}\n\n\nFunction: Gets the specified system alert\n\n\nReturn:\n\n\n{\n    \nname\n: \n{alertName}\n,\n    \ncondition\n:\n{condition in javascript}\n,        \n    \nemail\n:\n{email}\n,\n    \ntimeThresholdMillis\n:\n{time}\n,\n    \nalertStatus\n: {\n        \nisInAlert\n:{true/false}\n        \ninTime\n: \n{time}\n,\n        \nmessage\n: \n{message}\n,\n        \nemailSent\n: {true/false}\n    }\n}\n\n\n\n\nGET /ws/v2/systemAlerts/history\n\n\nFunction: Gets the history of alerts\n\n\nReturn:\n\n\n{\n    \nhistory\n: [\n        {\n            \nname\n:\n{alertName}\n,\n            \ninTime\n:\n{time}\n,\n            \noutTime\n: \n{time}\n,\n            \nmessage\n: \n{message}\n,\n            \nemailSent\n: {true/false}\n        }, ...\n     ]\n}\n\n\n\n\nGET /ws/v2/systemAlerts/topicData\n\n\nFunction: Gets the topic data that is used for evaluating alert\ncondition\n\n\nReturn:\n\n\n{\n     \n{topicName}\n: {json object data}, ...\n}\n\n\n\n\nGET /ws/v2/auth/users/{user}\n\n\nFunction: Gets the info of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPOST /ws/v2/auth/users/{user}\n\n\nFunction: Changes password and/or roles of the given user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \noldPassword\n: \n{oldPassword}\n,\n    \nnewPassword\n: \n{newPassword}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nPUT /ws/v2/auth/users/{user}\n\n\nFunction: Creates new user\n\n\nReturn:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n,\n    \nroles\n: [ \n{role1}\n, \n{role2}\n ]\n}\n\n\n\n\nDELETE /ws/v2/auth/users/{user}\n\n\nFunction: Deletes the specified user\n\n\nGET /ws/v2/auth/users\n\n\nFunction: Gets the list of users\n\n\nReturn:\n\n\n{\n    \nusers\n: [ {\n       \nuserName\n: \n{username1}\n,\n       \nroles\n: [ \n{role1}\n, \u2026 ],\n       \npermissions\n: [ \n{permission1}\n, \u2026 ]\n    }\n}\n\n\n\n\nPOST /ws/v2/login\n\n\nFunction: Login\nPayload:\n\n\n{\n    \nuserName\n: \n{userName}\n,\n    \npassword\n: \n{password}\n\n}\n\n\n\n\nReturn:\n\n\n{\n    \nauthScheme\n: \n{authScheme}\n,\n    \nuserName\n : \n{userName}\n,\n    \nroles\n: [ \n{role1}\n, \u2026 ],\n    \npermissions\n: [ \n{permission1}\n, \u2026 ]\n}\n\n\n\n\nPOST /ws/v2/logout\n\n\nFunction: Log out the current user\n\n\nReturn:\n\n\n{\n}\n\n\n\n\nPublisher-Subscriber WebSocket Protocol\n\n\nInput\n\n\nPublishing\n\n\n{\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nSubscribing\n\n\n{\"type\":\"subscribe\", \"topic\":\"{topic}\"}\n\n\n\nUnsubscribing\n\n\n{\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nSubscribing to the number of subscribers of a topic\n\n\n{\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}\n\n\n\nOutput\n\n\nNormal Published Data\n\n\n{\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}\n\n\n\nNumber of Subscribers:\n\n\n{\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}\n\n\n\nAuto publish topics\n\n\ndata that gets published every one second:\n\n\n\n\napplications\n - list of streaming applications running in the cluster\n\n\napplications.[appid]\n - information about a particular application\n\n\napplications.[appid].containers\n - information about containers of a particular application\n\n\napplications.[appid].physicalOperators\n - information about operators of a particular application\n\n\napplications.[appid].logicalOperators\n - information about logical operators of a particular application\n\n\napplications.[appid].events\n - events from the AM of a particularapplication\n\n\n\n\ndata that gets published every five seconds:\n\n\n\n\ncluster.metrics\n - metrics of the cluster", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#datatorrent-dtgateway-api-v2-specification", 
            "text": "", 
            "title": "DataTorrent dtGateway API v2 Specification"
        }, 
        {
            "location": "/dtgateway_api/#rest-api", 
            "text": "", 
            "title": "REST API"
        }, 
        {
            "location": "/dtgateway_api/#return-codes", 
            "text": "200 : OK  400 : The request is not in the format that the server expects  404 : The resource is not found  500 : Something is wrong on the server side", 
            "title": "Return codes"
        }, 
        {
            "location": "/dtgateway_api/#rest-uri-specification", 
            "text": "GET /ws/v2/about  Function:  Return:  {\n     buildVersion :  {buildVersion} ,\n     buildDate :  {date and time} ,\n     buildRevision :  {revision} ,\n     buildUser :  {user} ,\n     version :  {version} ,\n     gatewayUser :  {user} ,\n     javaVersion :  {java_version} ,\n     hadoopLocation :  {hadoop_location} ,\n     jvmName :  {pid}@{hostname} ,\n     configDirectory :  {configDir} ,\n     hostname :  {hostname} ,\n     hadoopIsSecurityEnabled :  {true/false} \n}  GET /ws/v2/cluster/metrics  Function: List metrics that are relevant to the entire cluster  Return:  {\n     averageAge :  {average running application age in milliseconds} ,\n     cpuPercentage :  {cpuPercentage} ,\n     currentMemoryAllocatedMB :  {currentMemoryAllocatedMB} ,\n     maxMemoryAllocatedMB :  {maxMemoryAllocatedMB} ,\n     numAppsFailed :  {numAppsFailed} ,\n     numAppsFinished :  {numAppsFinished} ,\n     numAppsKilled :  {numAppsKilled} ,\n     numAppsPending :  {numAppsPending} ,\n     numAppsRunning :  {numAppsRunning} ,\n     numAppsSubmitted :  {numAppsSubmitted} ,\n     numContainers :  {numContainers} ,\n     numOperators :  {numOperators} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}  GET /ws/v2/applications[?states={STATE_FILTER} name={NAME_FILTER} user={USER_FILTER]  Function: List IDs of all streaming applications  Return:  {\n     apps : [\n        {\n             diagnostics :  {diagnostics} ,\n             elapsedTime :  {elapsedTime} ,\n             finalStatus :  {finalStatus} ,\n             finishedTime :  {finishedTime} ,\n             id :  {appId} ,\n             name :  {name} ,\n             queue :  {queue} ,\n             startedTime :  {startedTime} ,\n             state :  {state} ,\n             trackingUrl :  {trackingUrl} ,\n             user :  {user} \n        },  \n        \u2026\n    ]\n}  GET /ws/v2/applications/{appid}  Function: Get the information for the specified application  Return:  {\n     id :  {appid} ,\n     name :  {name} ,\n     state :  {state} ,\n     trackingUrl :  {tracking url} ,\n     finalStatus : {finalStatus},\n     appPath :  {appPath} ,\n     gatewayAddress :  {gatewayAddress} ,\n     elapsedTime :  {elapsedTime} ,\n     startedTime :  {startTime} ,\n     user :  {user} ,\n     version :  {stram version} ,\n     remainingLicensedMB :  {remainingLicensedMB} ,\n     allocatedMB :  {allocatedMB} ,\n     gatewayConnected :  true/false ,\n     connectedToThisGateway :  true/false ,\n     attributes : {\n            {attributeName} :  {attributeValue} , \n            {attributeName-n} :  {attributeValue-n} , \n    },\n     stats : {\n         allocatedContainers :  {allocatedContainer} ,\n         totalMemoryAllocated :  {totalMemoryAllocated} ,\n         latency :  {overall latency} ,\n         criticalPath :  {list of operator id that represents the critical path} ,\n         failedContainers :  {failedContainers} ,\n         numOperators :  {numOperators} ,\n         plannedContainers :  {plannedContainers} ,\n         currentWindowId :  {min of operators:currentWindowId} ,\n         recoveryWindowId :  {min of operators:recoveryWindowId} ,\n         tuplesProcessedPSMA :  {sum of operators:tuplesProcessedPSMA} ,\n         totalTuplesProcessed : {sum of operators:totalTuplesProcessed} ,\n         tuplesEmittedPSMA : {sum of operators:tuplesEmittedPSMA} ,\n         totalTuplesEmitted : {sum of operators:totalTuplesEmitted} ,\n         totalBufferServerReadBytesPSMA :  {totalBufferServerReadBytesPSMA} ,\n         totalBufferServerWriteBytesPSMA :  {totalBufferServerWriteBytesPSMA} \n    }\n}  GET /ws/v2/applications/{appid}/physicalPlan  Function: Return the physical plan for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             isUnifier : true/false\n        },\n         \u2026\n     ],\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators  Function: Return list of operators for the given application  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             container :  {containerId} ,\n             counters : {\n                 {counterName} :  {counterValue} , \n                ...\n             },\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             host :  {host} ,\n             id :  {id} ,\n             ports : [\n                {\n                     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n                     name :  {name} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA :  {tuplesPSMA} ,\n                     type :  input/output ,\n                     recordingStartTime :  {recordingStartTime} \n                },\n                ...\n            ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,\n             recoveryWindowId :  {recoveryWindowId} ,\n             status :  {status} ,\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             logicalName :  {logicalName} ,\n             unifierClass :  {unifierClass} \n        },\n         \u2026\n     ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/streams  Function: Return physical streams  Return:  {\n      streams : [        \n        {\n             logicalName :  {logicalName} ,\n             sinks : [\n                {\n                     operatorId :  {operatorId} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorId :  {operatorId} ,\n                 portName :  {portName} \n            },\n             locality :  {locality} \n        }, ...\n     ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}  Function: Return information of the given operator for the given application  Return:  {\n     className :  {className} ,\n     container :  {containerId} ,\n     counters : {\n       {counterName}:  {counterValue} , ...            \n    }\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     currentWindowId :  {currentWindowId} ,\n     failureCount :  {failureCount} ,\n     host :  {host} ,\n     id :  {id} ,\n     ports : [\n       {\n           bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,\n           name :  {name} ,\n           totalTuples :  {totalTuples} ,\n           tuplesPSMA :  {tuplesPSMA} ,\n           type :  input/output \n       }, ...\n    ],\n     lastHeartbeat :  {lastHeartbeat} ,\n     latencyMA :  {latencyMA} ,\n     name :  {name} ,\n     recordingStartTime :  {recordingStartTime} ,\n     recoveryWindowId :  {recoveryWindowId} ,\n     status :  {status} ,\n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} \n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/deployHistory  Function: Return container deploy history of this operator\nSince: 1.0.6  Return:  {\n    containers : [  \n        {  \n             container :  {containerId} ,   \n             startTime :  {startTime}   \n        }, ...  \n    ],   \n     name :  {operatorName} \n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports  Function: Get the information of all ports of the given operator of the\ngiven application  Return:  {  \n     ports : [\n        {  \n             bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n             name :  {name} ,\n             recordingStartTime :  {recordingStartTime} ,  \n             totalTuples :  {totalTuples} ,   \n             tuplesPSMA :  {tuplesPSMA} ,   \n             type :  output   \n        }, \u2026\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}  Function: Get the information of a specified port  Return:  {  \n     bufferServerBytesPSMA :  {bufferServerBytesPSMA} ,   \n     name :  {name} ,   \n     totalTuples :  {totalTuples} ,   \n     tuplesPSMA :  {tuplesPSMA} ,   \n     type :  {type}   \n}  GET /ws/v2/applications/{appid}/operatorClasses[?parent={parent} q={searchTerm} packagePrefixes={comma-separated-package-prefixes}]  Function: Get the classes of operators, if given the parent parameter,\nall classes that inherits from parent  Return:  {  \n     operatorClasses : [  \n        {  name : {className}  },\n       \u2026\n     ]\n}  GET /ws/v2/applications/{appid}/operatorClasses/{operatorClass}  Function: Get the description of the given operator class  Return:  {\n     inputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n          ...\n    ],\n     outputPorts : [\n        {\n             name :  {name} ,\n             optional : {boolean}\n        },\n        \u2026\n    ],\n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}  POST /ws/v2/applications/{appid}/shutdown  Function: Shut down the application  Payload: none  POST /ws/v2/applications/{appid}/kill  Function: Kill the given application  Payload: none  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/start  Function: Start recording on operator  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/stop  Function: Stop recording on operator  Payload: none  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/start  Function: Start recording on port  Payload (optional):  {\n    numWindows : {number of windows to record}  (if not given, the\nrecording goes on forever)\n}  Returns:  {\n     id :  {recordingId} ,\n}  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/ports/{portName}/recordings/stop  Function: Stop recording on port  Payload: none  GET /ws/v2/applications/{appid}/physicalPlan/containers[?states={NEW,ALLOCATED,ACTIVE,KILLED}]  Function: Return the list of containers for this application  Return:  {\n     containers : [\n        {\n             host :  {host} ,\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             lastHeartbeat :  {lastHeartbeat} ,\n             memoryMBAllocated :  {memoryMBAllocated} ,\n             memoryMBFree :  {memoryMBFree} ,\n             numOperators :  {numOperators} ,\n             containerLogsUrl :  {containerLogsUrl} ,\n             state :  {state} \n        }, \u2026\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}  Function: Return the information of the specified container  Return:  {\n     host :  {host} ,\n     id :  {id} ,\n     jvmName :  {jvmName} ,\n     lastHeartbeat :  {lastHeartbeat} ,\n     memoryMBAllocated :  {memoryMBAllocated} ,\n     memoryMBFree :  {memoryMBFree} ,\n     numOperators :  {numOperators} ,\n     containerLogsUrl :  {containerLogsUrl} ,\n     state :  {state} \n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs  Function: Return the container log list  Return:  {\n     logs : [\n        {\n             length :  {log length} ,\n             name :  {logName} \n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/logs/{logName}[?start={startPos} end={endPos} grep={regexp} includeOffset={true/false}]  Function: Return the raw log  Return: if includeOffset=false or not provided, return raw log content\n(Content-Type: text/plain). Otherwise (Content-Type: application/json):  {\n     lines : [\n        {  byteOffset : {byteOffset} ,  line :  {line}  }, \u2026\n     ]\n}  POST /ws/v2/applications/{appid}/physicalPlan/containers/{containerId}/kill  Function: Kill this container  Payload: none  GET /ws/v2/applications/{appid}/logicalPlan  Function: Return the logical plan of this application  Return:  {\n     operators : [\n      {\n         name :  {name} ,\n         attributes : {attributeMap},\n         class :  {class} ,\n         ports : {\n           [\n            {\n                 name :  {name} ,\n                 attributes : {attributeMap},\n                 type :  input/output \n            }, ...\n           ]\n         },\n          properties : {\n             class :  {class} \n         }\n      }, ...\n    ],\n     streams : [\n        {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/logicalPlan/attributes  Function: Return the application attributes  Return:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators  Function: Return the list of info of the logical operator  Return:  {\n     operators : [\n        {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}  Function: Return the info of the logical operator  Return:  {\n             className :  {className} ,\n             containerIds : [  {containerid} , \u2026 ],\n             cpuPercentageMA :  {cpuPercentageMA} ,\n             currentWindowId :  {currentWindowId} ,\n             failureCount :  {failureCount} ,\n             hosts : [  {host} , \u2026 ],\n             lastHeartbeat :  {lastHeartbeat} ,\n             latencyMA :  {latencyMA} ,\n             name :  {name} ,\n             partitions : [  {operatorid} , \u2026 ],\n             recoveryWindowId :  {recoveryWindowId} ,\n             status : {\n                 {state} :  {number} , ...\n            },\n             totalTuplesEmitted :  {totalTuplesEmitted} ,\n             totalTuplesProcessed :  {totalTuplesProcessed} ,\n             tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n             tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n             unifiers : [  {operatorid} , \u2026 ],\n             counters : {\n                  {counterName}: {\n                     avg : \u2026,  max : \u2026,  min : \u2026,  sum : ...\n                 }\n            }\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties  Function: Return the properties of the logical operator  Return:  {\n     {name} : value, ...\n}  POST /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/properties  Function: Set the properties of the logical operator\nPayload:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties  Function: Return the properties of the physical operator  Return:  {\n     {name} : value, ...\n}  POST /ws/v2/applications/{appid}/physicalPlan/operators/{opId}/properties  Function: Set the properties of the physical operator\nPayload:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/attributes  Function: Get the attributes of the logical operator  Return:  {\n     {name} : value, ...\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/ports/{portName}/attributes  Function:  Get the attributes of the port  Return:  {\n     {name} : value, ...\n}  POST /ws/v2/applications/{appid}/logicalPlan  Function: Change logical plan of this application\nPayload:  {\n     requests : [\n        {\n             requestType :  AddStreamSinkRequest ,\n             streamName :  {streamName} ,\n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  CreateOperatorRequest ,\n             operatorName :  {operatorName} ,\n             operatorFQCN :  {operatorFQCN} ,\n        },\n        {\n             requestType :  CreateStreamRequest ,\n             streamName :  {streamName} ,\n             sourceOperatorName :  {sourceOperatorName} ,\n             sourceOperatorPortName :  {sourceOperatorPortName} \n             sinkOperatorName :  {sinkOperatorName} ,\n             sinkOperatorPortName :  {sinkOperatorPortName} \n        },\n        {\n             requestType :  RemoveOperatorRequest ,\n             operatorName :  {operatorName} ,\n        },\n        {\n             requestType :  RemoveStreamRequest ,\n             streamName :  {streamName} ,\n        },\n        {\n             requestType :  SetOperatorPropertyRequest ,\n             operatorName :  {operatorName} ,\n             propertyName :  {propertyName} ,\n             propertyValue :  {propertyValue} \n        },\n        ...\n    ]\n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats/meta  Function: Return the meta information about the statistics stored for\nthis operator  Return:  {\n     appId :  {appId} ,\n     operatorName :  {operatorName} ,\n     operatorIds : [ {opid}, \u2026 ],\n     startTime :  {startTime} ,\n     endTime :  {endTime} ,\n     count :  {count} ,\n     ended :  {boolean} \n}  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}/stats?startTime={startTime} endTime={endTime}  Function: Return the statistics stored for this logical operator  {\n     operatorStats : [\n        {\n             operatorId :  {operatorId} ,\n             timestamp :  {timestamp} ,\n             stats : {\n                 container :  containerId ,\n                 host :  host ,\n                 totalTuplesProcessed ,  {totalTuplesProcessed} ,\n                 totalTuplesEmitted ,  {totalTuplesEmitted} ,\n                 tuplesProcessedPSMA ,  {tuplesProcessedPSMA} ,\n                 tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n                 cpuPercentageMA :  {cpuPercentageMA} ,\n                 latencyMA :  {latencyMA} ,\n                 ports : [ {\n                     name :  {name} ,\n                     type : {input/output} ,\n                     totalTuples :  {totalTuples} ,\n                     tuplesPSMA ,  {tuplesPSMA} ,\n                     bufferServerBytesPSMA ,  {bufferServerBytesPSMA} \n                }, \u2026 ],\n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/stats/meta  Function: Return the meta information about the container statistics  {\n     appId :  {appId} ,\n     containers : {\n         {containerId} : {\n             id :  {id} ,\n             jvmName :  {jvmName} ,\n             host :  {host} ,\n             memoryMBAllocated ,  {memoryMBAllocated} \n        },\n        \u2026\n    },\n     startTime :  {startTime} \n     endTime :  {endTime} \n     count :  {count} \n     ended : {boolean}\n}  GET /ws/v2/applications/{appid}/physicalPlan/containers/stats?startTime={startTime} endTime={endTime}  Function: Return the container statistics stored for this application  {\n     containerStats : [\n        {\n             containerId :  {containerId} \n             timestamp :  {timestamp} \n             stats : {\n                 numOperators :  {numOperators} ,\n            }\n        }, ...\n    ]\n}  GET /ws/v2/applications/{appid}/recordings  Function: Get the list of all recordings for this application  Return:  {\n     recordings : [{\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings  Function: Get the list of recordings on this operator  Return:  {\n     recordings : [ {\n         id :  {id} ,\n         startTime :  {startTime} ,\n         appId :  {appId} ,\n         operatorId :  {operatorId} ,\n         containerId :  {containerId} ,\n         totalTuples :  {totalTuples} ,\n         ports : [ {\n             name :  {portName} ,\n             streamName :  {streamName} ,\n             type :  {type} ,\n             id :  {index} ,\n             tupleCount :  {tupleCount} \n        } \u2026 ],\n         ended : {boolean},\n         windowIdRanges : [ {\n             low :  {lowId} ,\n             high :  {highId} \n        } \u2026 ],\n         properties : {\n             name :  value , ...\n        }\n    }, ...]\n}  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}  Function: Get the information about the recording  Return:  {\n     id :  {id} ,\n     startTime :  {startTime} ,\n     appId :  {appId} ,\n     operatorId :  {operatorId} ,\n     containerId :  {containerId} ,\n     totalTuples :  {totalTuples} ,\n     ports : [ {\n        name :  {portName} ,\n        streamName :  {streamName} ,\n        type :  {type} ,\n        id :  {index} ,\n        tupleCount :  {tupleCount} \n     } \u2026 ],\n     ended : {boolean},\n     windowIdRanges : [ {\n        low :  {lowId} ,\n        high :  {highId} \n     } \u2026 ],\n     properties : {\n        name :  value , ...\n     }\n}  DELETE /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}  Function: Deletes the specified recording  Since: 1.0.4  GET /ws/v2/applications/{appid}/physicalPlan/operators/{opid}/recordings/{id}/tuples  Query Parameters:  offset\nstartWindow\nlimit\nports\nexecuteEmptyWindow  Function: Get the tuples  Return:  {\n     startOffset :  {startOffset} ,\n     tuples : [ {\n         windowId :  {windowId} ,\n         tuples : [ {\n             portId :  {portId} ,\n             data :  {tupleData} \n        }, \u2026 ]\n    }, \u2026 ]\n}  GET /ws/v2/applications/{appid}/events?from={fromTime} to={toTime} offset={offset} limit={limit}  Function: Get the events  Return:  {\n     events : [ {\n            id :  {id} ,\n         timestamp :  {timestamp} ,\n         type :  {type} ,\n         data : {\n             name :  value , \u2026\n        }\n    }, \u2026 ]\n}  GET /ws/v2/profile/user  Function: Get the user profile information, list of roles and list of\npermissions given the user  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}  GET /ws/v2/profile/settings  Function: Get the current user's settings  Return:  {\n     {key} : {value}, ...\n}  GET /ws/v2/profile/settings/{user}  Function: Get the specified user's settings  Return:  {\n     {key} : {value}, ...\n}  GET /ws/v2/profile/settings/{user}/{key}  Function: Get the specified user's setting key  Return:  {\n     value : {value}\n}  PUT /ws/v2/profile/settings/{user}/{key}  Function: Set the specified user's setting key\nPayload:  {\n     value : {value}\n}  GET /ws/v2/auth/roles  Function: Get the list of roles the system has  Return:  {\n     roles : [\n       {\n          name :  {role1} ,\n          permissions : [  {permission1} , \u2026 ]\n       }, \u2026\n    ]\n}  GET /ws/v2/auth/roles/{role}  Function: Get the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}  PUT /ws/v2/auth/roles/{role}  Function: create or edit the list of permissions given the role  Return:  {\n     permissions : [  {permissions1} , \u2026 ]\n}  POST /ws/v2/auth/restoreDefaultRoles  Function: Restores default roles  DELETE /ws/v2/auth/roles/{role}  Function: delete the given role  GET /ws/v2/auth/permissions  Function: Get the list of possible permissions  Return:  {\n     permissions : [ {\n        name :  {permissionName} ,\n        adminOnly : true/false\n    }, \u2026 ]\n}  PUT /ws/v2/applications/{appid}/permissions  Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  GET /ws/v2/applications/{appid}/permissions  Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  PUT /ws/v2/appPackages/{owner}/{name}/permissions  Function: Set the permissions details for this application  Payload:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  GET /ws/v2/appPackages/{owner}/{name}/permissions  Function: Get the permissions details for this application  Return:  {\n     readOnly : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    },\n     readWrite : {\n         roles : [  role1 , \u2026 ],\n         users : [  user1 , \u2026 ],\n         everyone : true/false\n    }\n}  POST /ws/v2/licenses  Function: Add a license to the registry or generate an eval license  Payload: The license file content, if payload is empty, it will try to generate an eval license and return the info  Return:  {\n   id :  {licenseId} ,\n   expireTime : {unixTimeMillis},\n   nodesAllowed : {nodesAllowed},\n   memoryMBAllowed : {memoryMBAllowed},\n   contextType :  {contextType} ,\n   type :  {type} ,\n   features : [  {feature1} , \u2026 ]\n}  GET /ws/v2/licenses/current  Function: Get info on the current license  {\n       id :  {licenseId} ,\n       expireTime : {unixTimeMillis},\n       nodesAllowed : {nodesAllowed},\n       nodesUsed : {nodesUsed},\n       memoryMBAllowed : {memoryMBAllowed},\n       memoryMBUsed : {memoryMBUsed},\n       contextType :  {community|standard|enterprise} ,\n       type :  {evaluation|non_production|production} \n       features : [  {feature1} , \u2026 ], // for community, empty array\n       current : true/false\n}  GET /ws/v2/config/installMode  Function: returns the install mode  {\n   installMode :  {evaluation|community|app} ,\n   appPackageName :  {optionalAppPackageName} ,\n   appPackageVersion :  {optionalAppPackageVersion} \n}  GET /ws/v2/config/properties/dt.phoneHome.enable  Function: returns the download type  {\n   value :  true/false \n}  PUT /ws/v2/config/properties/dt.phoneHome.enable  Function:  {\n   value :  true/false \n}  Feature List:     SYSTEM_APPS  SYSTEM_ALERTS  APP_DATA_DASHBOARDS  RUNTIME_DAG_CHANGE  RUNTIME_PROPERTY_CHANGE  APP_CONTAINER_LOGS  LOGGING_LEVELS  APP_DATA_TRACKER  JAAS_LDAP_AUTH  APP_BUILDER   GET /ws/v2/config/properties  Function: Returns list of properties from dt-site.xml.  Return:  {\n     {name} : {\n         value :  {PROPERTY_VALUE} ,\n         description :  {PROPERTY_DESCRIPTION} \n    }\n\n}  GET /ws/v2/config/properties/{PROPERTY_NAME}  Function: Returns single property from dt-site.xml, specify by name  Return:  {\n     value :  {PROPERTY_VALUE} ,\n     description :  {PROPERTY_DESCRIPTION} \n}  POST /ws/v2/config/properties  Function: Overwrites all specified properties in dt-site.xml  Payload:  {\n     properties : [\n        {\n             name :  {name} \n             value :  {PROPERTY_VALUE} ,\n             local : true/false,\n                     description :  {PROPERTY_DESCRIPTION} \n        }, \u2026\n    ]\n}  PUT /ws/v2/config/properties/{PROPERTY_NAME}  Function: Overwrites or creates new property in dt-site.xml\nPayload:  {\n     value :  {PROPERTY_VALUE} ,\n     local : true/false,\n     description :  {PROPERTY_DESCRIPTION} \n}  DELETE /ws/v2/config/properties/{PROPERTY_NAME}  Function: Deletes a property from dt-site.xml. This may have to be\nrestricted to custom properties?  GET /ws/v2/config/hadoopExecutable  Function: Returns the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}  PUT /ws/v2/config/hadoopExecutable  Function: Sets the hadoop executable  Return:  {\n     value :  {PROPERTY_VALUE} ,\n}  GET /ws/v2/config/issues  Function: Returns list of potential issues with environment  Return:  {\n     issues : [\n        {\n             key :  {issueKey} ,\n             propertyName :  {PROPERTY_NAME} ,\n             description :  {ISSUE_DESCRIPTION} ,\n             severity :  error | warning \n        },\n        {...},\n        {...}\n    ]    \n}  GET /ws/v2/config/ipAddresses  Function: Returns list of ip addresses the gateway can listen to  Return:  {\n     ipAddresses : [\n       1.2.3.4 , ...\n    ]    \n}  POST /ws/v2/config/restart  Function: Restarts the gateway  Payload: none  GET /proxy/rm/v1/\u2026  POST /proxy/rm/v1/\u2026  Function: Proxy calls to resource manager of Hadoop.  Only works for GET and POST calls.  GET /proxy/stram/v2/...  POST /proxy/stram/v2/\u2026  PUT /proxy/stram/v2/\u2026  DELETE /proxy/stram/v2/\u2026  Function: Proxy calls to Stram Web Services.  POST /ws/v2/applications/{appid}/loggers  Function: Set the logger levels of packages/classes.  Payload:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}  GET /ws/v2/applications/{appid}/loggers  Function: Gets the logger levels of packages/classes.  Return:  {\n     loggers  : [\n        {\n             logLevel : value,\n             target : value\n        }, \n        ...\n    ]\n}  GET /ws/v2/applications/{appid}/loggers/search?pattern=\"{pattern}\"  Function: searches for all classes that match the pattern.  Return:  {\n     loggers  : [\n        {\n             name  :  {fully qualified class name} ,\n             level :  {logger level} \n        }\n    ]\n}  GET /ws/v2/appPackages  Since: 1.0.4  Function: Gets the list of appPackages the user can view in the system  {\n     appPackages : [\n        {\n                  appPackageName :  {appPackageName} ,\n                  appPackageVersion :  {appPackageVersion} ,\n             modificationTime :  {modificationTime} ,\n             owner :  {owner} ,\n        }, ...\n    ]\n}  POST /ws/v2/appPackages?merge={replace|fail|ours|theirs}  Since: 1.0.4  Function: Uploads an appPackage file, merge with existing app package if exists. Default is replace.  Payload: the raw zip file  Return: The information of the app package  GET /ws/v2/appPackages/{owner}/{name}  Since: 1.0.4  Function: Gets the list of versions of appPackages with the given name in the system owned by the specified user  {\n     versions : [\n         1.0-SNAPSHOT \n    ]\n}  DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}  Since: 1.0.4  Function: Deletes the appPackage  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/download  Since: 1.0.4  Function: Downloads the appPackage zip file  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}  Since: 1.0.4  Function: Gets the meta information of the app package  Returns:  {\n     appPackageName :  {appPackageName} ,\n     appPackageVersion :  {appPackageVersion} ,\n     modificationTime :   {modificationTime} ,\n     owner :  {owner} ,\n    ...\n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs  Since: 1.0.4\nFunction: Gets the list of configurations of the app package\nReturns:  {\n     configs : [\n         my-app-conf1.xml \n    ]\n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}  Since: 1.0.4  Function: Gets the properties XML of the specified config  Returns:  configuration \n         property \n                 name ... /name \n                 value ... /value \n         /property \n        \u2026 /configuration   PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}  Since: 1.0.4  Function: Creates or replaces the specified config with the property parameters specified payload  Payload: configuration in XML  DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/configs/{configName}  Since: 1.0.4  Function: Deletes the specified config  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications  Since: 1.0.4  Function: Gets the list of applications in the appPackage  Returns:  {\n     applications : [\n        {\n             dag : {dag in json format},\n             file :  {fileName} ,\n             name :  {name} ,\n             type :  {type} ,\n             error :  {error} ,\n             fileContent : {originalFileContentForJSONTypeApp}\n        }\n    ]\n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}  Since: 1.0.4  Function: Gets the meta data for that application  Returns:  {\n     file :  {fileName} ,\n     name :  {name} ,\n     type :  {json/class/properties} ,\n     error :  {error} \n     dag : {\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n         }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n    },\n     fileContent : {originalFileContentForJSONTypeApp}\n}  POST /ws/v2/appPackages/{user}/{appPackageName}/{appPackageVersion}/merge  Function: Merge the configuration, json apps, and resources files from the version specified from the payload to the specified app package in the url, without overwriting any existing file in the specified app package  Payload:  {\n  version :  {versionToMergeFrom} \n}  POST /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{appName}/launch[?config={configName} originalAppId={originalAppId} queue={queueName}]  Since: 1.0.4  Function: Launches the application with the given configuration specified in the POST payload  Payload:  {\n     {propertyName}  :  {propertyValue} , ...\n}  Return:  {\n     appId :  {appId} \n}  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators/{classname}  Since: 1.0.4  Function: Get the properties of the operator given the classname in the jar  {  \n     properties : [  \n        {\n           name : {className} ,\n           canGet : {canGet},\n           canSet : {canSet},\n           type : {type} ,\n           description : {description} ,\n           properties : ...\n        },\n       \u2026\n     ]\n}  PUT /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}[?errorIfExists={true/false}]  Function: Creates or Replaces an application using json. Note that \"ports\" are only needed if you need to specify port attributes.  If errorIfExists is true, it returns an error if the application with the same name already exists in the app ackage  Payload:  {\n         displayName :  {displayName} ,\n         description :  {description} ,\n         operators : [\n          {\n             name :  {name} ,\n             attributes :  {\n                 {attributeKey} :  {attributeValue} , ...\n            },\n             class :  {class} ,\n             ports : [\n                  {\n                     name :  {name} ,\n                     attributes :  {\n                        {attributeKey} :  {attributeValue} , ...\n                     },\n                  }, ...\n            ],\n             properties : {\n                {propertyName} :  {propertyValue} \n            }\n          }, ...\n        ],\n         streams : [\n          {\n             name :  {name} ,\n             locality :  {locality} ,\n             sinks : [\n                {\n                     operatorName :  {operatorName} ,\n                     portName :  {portName} \n                }, ...\n            ],\n             source : {\n                 operatorName :  {operatorName} ,\n                 portName :  {portName} \n            }\n          }, ...\n        ]\n}  Return:  {\n         error :  {error} \n}  Available port attributes to set:    AUTO_RECORD  IS_OUTPUT_UNIFIED  PARTITION_PARALLEL  QUEUE_CAPACITY  SPIN_MILLIS  STREAM_CODEC  UNIFIER_LIMIT   Available locality options to set:    THREAD_LOCAL  CONTAINER_LOCAL  NODE_LOCAL  RACK_LOCAL   DELETE /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/applications/{applicationName}  Since: 1.0.5  Function: Deletes non-jar based application in the app package  GET /ws/v2/appPackages/{owner}/{packageName}/{packageVersion}/operators  Since: 1.0.5  Function: Get the classes of operators from specified app package.  Return:  {  \n     operatorClasses : [  \n        {\n             name : {fullyQualifiedClassName} , \n             title :  {title} ,\n             shortDesc :  {description} ,\n             longDesc :  {description} ,\n             category :  {categoryName} ,\n             doclink :  {doc url} ,\n             tags : [  {tag} ,  {tag} , \u2026 ],\n             inputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ]\n             outputPorts : [\n                {\n                     name :  {portName} ,\n                     type :  {tupleType} ,\n                     optional : true/false  \n                }, \u2026\n            ],\n             properties : [  \n                {\n                     name : {propertyName} ,\n                     canGet : {canGet},\n                     canSet : {canSet},\n                     type : {type} ,\n                     description : {description} ,\n                     properties : ...\n                }, \u2026\n            ],\n             defaultValue : {\n                 {propertyName} : [VALUE], // type depends on property\n                ...\n            }\n\n        }, \u2026\n    ]\n}  GET /ws/v2/appPackages/import  Function: List the importable app packages on Gateway's local file\nsystem  Return:  {\n     appPackages: [\n        {\n             file :  {file} ,\n             name :  {name} ,\n             displayName :  {displayName} ,\n             version :  {version} ,\n             description :  {description} \n        }\n    ]\n}  POST /ws/v2/appPackages/import  Function: Import app package from Gateway's local file system  Payload:  {\n         files : [ {file} , \u2026 ]\n}  PUT /ws/v2/systemAlerts/alerts/{name}  Function: Creates or replaces the specified system alert. The condition has access to an object in its scope called  _topic . An example alert might take the form of the following:  _topic[\"applications.application_1400294100000_0001\"].allocatedContainers   5  Payload:  {\n         condition : {condition in javascript} ,\n         email : {email} ,\n         timeThresholdMillis : {time} \n}  DELETE /ws/v2/systemAlerts/alerts/{name}  Function: Deletes the specified system alert  GET /ws/v2/systemAlerts/alerts?inAlert={true/false}  Function: Gets the created alerts  Return:  {\n     alerts : [{\n         name :  {alertName} ,\n         condition : {condition in javascript} ,\n         email : {email} ,\n         timeThresholdMillis : {time} ,\n         alertStatus : {\n             isInAlert :{true/false}\n             inTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }\n    }, \u2026  ]\n}  GET /ws/v2/systemAlerts/alerts/{name}  Function: Gets the specified system alert  Return:  {\n     name :  {alertName} ,\n     condition : {condition in javascript} ,        \n     email : {email} ,\n     timeThresholdMillis : {time} ,\n     alertStatus : {\n         isInAlert :{true/false}\n         inTime :  {time} ,\n         message :  {message} ,\n         emailSent : {true/false}\n    }\n}  GET /ws/v2/systemAlerts/history  Function: Gets the history of alerts  Return:  {\n     history : [\n        {\n             name : {alertName} ,\n             inTime : {time} ,\n             outTime :  {time} ,\n             message :  {message} ,\n             emailSent : {true/false}\n        }, ...\n     ]\n}  GET /ws/v2/systemAlerts/topicData  Function: Gets the topic data that is used for evaluating alert\ncondition  Return:  {\n      {topicName} : {json object data}, ...\n}  GET /ws/v2/auth/users/{user}  Function: Gets the info of the given user  Return:  {\n     userName :  {userName} ,\n     roles : [  {role1} ,  {role2}  ]\n}  POST /ws/v2/auth/users/{user}  Function: Changes password and/or roles of the given user  Return:  {\n     userName :  {userName} ,\n     oldPassword :  {oldPassword} ,\n     newPassword :  {newPassword} ,\n     roles : [  {role1} ,  {role2}  ]\n}  PUT /ws/v2/auth/users/{user}  Function: Creates new user  Return:  {\n     userName :  {userName} ,\n     password :  {password} ,\n     roles : [  {role1} ,  {role2}  ]\n}  DELETE /ws/v2/auth/users/{user}  Function: Deletes the specified user  GET /ws/v2/auth/users  Function: Gets the list of users  Return:  {\n     users : [ {\n        userName :  {username1} ,\n        roles : [  {role1} , \u2026 ],\n        permissions : [  {permission1} , \u2026 ]\n    }\n}  POST /ws/v2/login  Function: Login\nPayload:  {\n     userName :  {userName} ,\n     password :  {password} \n}  Return:  {\n     authScheme :  {authScheme} ,\n     userName  :  {userName} ,\n     roles : [  {role1} , \u2026 ],\n     permissions : [  {permission1} , \u2026 ]\n}  POST /ws/v2/logout  Function: Log out the current user  Return:  {\n}", 
            "title": "REST URI Specification"
        }, 
        {
            "location": "/dtgateway_api/#publisher-subscriber-websocket-protocol", 
            "text": "", 
            "title": "Publisher-Subscriber WebSocket Protocol"
        }, 
        {
            "location": "/dtgateway_api/#input", 
            "text": "Publishing  {\"type\":\"publish\", \"topic\":\"{topic}\", \"data\":{data}}  Subscribing  {\"type\":\"subscribe\", \"topic\":\"{topic}\"}  Unsubscribing  {\"type\":\"unsubscribe\", \"topic\":\"{topic}\"}  Subscribing to the number of subscribers of a topic  {\"type\":\"subscribeNumSubscribers\", \"topic\":\"{topic}\"}  Subscribing to the number of subscribers of a topic  {\"type\":\"unsubscribeNumSubscribers\", \"topic\":\"{topic}\"}", 
            "title": "Input"
        }, 
        {
            "location": "/dtgateway_api/#output", 
            "text": "Normal Published Data  {\"type\":\"data\", \"topic\":\"{topic}\", \"data\":{data}}  Number of Subscribers:  {\"type\":\"data\", \"topic\":\"{topic}.numSubscribers\", \"data\":{data}}", 
            "title": "Output"
        }, 
        {
            "location": "/dtgateway_api/#auto-publish-topics", 
            "text": "data that gets published every one second:   applications  - list of streaming applications running in the cluster  applications.[appid]  - information about a particular application  applications.[appid].containers  - information about containers of a particular application  applications.[appid].physicalOperators  - information about operators of a particular application  applications.[appid].logicalOperators  - information about logical operators of a particular application  applications.[appid].events  - events from the AM of a particularapplication   data that gets published every five seconds:   cluster.metrics  - metrics of the cluster", 
            "title": "Auto publish topics"
        }, 
        {
            "location": "/autometrics/", 
            "text": "Apache Apex AutoMetric API\n\n\nIntroduction\n\n\nMetrics help to collect some statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of \nAutoMetric\n API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.\n\n\nSpecifying AutoMetrics in an Operator\n\n\nAn \nAutoMetric\n can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a \nget\n method in an operator can be annotated with \n@AutoMetric\n to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.\n\n\npublic class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}\n\n\n\n\nThere are 2 auto-metrics declared in the \nLineReceiver\n. At the end of each application window, the platform will send a map with 2 entries - \n[(length, 100), (count, 10)]\n to the application master.\n\n\nAggregating AutoMetrics across Partitions\n\n\nWhen an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.\n\n\nThe AutoMetric API helps to achieve this by providing an interface for writing aggregators- \nAutoMetric.Aggregator\n. Any implementation of \nAutoMetric.Aggregator\n can be set as an operator attribute - \nMETRICS_AGGREGATOR\n for a particular operator which in turn is used for aggregating physical metrics.\n\n\nDefault aggregators\n\n\nMetricsAggregator\n is a simple implementation of \nAutoMetric.Aggregator\n that platform uses as a default for summing up primitive types - int, long, float and double.\n\n\nMetricsAggregator\n is just a collection of \nSingleMetricAggregator\ns. There are multiple implementations of \nSingleMetricAggregator\n that perform sum, min, max, avg which are present in Apex core and Apex malhar.\n\n\nFor the \nLineReceiver\n operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of \nMetricsAggregator\n that contains two \nLongSumAggregator\ns - one for \nlength\n and one for \ncount\n. This aggregator will report sum of length and sum of count across all the partitions of \nLineReceiver\n.\n\n\nBuilding custom aggregators\n\n\nPlatform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the \nLineReceiver\n was modified to have a complex metric as shown below.\n\n\npublic class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort\nString\n input = new DefaultInputPort\nString\n()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}\n\n\n\n\nBelow is a custom aggregator that can calculate average line length across all partitions of \nAnotherLineReceiver\n.\n\n\npublic class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map\nString, Object\n result = Maps.newHashMap();\n\n  @Override\n  public Map\nString, Object\n aggregate(long l, Collection\nAutoMetric.PhysicalMetricsContext\n collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get(\nlineMetrics\n);\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put(\navgLineLength\n, totalLength/totalCount);\n    return result;\n  }\n}\n\n\n\n\nAn instance of above aggregator can be specified as the \nMETRIC_AGGREGATOR\n for \nAnotherLineReceiver\n while creating the DAG as shown below.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }\n\n\n\n\nRetrieving AutoMetrics\n\n\nThe Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \nautoMetrics\n: {\n       \ncount\n: \n71314\n,\n       \nlength\n: \n27780706\n\n    },\n    \nclassName\n: \ncom.datatorrent.autometric.LineReceiver\n,\n    ...\n}\n\n\n\n\nSystem Metrics\n\n\nSystem metrics are standard operator metrics provided by the system.  Examples include:\n\n\n\n\nprocessed tuples per second\n\n\nemitted tuples per second\n\n\ntotal tuples processed\n\n\ntotal tuples emitted\n\n\nlatency\n\n\nCPU percentage\n\n\nfailure count\n\n\ncheckpoint elapsed time\n\n\n\n\nThe Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.\n\n\nGET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n    \ncpuPercentageMA\n: \n{cpuPercentageMA}\n,\n    \nfailureCount\n: \n{failureCount}\n,\n    \nlatencyMA\n: \n{latencyMA}\n,  \n    \ntotalTuplesEmitted\n: \n{totalTuplesEmitted}\n,\n    \ntotalTuplesProcessed\n: \n{totalTuplesProcessed}\n,\n    \ntuplesEmittedPSMA\n: \n{tuplesEmittedPSMA}\n,\n    \ntuplesProcessedPSMA\n: \n{tuplesProcessedPSMA}\n,\n    ...\n}\n\n\n\n\nHowever, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of App Data Tracker.\n\n\nApp Data Tracker\n\n\nAs discussed above, STRAM aggregates the AutoMetrics from physical operators (partitions) to something that makes sense in one logical operator.  It pushes the aggregated AutoMetrics values using Websocket to the Gateway at every second along with system metrics for each operator.  Gateway relays the information to an application called App Data Tracker.  It is another Apex application that runs in the background and further aggregates the incoming values by time bucket and stores the values in HDHT.  It also allows the outside to retrieve the aggregated AutoMetrics and system metrics through websocket interface.\n\n\n\n\nApp Data Tracker is enabled by having these properties in dt-site.xml:\n\n\nproperty\n\n  \nname\ndt.appDataTracker.enable\n/name\n\n  \nvalue\ntrue\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.appDataTracker.transport\n/name\n\n  \nvalue\nbuiltin:AppDataTrackerFeed\n/value\n\n\n/property\n\n\nproperty\n\n  \nname\ndt.attr.METRICS_TRANSPORT\n/name\n\n  \nvalue\nbuiltin:AppDataTrackerFeed\n/value\n\n\n/property\n\n\n\n\n\nAll the applications launched after the App Data Tracker is enabled will have metrics sent to it.\n\n\nNote\n: The App Data Tracker will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.\n\n\nBy default, the time buckets App Data Tracker aggregates upon are one minute, one hour and one day.  It can be overridden by changing the operator attribute \nMETRICS_DIMENSIONS_SCHEME\n.\n\n\nAlso by default, the app data tracker performs all these aggregations: SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute \nMETRICS_DIMENSIONS_SCHEME\n, provided the custom aggregator is known to the App Data Tracker.  (See next section)\n\n\nCustom Aggregator in App Data Tracker\n\n\nCustom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to implement a Custom aggregator you have to do two things:\n\n\n\n\nCombining new inputs with the current aggregation\n\n\nCombining two aggregations together into one aggregation\n\n\n\n\nLet\u2019s consider the case where we want to perform the following rolling average:\n\n\nY_n = \u00bd * X_n + \u00bd * X_n-1 + \u00bc * X_n-2 + \u215b * X_n-3 +...\n\n\nThis aggregation could be performed by the following Custom Aggregator:\n\n\n@Name(\nIIRAVG\n)\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index \n destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}\n\n\n\n\nDiscovery of Custom Aggregators\n\n\nAppDataTracker searches for custom aggregator jars under the following directories statically before launching:\n\n\n\n\n{dt_installation_dir}/plugin/aggregators\n\n\n{user_home_dir}/.dt/plugin/aggregators\n\n\n\n\nIt uses reflection to find all the classes that extend from \nIncrementalAggregator\n and \nOTFAggregator\n in these jars and registers them with the name provided by \n@Name\n annotation (or class name when \n@Name\n is absent).\n\n\nUsing \nMETRICS_DIMENSIONS_SCHEME\n\n\nHere is a sample code snippet on how you can make use of \nMETRICS_DIMENSIONS_SCHEME\n to set your own time buckets and your own set of aggregators for certain \nAutoMetric\ns performed by the App Data Tracker in your application.\n\n\n  @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator(\nLineReceiver\n, new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] { \n1s\n, \n1m\n, \n1h\n };\n      String[] lengthAggregators = new String[] { \nIIRAVG\n, \nSUM\n };\n      String[] countAggregators = new String[] { \nSUM\n };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if (\nlength\n.equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if (\ncount\n.equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }\n\n\n\n\nDashboards\n\n\nWith App Data Tracker enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the diagram in the App Data Tracker section, dtGateway relays queries and query results to and from the App Data Tracker.  In this way, dtManage sends queries and receives results from the App Data Tracker via dtGateway and uses the results to let the user visualize the data.\n\n\nClick on the visualize button in dtManage's application page.\n\n\n\n\nYou will see the dashboard for the AutoMetrics and the system metrics.\n\n\n\n\nThe left widget shows the AutoMetrics of \nline\n and \ncount\n for the LineReceiver operator.  The right widget shows the system metrics.\n\n\nThe Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "AutoMetric API"
        }, 
        {
            "location": "/autometrics/#apache-apex-autometric-api", 
            "text": "", 
            "title": "Apache Apex AutoMetric API"
        }, 
        {
            "location": "/autometrics/#introduction", 
            "text": "Metrics help to collect some statistical information about a process which can be very useful for diagnosis. Auto Metrics in Apex can help monitor operators in a running application.  The goal of  AutoMetric  API is to enable operator developer to define relevant metrics for an operator in a simple way which the platform collects and reports automatically.", 
            "title": "Introduction"
        }, 
        {
            "location": "/autometrics/#specifying-autometrics-in-an-operator", 
            "text": "An  AutoMetric  can be any object. It can be of a primitive type - int, long, etc. or a complex one. A field or a  get  method in an operator can be annotated with  @AutoMetric  to specify that its value is a metric. After every application end window, the platform collects the values of these fields/methods in a map and sends it to application master.  public class LineReceiver extends BaseOperator\n{\n @AutoMetric\n long length;\n\n @AutoMetric\n long count;\n\n public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n {\n   @Override\n   public void process(String s)\n   {\n     length += s.length();\n     count++;\n   }\n };\n\n @Override\n public void beginWindow(long windowId)\n {\n   length = 0;\n   count = 0;\n }\n}  There are 2 auto-metrics declared in the  LineReceiver . At the end of each application window, the platform will send a map with 2 entries -  [(length, 100), (count, 10)]  to the application master.", 
            "title": "Specifying AutoMetrics in an Operator"
        }, 
        {
            "location": "/autometrics/#aggregating-autometrics-across-partitions", 
            "text": "When an operator is partitioned, it is useful to aggregate the values of auto-metrics across all its partitions every window to get a logical view of these metrics. The application master performs these aggregations using metrics aggregators.  The AutoMetric API helps to achieve this by providing an interface for writing aggregators-  AutoMetric.Aggregator . Any implementation of  AutoMetric.Aggregator  can be set as an operator attribute -  METRICS_AGGREGATOR  for a particular operator which in turn is used for aggregating physical metrics.", 
            "title": "Aggregating AutoMetrics across Partitions"
        }, 
        {
            "location": "/autometrics/#default-aggregators", 
            "text": "MetricsAggregator  is a simple implementation of  AutoMetric.Aggregator  that platform uses as a default for summing up primitive types - int, long, float and double.  MetricsAggregator  is just a collection of  SingleMetricAggregator s. There are multiple implementations of  SingleMetricAggregator  that perform sum, min, max, avg which are present in Apex core and Apex malhar.  For the  LineReceiver  operator, the application developer need not specify any aggregator. The platform will automatically inject an instance of  MetricsAggregator  that contains two  LongSumAggregator s - one for  length  and one for  count . This aggregator will report sum of length and sum of count across all the partitions of  LineReceiver .", 
            "title": "Default aggregators"
        }, 
        {
            "location": "/autometrics/#building-custom-aggregators", 
            "text": "Platform cannot perform any meaningful aggregations for non-numeric metrics. In such cases, the operator or application developer can write custom aggregators. Let\u2019s say, if the  LineReceiver  was modified to have a complex metric as shown below.  public class AnotherLineReceiver extends BaseOperator\n{\n  @AutoMetric\n  final LineMetrics lineMetrics = new LineMetrics();\n\n  public final transient DefaultInputPort String  input = new DefaultInputPort String ()\n  {\n    @Override\n    public void process(String s)\n    {\n      lineMetrics.length += s.length();\n      lineMetrics.count++;\n    }\n  };\n\n  @Override\n  public void beginWindow(long windowId)\n  {\n    lineMetrics.length = 0;\n    lineMetrics.count = 0;\n  }\n\n  public static class LineMetrics implements Serializable\n  {\n    long length;\n    long count;\n\n    private static final long serialVersionUID = 201511041908L;\n  }\n}  Below is a custom aggregator that can calculate average line length across all partitions of  AnotherLineReceiver .  public class AvgLineLengthAggregator implements AutoMetric.Aggregator\n{\n\n  Map String, Object  result = Maps.newHashMap();\n\n  @Override\n  public Map String, Object  aggregate(long l, Collection AutoMetric.PhysicalMetricsContext  collection)\n  {\n    long totalLength = 0;\n    long totalCount = 0;\n    for (AutoMetric.PhysicalMetricsContext pmc : collection) {\n      AnotherLineReceiver.LineMetrics lm = (AnotherLineReceiver.LineMetrics)pmc.getMetrics().get( lineMetrics );\n      totalLength += lm.length;\n      totalCount += lm.count;\n    }\n    result.put( avgLineLength , totalLength/totalCount);\n    return result;\n  }\n}  An instance of above aggregator can be specified as the  METRIC_AGGREGATOR  for  AnotherLineReceiver  while creating the DAG as shown below.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    AnotherLineReceiver lineReceiver = dag.addOperator( LineReceiver , new AnotherLineReceiver());\n    dag.setAttribute(lineReceiver, Context.OperatorContext.METRICS_AGGREGATOR, new AvgLineLengthAggregator());\n    ...\n  }", 
            "title": "Building custom aggregators"
        }, 
        {
            "location": "/autometrics/#retrieving-autometrics", 
            "text": "The Gateway REST API provides a way to retrieve the latest AutoMetrics for each logical operator.  For example:  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     autoMetrics : {\n        count :  71314 ,\n        length :  27780706 \n    },\n     className :  com.datatorrent.autometric.LineReceiver ,\n    ...\n}", 
            "title": "Retrieving AutoMetrics"
        }, 
        {
            "location": "/autometrics/#system-metrics", 
            "text": "System metrics are standard operator metrics provided by the system.  Examples include:   processed tuples per second  emitted tuples per second  total tuples processed  total tuples emitted  latency  CPU percentage  failure count  checkpoint elapsed time   The Gateway REST API provides a way to retrieve the latest values for all of the above for each of the logical operators in the application.  GET /ws/v2/applications/{appid}/logicalPlan/operators/{opName}\n{\n    ...\n     cpuPercentageMA :  {cpuPercentageMA} ,\n     failureCount :  {failureCount} ,\n     latencyMA :  {latencyMA} ,  \n     totalTuplesEmitted :  {totalTuplesEmitted} ,\n     totalTuplesProcessed :  {totalTuplesProcessed} ,\n     tuplesEmittedPSMA :  {tuplesEmittedPSMA} ,\n     tuplesProcessedPSMA :  {tuplesProcessedPSMA} ,\n    ...\n}  However, just like AutoMetrics, the Gateway only provides the latest metrics.  For historical metrics, we will need the help of App Data Tracker.", 
            "title": "System Metrics"
        }, 
        {
            "location": "/autometrics/#app-data-tracker", 
            "text": "As discussed above, STRAM aggregates the AutoMetrics from physical operators (partitions) to something that makes sense in one logical operator.  It pushes the aggregated AutoMetrics values using Websocket to the Gateway at every second along with system metrics for each operator.  Gateway relays the information to an application called App Data Tracker.  It is another Apex application that runs in the background and further aggregates the incoming values by time bucket and stores the values in HDHT.  It also allows the outside to retrieve the aggregated AutoMetrics and system metrics through websocket interface.   App Data Tracker is enabled by having these properties in dt-site.xml:  property \n   name dt.appDataTracker.enable /name \n   value true /value  /property  property \n   name dt.appDataTracker.transport /name \n   value builtin:AppDataTrackerFeed /value  /property  property \n   name dt.attr.METRICS_TRANSPORT /name \n   value builtin:AppDataTrackerFeed /value  /property   All the applications launched after the App Data Tracker is enabled will have metrics sent to it.  Note : The App Data Tracker will be shown running in dtManage as a \u201csystem app\u201d.  It will show up if the \u201cshow system apps\u201d button is pressed.  By default, the time buckets App Data Tracker aggregates upon are one minute, one hour and one day.  It can be overridden by changing the operator attribute  METRICS_DIMENSIONS_SCHEME .  Also by default, the app data tracker performs all these aggregations: SUM, MIN, MAX, AVG, COUNT, FIRST, LAST on all number metrics.  You can also override by changing the same operator attribute  METRICS_DIMENSIONS_SCHEME , provided the custom aggregator is known to the App Data Tracker.  (See next section)", 
            "title": "App Data Tracker"
        }, 
        {
            "location": "/autometrics/#custom-aggregator-in-app-data-tracker", 
            "text": "Custom aggregators allow you to do your own custom computation on statistics generated by any of your applications. In order to implement a Custom aggregator you have to do two things:   Combining new inputs with the current aggregation  Combining two aggregations together into one aggregation   Let\u2019s consider the case where we want to perform the following rolling average:  Y_n = \u00bd * X_n + \u00bd * X_n-1 + \u00bc * X_n-2 + \u215b * X_n-3 +...  This aggregation could be performed by the following Custom Aggregator:  @Name( IIRAVG )\npublic class AggregatorIIRAVG extends AbstractIncrementalAggregator\n{\n  ...\n\n  private void aggregateHelper(DimensionsEvent dest, DimensionsEvent src)\n  {\n    double[] destVals = dest.getAggregates().getFieldsDouble();\n    double[] srcVals = src.getAggregates().getFieldsDouble();\n\n    for (int index = 0; index   destLongs.length; index++) {\n      destVals[index] = .5 * destVals[index] + .5 * srcVals[index];\n    }\n  }\n\n  @Override\n  public void aggregate(Aggregate dest, InputEvent src)\n  {\n    //Aggregate a current aggregation with a new input\n    aggregateHelper(dest, src);\n  }\n\n  @Override\n  public void aggregate(Aggregate destAgg, Aggregate srcAgg)\n  {\n    //Combine two existing aggregations together\n    aggregateHelper(destAgg, srcAgg);\n  }\n}", 
            "title": "Custom Aggregator in App Data Tracker"
        }, 
        {
            "location": "/autometrics/#discovery-of-custom-aggregators", 
            "text": "AppDataTracker searches for custom aggregator jars under the following directories statically before launching:   {dt_installation_dir}/plugin/aggregators  {user_home_dir}/.dt/plugin/aggregators   It uses reflection to find all the classes that extend from  IncrementalAggregator  and  OTFAggregator  in these jars and registers them with the name provided by  @Name  annotation (or class name when  @Name  is absent).", 
            "title": "Discovery of Custom Aggregators"
        }, 
        {
            "location": "/autometrics/#using-metrics_dimensions_scheme", 
            "text": "Here is a sample code snippet on how you can make use of  METRICS_DIMENSIONS_SCHEME  to set your own time buckets and your own set of aggregators for certain  AutoMetric s performed by the App Data Tracker in your application.    @Override\n  public void populateDAG(DAG dag, Configuration configuration)\n  {\n    ...\n    LineReceiver lineReceiver = dag.addOperator( LineReceiver , new LineReceiver());\n    ...\n    AutoMetric.DimensionsScheme dimensionsScheme = new AutoMetric.DimensionsScheme()\n    {\n      String[] timeBuckets = new String[] {  1s ,  1m ,  1h  };\n      String[] lengthAggregators = new String[] {  IIRAVG ,  SUM  };\n      String[] countAggregators = new String[] {  SUM  };\n\n      /* Setting the aggregation time bucket to be one second, one minute and one hour */\n      @Override\n      public String[] getTimeBuckets()\n      {\n        return timeBuckets;\n      }\n\n      @Override\n      public String[] getDimensionAggregationsFor(String logicalMetricName)\n      {\n        if ( length .equals(logicalMetricName)) {\n          return lengthAggregators;\n        } else if ( count .equals(logicalMetricName)) {\n          return countAggregators;\n        } else {\n          return null; // use default\n        }\n      }\n    };\n\n    dag.setAttribute(lineReceiver, OperatorContext.METRICS_DIMENSIONS_SCHEME, dimensionsScheme);\n    ...\n  }", 
            "title": "Using METRICS_DIMENSIONS_SCHEME"
        }, 
        {
            "location": "/autometrics/#dashboards", 
            "text": "With App Data Tracker enabled, you can visualize the AutoMetrics and system metrics in the Dashboards within dtManage.   Refer back to the diagram in the App Data Tracker section, dtGateway relays queries and query results to and from the App Data Tracker.  In this way, dtManage sends queries and receives results from the App Data Tracker via dtGateway and uses the results to let the user visualize the data.  Click on the visualize button in dtManage's application page.   You will see the dashboard for the AutoMetrics and the system metrics.   The left widget shows the AutoMetrics of  line  and  count  for the LineReceiver operator.  The right widget shows the system metrics.  The Dashboards have some simple builtin widgets to visualize the data.  Line charts and bar charts are some examples.\nUsers will be able to implement their own widgets to visualize their data.", 
            "title": "Dashboards"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Scalable Design"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtingest/", 
            "text": "dtIngest Tutorial\n\n\n\"dtIngest\" is a datatorrent application that can read data from various\nsources and store the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the \ndtIngest\nblog\n.\n\n\nThis tutorial refers to dtIngest version 1.0.0\n\n\nPre-requisites\n\n\n\n\n\n\nDatatorrent RTS should be installed on your hadoop cluster. Please\n    refer \nInstallation\n    guide\n\u00a0for\n    installation steps.\n\n\n\n\n\n\nSource and destination file systems should be accessible from all\n    nodes of the cluster running DataTorrent RTS. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.\n\n\n\n\n\n\nIn case your source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.\n\n\n\n\n\n\nLaunching ingestion application\n\n\ndtIngest application can be configured and launched from \nDatatorrent\nManagement\nConsole\n.\n\n\n\n\n\n\nNavigate to 'Develop' tab.\n    \n\n\n\n\n\n\nThe dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0\n\n\n\n\n\n\nSelect 'Ingestion Application' from list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n    \n\n\n\n\n\n\nConfiguration page for dtingest is displayed after the 'launch'.\n    Just provide desired values for the configuration and click\n    'Launch' to ingest your data.\n\n\n\n\n\n\nConfiguring dtingest instance properties\n\n\n\n\n\n\nIn the 'Name this application' textbox; enter suitable name of\n    your choice for this ingestion instance. For example, 'Ingestion\n    test'\n    \n\n\n\n\n\n\nUnder 'Specify a queue' option; keep the 'Specify a queue'\n    checkbox unselected if you want to use default queue.\n\n\nIf you want to specify some specific queue to launch this application; then\nselect the 'Specify a queue' checkbox and select desired queue from the\ndropdown. To know more about this read \nHadoop Capacity Scheduler Docs\n\n\n\n\n\n\n\n\nUnder 'Use config a file' option; select the checkbox if you wish\n    to use some configuration which is already saved. In the drop down\n    select the desired configuration.\n    After selection, all the input options will load the values saved in the configuration.\n    \n\n\nAfter values are loaded you can modify the values of any option to the\ndesired value. You can also save this new combination of values as a\nconfiguration under the same name or different name.\n\n\nIf you want to specify all the options from scratch without using any pre-saved configuration; then unselect a checkbox 'Use a config file'\n\n\n\n\n\n\nConfigure input source, refer to \nConfiguring input\n    source\n\u00a0section for details.\n\n\n\n\n\n\nConfigure output destination, refer to \nConfiguring output\n    destination\n\u00a0section for details.\n\n\n\n\n\n\nConfigure processing steps, refer to \nConfiguring processing\n    steps\n\u00a0section for details.\n\n\n\n\n\n\nUnder 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.\u00a0\n\n\n\n\n\n\nConfiguring input source\n\n\nConfiguring HDFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n    \nhdfs://namenode1.cluster.company.org:8020/user/john/data\n.\n\n    Where,\n\n\n\n\nhdfs://\n indicates HDFS protocol\n\n\nnamenode1.cluster.company.org\n indicates fully qualified domain\n   name for the namenode of source HDFS.\n\n\n8020\n indicates port number for HDFS namenode service\n\n\n/user/john/data\n indicates full path for destination directory  \n\n\n\n\n\n\nIf there are more than one directories/file to be ingested; click on\n'Add directory' button. Then specify complete URL for the file path to be\ningested as specified above\n\n\n\n\n\n\nIn the 'Filtering criteria' field; specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n    \nJava regular expression  documentation\n.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.  \n\n\n\n\nWhere,\n-   \n.*\n indicates any character zero or more times\n-   \n\\.\n indicates dot escaped with backslash\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only '.log' files from the source\ndirectories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you expect\n    application to shutdown after ingesting files currently present in\n    the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based on\n'modification timestamp'. Entire file will be ingested again in case of\nany change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.  \n\n\n\n\n\n\n\n\nConfiguring\u00a0NFS input\n\n\n\n\n\n\nFor 'Input data source' field; select 'File/NFS' option from the\n    drop-down. \n\n\n\n\n\n\nUnder 'Source directories'; specify complete URL for the file path\n    to be ingested.\n\n\nFor example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk5/nfsmount/path/to/data/directory\n.\nWhere,\n-   \nfile://\n indicates that it is some file system mounted on the node.\n-   \n/disk5/nfsmount/\n indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-   \npath/to/data/directory\n is the directory to be ingested\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nIf there are more than one directories to be ingested; click on\n    'Add directory' button. Then specify complete URL for the file\n    path to be ingested as specified in point 3.\n\n\n\n\n\n\nIf there are specific files to be ingested; then specify complete\n    URL for the file path to be ingested.\n\n\nFor example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be \nfile:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt\n.\n\n\n\n\n\n\n\n\nIn the 'Filtering criteria' field; specify regular expression for\n    files to be copied.\n    For example, if only \n.log\n files need to be ingested; then use \n.*\\.log\n as regular expression.\n\n\n\n\nWhere,\n-   \n.\\*\n indicates any character zero or more times\n-   \n\\\\.\n indicates dot escaped with backslash '\\'\n-   \nlog\n indicates desired extension which is 'log'\n\n\nIn this case, dtingest ingests only \n.log\n files from the source directories.\n\n\n\n\n\n\nUnder 'Runs' field, select 'Single run' if you expect application\n    to shutdown after ingesting files currently present in the directory.\n\n\nSelect 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based\non 'modification timestamp'. Entire file will be ingested again in case\nof any change.\n\n\nIf 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.\n\n\n\n\n\n\n\n\nConfiguring FTP input\n\n\nFTP is most widely used transfer protocol, used for transfer files from\none host to another. Use dtIngest, to copy files/directory from ftp\nsource location to some destination. This section gives details about\nhow to ingest files/directories from FTP using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0FTP as input type\n    \n\n\n\n\n\n\nAfter selecting the FTP as input type, snapshot of UI as below:     \n\n\n\n\n\n\nConfigure FTP input url\n    Input url for FTP needs to be provided in following format,  \nftp://username:password@host:port/path\n\n    where,\n\n\n\n\nftp\n : \u00a0protocol name\n\n\nusername\n : \u00a0username for ftp server\n\n\npassword\n : password\n\n\nhost\n : FTP host\n\n\nport\n : port number\n\n\npath\n : path to either file / directory\n\n\n\n\n\n\nIf you want to copy multiple files/directories, click on (+) button. If\nyou are copying multiple files, then UI would be as follows:\n\n\n\nIf you are copying multiple directories, then UI would be as follows:\n\n\n\n\n\n\n\nConfiguring Amazon S3 input\n\n\nS3 is clustered storage service of Amazon. Amazon s3 provides a web\nservices interface that can be used to store and retrieve any amount of\ndata, at any time, from anywhere on web. For more details about Amazon\nS3, please refer to \nAmazon S3\nDocumentation\n.\n\n\nUse dtIngest, you can copy files/directory efficiently from S3 source\nlocation to some destination. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0\n\n\n\n\n\n\nSelect \u00a0S3 as input type\n\n\n\n\n\n\n\nAfter selecting the S3 as source type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure S3 input url.\n\n\nInput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\n\nwhere,\n- \ns3n\n: \u00a0protocol name\n- \nukey\n: access key\n- \nupass\n: secret access key\n- \nbucketName\n : bucketName\n- \npath\n : path to either file / directory\n\n\n\n\n\n\n\n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n  \n\n\nConfiguring Kafka input\n\n\nKafka is a distributed publish subscribe messaging system. Since kafka\nis distributed system, topics are partitioned and replicated across\nnodes. Messages in kafka are expired after some time. For more details\nabout Kafka, please refer to \nApache Kafka\nDocumentation\n.\n\n\nWe need to copy messages from multiple partitions to one destination\nbefore expiry. This section gives details about how to ingest messages\nfrom kafka using dtIngest.\n\n\n\n\n\n\nSelect kafka as input type\n    \n\n\n\n\n\n\nAfter selecting kafka as input type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure topic name and zookeeper quorum.\n    zookeeper quorum \u00a0is a string in the form of\n    \nhostname1:port1,hostname2:port2,hostname3:port3\n\n\nwhere,\n\n\n\n\nhostname1,hostname2,hostname3\n are hosts\n\n\nport1,port2,port3\n are ports of zookeeper server\n\n\n\n\ne.g. localhost:2181,localhost:2182\n\n\n\n\n\n\n\nSelect the offset type. By default, \u201cLatest\u201d offset is enabled. If\n    you want to consume messages from beginning of kafka queue, then\n    select \u201cEarliest\u201d offset option.\n\n\n\n\n\n\nIf the topic name is same across the kafka clusters and want to\n    ingest data from these clusters, then configure the zookeeper quorum\n    as follows:\n\n\nc1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6\n\n\nwhere,\n- \nc1,c2,c3\n indicates the cluster names,\n- \nhs1,hs2,hs3,hs4,hs5,hs6\n are zookeeper host names\n- \np1,p2,p3,p4,p5,p6\n are corresponding ports.\n\n\nFor\u00a0example, ClusterA and ClusterB are 2 kafka clusters as below, then\nzookeeper quorum would be as \nClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181\n\n\n\n\n\n\n\n\nConfiguring JMS input\n\n\nJMS provides the facility to create, send and read messages. You can\ncopy messages from JMS source location to some destination using\ndtIngest. This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0\n\n\n\n\n\n\nSelect JMS as input type.\n    \n\n\n\n\n\n\nAfter selecting the JMS as source type then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure Broker URL and topic name.\n    Broker url would be in the form of tcp://hostName:port\n    \n\n\n\n\n\n\nConfiguring\u00a0output destination\n\n\nConfiguring HDFS output\n\n\n\n\n\n\nFor 'Output Location' field; select 'HDFS' option from the\n    drop-down.\n\n\n\n\n\n\n\n\nUnder 'Target directory' specify complete URL for the HDFS path\n    for the destination directory. For example, URL should be like\n\nhdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory\n\n\n\n\n\n\n\n\nWhere,\n    - \nhdfs://\n indicates HDFS protocol\n    - \nnamenode1.cluster.company.org\n indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    - \n:8020\n indicates port number for HDFS namenode service\n    - \n/user/username/path/to/destination/directory\n indicates full path\n    for destination directory.\n\n\n\n\n\n\nUnder 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.\n\n\n\n\n\n\nUnder 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.\n\n\n\n\n\n\nCompact files\n\n\nIf you want to copy the data to the HDFS and partition the data into\npartitions of fixed size; use 'Compact files' feature to do this. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Additionally, you may also use this to breakdown very\nlarge file into partitions of manageable size.\n\n\n\n\n\n\n\n\nSelect 'yes' for radio button under 'Compact files' option. This\n    will display other additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.\n\n\n\n\n\n\nSelect delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with \n\\\n. For example, tab character\n    \n\\t\n should be specified as \n\\\\t\n.\n\n\n\n\n\n\nSpecify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.\n\n\n\n\n\n\nNote that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.\n\n\nConfiguring NFS output\n\n\n\n\n\n\nFor \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down  \n\n\n\n\n\n\n\n\nUnder \u2018Target directory\u2019 specify complete URL for NFS path for\n    destination directory.\n\n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n    \nfile:///disk5/nfsmount/path/to/data/directory\n\n\nWhere,\n\n\n\n\nfile://\n indicates that it is some file system mounted on the node.\n\n\n/disk5/nfsmount/\n indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.\n\n\npath/to/data/directory\n is the directory to be ingested\n\n\n\n\n\n\nNote that, for the above example, there should be \n///\n (triple slash)\nafter \nfile:\n.\n\n\n\n\n\n\nConfiguring FTP output\n\n\n\n\n\n\nSelect FTP as output type.\n    \n\n\n\n\n\n\nAfter selecting FTP as output type then UI looks like as below:   \n\n\n\n\n\n\nSpecify the destination url below the \u201cOutput directory\u201d label.\n    Output url for FTP needs to be provided in following format,  \nftp://username:password@host:port/path\n\n\nWhere,\n- \nftp\n : \u00a0protocol name\n- \nusername\n : username for ftp server\n- \npassword\n : password\n- \nhost\n : FTP host\n- \nport\n : port number\n- \npath\n : Directory path to ingested\n\n\n\n\n\n\n\n\nConfiguring Amazon S3 output\n\n\n\n\n\n\nSelect S3 as output type.\n    \n\n\n\n\n\n\nAfter selecting S3 as output then UI looks like as below:\n    \n\n\n\n\n\n\nSpecify the destination url below the 'Output directory' label.\noutput url for S3 needs to be provided in following format,\n\n\ns3n://ukey:upass@bucketName/path\n\n    Where,\n\n\n\n\ns3n\n\u00a0: protocol name\n\n\nukey\n : access key\n\n\nupass\n :\u00a0secret access key\n\n\nbucketName\n : \u00a0bucketName\n\n\npath\n : Directory path\n\n\n\n\n\n\n\n\nConfiguring Kafka output\n\n\n\n\n\n\nSelect kafka as output type.\n    \n\n\n\n\n\n\nAfter Selecting kafka as output then UI looks like as below:\n    \n\n\n\n\n\n\nConfigure broker list and topic name.\n\n\n\n\n\n\nConfiguring JMS output\n\n\n\n\n\n\nSelect JMS as output type.\n\n\n\n\n\n\n\nAfter selecting JMS as output type then UI looks like as below:\n\n\n\n\n\n\n\nConfigure Broker URL and topic name. Broker URL would be in the form of tcp://host:port\n\n\n\n\n\n\n\nConfiguring processing steps\n\n\nConfiguring compression\n\n\nSelect compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\nSelect LZO radio button to apply LZO compression\n\n\n\n\n\n\nLzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/\n\n\n\n\nSelect GZIP radio button to apply GZIP compression\n\n\n\n\n\nConfiguring encryption\n\n\nSelect type of encryption on configuration page.\n\n\n\n\n\nApply AES encryption:\n\n\n\n\nSelect AES radio button to apply AES encryption\n\n\n\n\n\n\nProvide AES symmetric encryption key in \u201cAES key\u201d text box\n\n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.\n\n\n\n\n\n\n\nApply PKI encryption:  \n\n\n\n\nSelect PKI encryption button to apply PKI encryption  \n\n\nProvide Asymmetric public key to be used for PKI encryption", 
            "title": "dtIngest"
        }, 
        {
            "location": "/dtingest/#dtingest-tutorial", 
            "text": "\"dtIngest\" is a datatorrent application that can read data from various\nsources and store the processed data to various sinks. The data movement\nhappens at scale and in parallel. To know more about dtIngest please\nrefer the  dtIngest\nblog .  This tutorial refers to dtIngest version 1.0.0", 
            "title": "dtIngest Tutorial"
        }, 
        {
            "location": "/dtingest/#pre-requisites", 
            "text": "Datatorrent RTS should be installed on your hadoop cluster. Please\n    refer  Installation\n    guide \u00a0for\n    installation steps.    Source and destination file systems should be accessible from all\n    nodes of the cluster running DataTorrent RTS. It can be any of HDFS,\n    NFS, S3, FTP. For sandbox image or single node hadoop cluster you can also use local\n    files as source. But, for multi-node cluster local files cannot be used as source.    In case your source or destination file system is NFS; then NFS\n    should be mounted on all the nodes within the hadoop cluster at a\n    common mount point and should have read/write permission to the user\n    running dtIngest application.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/dtingest/#launching-ingestion-application", 
            "text": "dtIngest application can be configured and launched from  Datatorrent\nManagement\nConsole .    Navigate to 'Develop' tab.\n        The dtIngest application package is already uploaded and available\n    to use under 'Application Packages' section.\u00a0    Select 'Ingestion Application' from list of App\u00a0packages. And click\n    on \u2018launch application\u2019 button.\n        Configuration page for dtingest is displayed after the 'launch'.\n    Just provide desired values for the configuration and click\n    'Launch' to ingest your data.", 
            "title": "Launching ingestion application"
        }, 
        {
            "location": "/dtingest/#configuring-dtingest-instance-properties", 
            "text": "In the 'Name this application' textbox; enter suitable name of\n    your choice for this ingestion instance. For example, 'Ingestion\n    test'\n        Under 'Specify a queue' option; keep the 'Specify a queue'\n    checkbox unselected if you want to use default queue.  If you want to specify some specific queue to launch this application; then\nselect the 'Specify a queue' checkbox and select desired queue from the\ndropdown. To know more about this read  Hadoop Capacity Scheduler Docs     Under 'Use config a file' option; select the checkbox if you wish\n    to use some configuration which is already saved. In the drop down\n    select the desired configuration.\n    After selection, all the input options will load the values saved in the configuration.\n      After values are loaded you can modify the values of any option to the\ndesired value. You can also save this new combination of values as a\nconfiguration under the same name or different name.  If you want to specify all the options from scratch without using any pre-saved configuration; then unselect a checkbox 'Use a config file'    Configure input source, refer to  Configuring input\n    source \u00a0section for details.    Configure output destination, refer to  Configuring output\n    destination \u00a0section for details.    Configure processing steps, refer to  Configuring processing\n    steps \u00a0section for details.    Under 'Save Configuration file' give name for configuration; if\n    you wish to save this combination of values for future use.\n    You may keep this blank if you do not want to save this for future use.", 
            "title": "Configuring dtingest instance properties"
        }, 
        {
            "location": "/dtingest/#configuring-input-source", 
            "text": "Configuring HDFS input    For 'Input data source' field; select 'HDFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested. \n    For example, if the namenode is 'namenode1.cluster.company.org' and\n    port is '8020' and file path is ''/user/john/data' then complete URL in\n    this case will be\n     hdfs://namenode1.cluster.company.org:8020/user/john/data . \n    Where,   hdfs://  indicates HDFS protocol  namenode1.cluster.company.org  indicates fully qualified domain\n   name for the namenode of source HDFS.  8020  indicates port number for HDFS namenode service  /user/john/data  indicates full path for destination directory      If there are more than one directories/file to be ingested; click on\n'Add directory' button. Then specify complete URL for the file path to be\ningested as specified above    In the 'Filtering criteria' field; specify regular\u00a0expression for\n    files to be copied. \u00a0For regular expression syntax, please refer to\n     Java regular expression  documentation .\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.     Where,\n-    .*  indicates any character zero or more times\n-    \\.  indicates dot escaped with backslash\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only '.log' files from the source\ndirectories.    Under 'Runs' field, select 'Single run' if you expect\n    application to shutdown after ingesting files currently present in\n    the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based on\n'modification timestamp'. Entire file will be ingested again in case of\nany change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.       Configuring\u00a0NFS input    For 'Input data source' field; select 'File/NFS' option from the\n    drop-down.     Under 'Source directories'; specify complete URL for the file path\n    to be ingested.  For example, if the NFS mount is located at '/disk5/nfsmount' and\n'path/to/data/directory' is the directory under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk5/nfsmount/path/to/data/directory .\nWhere,\n-    file://  indicates that it is some file system mounted on the node.\n-    /disk5/nfsmount/  indicates the mount point. Note that, this has\nto be uniform across all the nodes in the cluster\n-    path/to/data/directory  is the directory to be ingested   Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    If there are more than one directories to be ingested; click on\n    'Add directory' button. Then specify complete URL for the file\n    path to be ingested as specified in point 3.    If there are specific files to be ingested; then specify complete\n    URL for the file path to be ingested.  For example, \u00a0if some other NFS mount is located at '/disk6/nfsmount2' and 'path/to/file/to/copy/datafile.txt' is a file under this mount which needs\nto be ingested; then complete URL in this case will be  file:///disk6/nfsmount2/path/to/file/to/copy/datafile.txt .     In the 'Filtering criteria' field; specify regular expression for\n    files to be copied.\n    For example, if only  .log  files need to be ingested; then use  .*\\.log  as regular expression.   Where,\n-    .\\*  indicates any character zero or more times\n-    \\\\.  indicates dot escaped with backslash '\\'\n-    log  indicates desired extension which is 'log'  In this case, dtingest ingests only  .log  files from the source directories.    Under 'Runs' field, select 'Single run' if you expect application\n    to shutdown after ingesting files currently present in the directory.  Select 'Polling' if you expect application to periodically poll the\ndirectory/file for the changes. Change of file is detected based\non 'modification timestamp'. Entire file will be ingested again in case\nof any change.  If 'Polling' mode is selected; then 'Polling interval' should be specified.\nThis is the time interval between sub-sequent scans for detecting\nnew/modified files.     Configuring FTP input  FTP is most widely used transfer protocol, used for transfer files from\none host to another. Use dtIngest, to copy files/directory from ftp\nsource location to some destination. This section gives details about\nhow to ingest files/directories from FTP using dtIngest. \u00a0    Select \u00a0FTP as input type\n        After selecting the FTP as input type, snapshot of UI as below:         Configure FTP input url\n    Input url for FTP needs to be provided in following format,   ftp://username:password@host:port/path \n    where,   ftp  : \u00a0protocol name  username  : \u00a0username for ftp server  password  : password  host  : FTP host  port  : port number  path  : path to either file / directory    If you want to copy multiple files/directories, click on (+) button. If\nyou are copying multiple files, then UI would be as follows:  If you are copying multiple directories, then UI would be as follows:    Configuring Amazon S3 input  S3 is clustered storage service of Amazon. Amazon s3 provides a web\nservices interface that can be used to store and retrieve any amount of\ndata, at any time, from anywhere on web. For more details about Amazon\nS3, please refer to  Amazon S3\nDocumentation .  Use dtIngest, you can copy files/directory efficiently from S3 source\nlocation to some destination. This section gives details about how to\ningest files/directories from S3 using dtIngest. \u00a0    Select \u00a0S3 as input type    After selecting the S3 as source type then UI looks like as below:    Configure S3 input url.  Input url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path  where,\n-  s3n : \u00a0protocol name\n-  ukey : access key\n-  upass : secret access key\n-  bucketName  : bucketName\n-  path  : path to either file / directory    \n  If you want to copy multiple directories, then click on (+) button and\nspecify the url\u2019s, UI would be as below:\n    Configuring Kafka input  Kafka is a distributed publish subscribe messaging system. Since kafka\nis distributed system, topics are partitioned and replicated across\nnodes. Messages in kafka are expired after some time. For more details\nabout Kafka, please refer to  Apache Kafka\nDocumentation .  We need to copy messages from multiple partitions to one destination\nbefore expiry. This section gives details about how to ingest messages\nfrom kafka using dtIngest.    Select kafka as input type\n        After selecting kafka as input type then UI looks like as below:\n        Configure topic name and zookeeper quorum.\n    zookeeper quorum \u00a0is a string in the form of\n     hostname1:port1,hostname2:port2,hostname3:port3  where,   hostname1,hostname2,hostname3  are hosts  port1,port2,port3  are ports of zookeeper server   e.g. localhost:2181,localhost:2182    Select the offset type. By default, \u201cLatest\u201d offset is enabled. If\n    you want to consume messages from beginning of kafka queue, then\n    select \u201cEarliest\u201d offset option.    If the topic name is same across the kafka clusters and want to\n    ingest data from these clusters, then configure the zookeeper quorum\n    as follows:  c1::hs1:p1,hs2:p2,hs3:p3;c2::hs4:p4,hs5:p5,c3::hs6:p6  where,\n-  c1,c2,c3  indicates the cluster names,\n-  hs1,hs2,hs3,hs4,hs5,hs6  are zookeeper host names\n-  p1,p2,p3,p4,p5,p6  are corresponding ports.  For\u00a0example, ClusterA and ClusterB are 2 kafka clusters as below, then\nzookeeper quorum would be as  ClusterA::node3.example.com:2181,node4.example.com:2181;ClusterB::node8.example.com:2181     Configuring JMS input  JMS provides the facility to create, send and read messages. You can\ncopy messages from JMS source location to some destination using\ndtIngest. This section gives details about how to ingest messages from\nJMS using dtIngest. \u00a0    Select JMS as input type.\n        After selecting the JMS as source type then UI looks like as below:\n        Configure Broker URL and topic name.\n    Broker url would be in the form of tcp://hostName:port", 
            "title": "Configuring input source"
        }, 
        {
            "location": "/dtingest/#configuring-output-destination", 
            "text": "Configuring HDFS output    For 'Output Location' field; select 'HDFS' option from the\n    drop-down.     Under 'Target directory' specify complete URL for the HDFS path\n    for the destination directory. For example, URL should be like hdfs://namenode1.cluster.company.org:8020/user/username/path/to/destination/directory     Where,\n    -  hdfs://  indicates HDFS protocol\n    -  namenode1.cluster.company.org  indicates fully qualified domain name for\n    the namenode of destination HDFS.\n    -  :8020  indicates port number for HDFS namenode service\n    -  /user/username/path/to/destination/directory  indicates full path\n    for destination directory.    Under 'Recursive copy' option, select 'Yes' if you wish to copy\n    entire directory structure under source directory to the\n    destination. Select 'No' if you want non-recursive copy.    Under 'Overwrite conflicting files' option, select 'Yes' if you\n    wish to overwrite the file at the destination if file with the same\n    name is discovered under input source.    Compact files  If you want to copy the data to the HDFS and partition the data into\npartitions of fixed size; use 'Compact files' feature to do this. This\ncan be used to combine large number of small files into partitions of\nmanageable size. Additionally, you may also use this to breakdown very\nlarge file into partitions of manageable size.     Select 'yes' for radio button under 'Compact files' option. This\n    will display other additional options for compaction. If you do not\n    want to compact files but copy them as they are; then select 'no'\n    for 'Compact files' option. If you select 'no' ; additional\n    options for compaction will be hidden.    Select delimiter to be used for separating contents of the files.\n    This will be useful if you decide to use some custom logic for\n    parsing partition files. Default value for 'delimiter' option is\n    'none'. You can use new line or any other custom delimiter based\n    on your requirement. Note that, special characters in the custom\n    delimiter should be escaped with  \\ . For example, tab character\n     \\t  should be specified as  \\\\t .    Specify the size for each partition under 'Max compacted file\n    size'. You can specify partition size in bytes, MB, GB. Data will\n    spill over to the next partition once this size is reached.    Note that, partition will be of exact sizes in case of continuous\nincoming data. If there is no incoming data for consecutive 600 windows\nthen that partition will be committed to the HDFS. In this case, new\nincoming data will be spilled to the next partition.  Configuring NFS output    For \u2018Output Location\u2019 field; select \u2018File/NFS\u2019 option from the\n    drop-down       Under \u2018Target directory\u2019 specify complete URL for NFS path for\n    destination directory. \n    For example, \u00a0if the NFS mount is located at '/disk5/nfsmount' and\n    'path/to/data/directory' is the directory under this mount which\n    needs to be ingested; then complete URL in this case will be\n     file:///disk5/nfsmount/path/to/data/directory  Where,   file://  indicates that it is some file system mounted on the node.  /disk5/nfsmount/  indicates the mount point.\nNote that, this has to be uniform across all the nodes in cluster.  path/to/data/directory  is the directory to be ingested    Note that, for the above example, there should be  ///  (triple slash)\nafter  file: .    Configuring FTP output    Select FTP as output type.\n        After selecting FTP as output type then UI looks like as below:       Specify the destination url below the \u201cOutput directory\u201d label.\n    Output url for FTP needs to be provided in following format,   ftp://username:password@host:port/path  Where,\n-  ftp  : \u00a0protocol name\n-  username  : username for ftp server\n-  password  : password\n-  host  : FTP host\n-  port  : port number\n-  path  : Directory path to ingested     Configuring Amazon S3 output    Select S3 as output type.\n        After selecting S3 as output then UI looks like as below:\n        Specify the destination url below the 'Output directory' label.\noutput url for S3 needs to be provided in following format,  s3n://ukey:upass@bucketName/path \n    Where,   s3n \u00a0: protocol name  ukey  : access key  upass  :\u00a0secret access key  bucketName  : \u00a0bucketName  path  : Directory path     Configuring Kafka output    Select kafka as output type.\n        After Selecting kafka as output then UI looks like as below:\n        Configure broker list and topic name.    Configuring JMS output    Select JMS as output type.    After selecting JMS as output type then UI looks like as below:    Configure Broker URL and topic name. Broker URL would be in the form of tcp://host:port", 
            "title": "Configuring\u00a0output destination"
        }, 
        {
            "location": "/dtingest/#configuring-processing-steps", 
            "text": "Configuring compression  Select compression type on configuration page\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0    Select LZO radio button to apply LZO compression    Lzo compression is not directly supported. To use lzo compression provide\n  plugin to ingestion app which provides lzo implementation and extends from java FilterOutputStream class. Copy plugin to \\~/.dt/plugins folder\n  (i.e. HOME_DIR/.dt/plugins) of the user who launches ingestion app.\n  We do ship default lzo plugin, and is available to download on maven repository at\nhttps://oss.sonatype.org/content/repositories/releases/com/datatorrent/dtIngest-lzo/1.0.0/   Select GZIP radio button to apply GZIP compression   Configuring encryption  Select type of encryption on configuration page.   Apply AES encryption:   Select AES radio button to apply AES encryption    Provide AES symmetric encryption key in \u201cAES key\u201d text box \n \u00a0 \u00a0 Note: AES symmetric key should be of size 128, 192 or 256 bits.    Apply PKI encryption:     Select PKI encryption button to apply PKI encryption    Provide Asymmetric public key to be used for PKI encryption", 
            "title": "Configuring processing steps"
        }, 
        {
            "location": "/rts/", 
            "text": "DataTorrent RTS Overview\n\n\nDataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.  \n\n\n\n\nDataTorrent RTS platform enables creation and management of real-time big data applications in a way that is\n\n\n\n\nhighly scalable and performant\n - millions of events per second per node with linear scalability\n\n\nfault tolerant\n - automatic recovery with no data or state loss\n\n\nHadoop native\n - installs in seconds and works with all existing Hadoop distributions\n\n\neasily developed\n - write and re-use generic Java code\n\n\neasily integrated\n - customizable connectors to file, database, and messaging systems\n\n\neasily operable\n - full suite of management, monitoring, development, and visualization tools\n\n\n\n\nThe system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using \ndtAssemble\n, a graphical application assembly tool.  Application data can be easily visualized with \ndtDashboard\n real-time data visualizations.", 
            "title": "RTS"
        }, 
        {
            "location": "/rts/#datatorrent-rts-overview", 
            "text": "DataTorrent RTS is an enterprise product built around Apache Apex, a Hadoop-native unified stream and batch processing platform.  DataTorrent RTS combines Apache Apex engine with a set of enterprise-grade management, monitoring, development, and visualization tools.     DataTorrent RTS platform enables creation and management of real-time big data applications in a way that is   highly scalable and performant  - millions of events per second per node with linear scalability  fault tolerant  - automatic recovery with no data or state loss  Hadoop native  - installs in seconds and works with all existing Hadoop distributions  easily developed  - write and re-use generic Java code  easily integrated  - customizable connectors to file, database, and messaging systems  easily operable  - full suite of management, monitoring, development, and visualization tools   The system is capable of processing billions of events per second, while automatically recovering without any state or data loss when individual nodes fail.  A simple API enables developers to write new and re-use existing generic Java code, lowering the expertise needed to write big data applications.  A library of existing demos and re-usable operators allows applications to be developed quickly.  Native Hadoop support allows DataTorrent RTS to be installed in seconds on any existing Hadoop cluster.  Application adiminstration can be done from a browser with dtManage, a full suite of management, monitoring, and visualization tools.  New applications can be visually built from existing components using  dtAssemble , a graphical application assembly tool.  Application data can be easily visualized with  dtDashboard  real-time data visualizations.", 
            "title": "DataTorrent RTS Overview"
        }, 
        {
            "location": "/dtmanage/", 
            "text": "dtManage Guide\n\n\nIntroduction\n\n\nThe DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and interact with the DataTorrent platform running on your Hadoop cluster. It is a web-based dashboard that is served by the DataTorrent Gateway, and has five areas: configuration, monitoring, development, and , visualization, and learning.\n\n\nTo download the platform or the VM sandbox, go to \nhttp://www.datatorrent.com/download\n.\n\n\n\n\nConnection Requirements\n\n\nWhen you install DataTorrent RTS on your Hadoop cluster using the installer binary, the DataTorrent Gateway service is started on the node where the installer is executed. By default, the Gateway serves the console from port 9090. The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.\n\n\n\n\nBrowser Requirements\n\n\nThe Console currently supports Chrome, Firefox, Safari, and IE (version 10 and up).\n\n\nInstallation Wizard\n\n\nThe first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:\n\n\n\n\nLocation of the hadoop executable\n\n\nDFS location where all the DataTorrent files are stored\n\n\nDataTorrent license\n\n\nSummary and review of any remaining configuration items\n\n\n\n\n\n\nWhen Kerberos Security is Enabled\n\n\nWhen your hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard: \n\n\n\n\nKerberos Principal\n: The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.\n\n\nKerberos Keytab\n: The location (path) of the Kerberos keytab file to use on the gateway node's local file system.\n\n\nYARN delegation token lifetime\n: If the value of the \nyarn.resourcemanager.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\nNamenode delegation token lifetime\n: If the value of the \ndfs.namenode.delegation.token.max-lifetime\n property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.\n\n\n\n\n\n\nNote:\n The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.\n\n\n\n\nConfigure Tab\n\n\nThe configuration page can be found by clicking the \u201cConfigure\u201d link in the main navigation bar at the top. There are links to various tools to help you configure and troubleshoot your DataTorrent installation. \nThe configuration page links may differ depending on your cluster setup. The following is a screenshot with a cluster that has simple authentication/authorization enabled.\n\n\n\n\nSystem Configuration\n\n\nThis page shows diagnostic information regarding the gateway and console, as well as any issues that the gateway may detect.\n\n\n\n\nIn addition, you can perform the following actions from this page:\n\n\nRestart the Gateway\n\n\n\nThis can be useful when Hadoop configuration has changed or some other factor of your cluster environment has changed.\n\n\nToggle Reporting\n\n\n\n\nIf enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.\n\n\nLicense Information\n\n\nUse the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.\n\n\n\n\nUser Profile\n\n\nThe User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:\n\n\n\n\nchange password \n\n\nchange the default home page\n\n\nchange the theme of the console\n\n\nrestore the default options of the console\n\n\n\n\n\n\nUser Management\n\n\nUse this page to manage users and roles of your DataTorrent cluster:\n\n\n\n\nadd users\n\n\nchange users\u2019 roles\n\n\nchange users\u2019 password\n\n\ndelete users\n\n\nadd roles\n\n\nedit role permissions\n\n\ndelete roles\n\n\n\n\n\n\n\n\nNote:\n With most authentication schemes, the admin role cannot be deleted.\n\n\n\n\nInstallation Wizard\n\n\nAt any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.\n\n\nDevelop Tab\n\n\nThe development area of dtManage is mainly geared towards the creation, upload, configuration, and launch of DataTorrent applications. The development home can be viewed by clicking the \u201cDevelop\u201d tab in the main navigation bar on the top of the screen. A prerequisite to using the development tools of the UI is an understanding of what Apex Application Packages are. For more information, see the \nApplication Packages Guide\n.\n\n\n\n\nApplication Packages\n\n\nTo access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operators directly on application packages:\n\n\n\n\nDownload the app package\n\n\nDelete the app package\n\n\nCreate a new application in an application package via dtAssemble (requires enterprise license)\n\n\nLaunch applications in the app package\n\n\nImport default packages (see below)\n\n\n\n\n\n\n\n\nNote:\n If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.\n\n\n\n\nImporting Default Packages\n\n\nWhen you install the DataTorrent platform, a folder located in the installation directory called \ndemos/app-packages\n will contain various default app packages that can be imported into HDFS for use. Just above the list of Application Packages, there should be a button that says \nImport default packages\n. Clicking this will take you to a page that shows the list of these demo app packages. Select one or more to import and click the \nImport\n button. This will upload the selected app package to HDFS.\n\n\n\n\nApplication Package Page\n\n\nOnce you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.\n\n\n\n\nAside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package. \n\n\nLaunching Apps\n\n\nTo launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options: \n\n\n\n\nSpecify a name for the running app\n\n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Just be weary that it must be unique compared to the other applications running on your DataTorrent installation.\n\n\nSpecify the \nscheduler queue\n\n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be \nroot.[USER_NAME]\n.\n\n\nUse a config file when launching\n\n  App Package config files are xml files that contain \nproperties\n that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.\n\n\nSpecify custom properties\n\n  In addition to choosing a config file, you may also specify properties directly in the launch popup by selecting this option. Note that there are three helpful functions when specifying custom properties:\n\n\nadd required properties\n - App Packages can have required properties (for example, twitter API access keys for the twitter demo). This function adds a new property with the required property name to the form, making it easy to fill in.\n\n\nadd default properties\n - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults\n\n\nsave this configuration as\u2026\n - This function creates a new config file and saves it to the App Package, which can then be used later to launch the app with. That way, you can relaunch the app with the same properties without having to re-enter them.\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n For more information about config files and custom properties, see the \nApplication Packages Guide\n\n\n\n\nViewing an Application DAG\n\n\nAll DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications.\n\n\n\n\nCreating apps with dtAssemble\n\n\nIf you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the \ndtAssemble guide\n.\n\n\nMonitor Tab\n\n\nThe main operations dashboard can be visited by clicking on the \u201cMonitor\u201d link in the main top navigation bar. This section of the Console can be used to monitor, debug, and kill running DataTorrent applications.\n\n\nOperations Home\n\n\nThe operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.\n\n\n\n\nThe cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of: \nretrieve ended apps\n and \ninclude system apps\n. The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.\n\n\nInstance Page\n\n\nTo get to an application instance page, click on either the app name or the app id in the list of running applications.\n\n\n\n\nAll sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:\n\n\n\n\nThere are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations (\nlogical\n, \nphysical\n, \nphysical-dag-view\n, \nmetric-view\n) will suffice. The following is a list of widgets available on an app instance page:\n\n\nApplication Overview Widget\n\n\nAll the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:\n\n\n\n\nThe \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.\n\n\nYou can also use the \nset logging level\n button on this widget to specify what logging level gets written to the dt.log files. \n\n\n\n\nYou will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:\n\n\n\n\nStram Events Widget\n\n\nEach application has a stream of notable events that can be viewed with the StrAM Events widget:\n\n\n\n\nSome events have additional information attached to it, which can be viewed by clicking the \u201cdetails\u201d button in the list:\n\n\n\n\nLogical DAG Widget\n\n\nThis widget visualizes the logical plan of the application being viewed:\n\n\n\n\nAdditionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown. \n\n\n\n\nPro tip:\n Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.\n\n\n\n\nPhysical DAG Widget\n\n\nThis is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.\n\n\n\n\nSame-colored physical operators in this widget indicates that these operators are in the same container.\n\n\nLogical Operators List Widget\n\n\nThis widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing. \n\n\nOne nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:\n\n\n\n\nPhysical Operators List Widget\n\n\nShows the physical operators in the application.\n\n\nContainers List Widget\n\n\nShows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.\n\n\nLogical Streams List Widget\n\n\nShows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.\n\n\nMetrics Chart\n\n\nShows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.\n\n\nRecording and Viewing Sample Tuples\n\n\nThere is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):\n\n\n\n\nViewing Logs\n\n\nAnother useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:\n\n\n\n\nOnce you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:\n\n\n\n\nThere are numerous improvements in store for dtManage, and user feedback is highly valued in the planning, so please provide any that will help in your usage of the tool!\n\n\n~The DataTorrent UI Team", 
            "title": "dtManage"
        }, 
        {
            "location": "/dtmanage/#dtmanage-guide", 
            "text": "", 
            "title": "dtManage Guide"
        }, 
        {
            "location": "/dtmanage/#introduction", 
            "text": "The DataTorrent Console (aka dtManage) is a web-based user interface that allows you to monitor and interact with the DataTorrent platform running on your Hadoop cluster. It is a web-based dashboard that is served by the DataTorrent Gateway, and has five areas: configuration, monitoring, development, and , visualization, and learning.  To download the platform or the VM sandbox, go to  http://www.datatorrent.com/download .   Connection Requirements  When you install DataTorrent RTS on your Hadoop cluster using the installer binary, the DataTorrent Gateway service is started on the node where the installer is executed. By default, the Gateway serves the console from port 9090. The ability to connect to this node and port depends on your environment, and may require special setup such as an ssh tunnel, firewall configuration changes, VPN access, etc.   Browser Requirements  The Console currently supports Chrome, Firefox, Safari, and IE (version 10 and up).  Installation Wizard  The first time you open the Console, after installing DataTorrent RTS on your cluster, it will take you to the Installation Wizard. This walks you through the initial configuration of your DataTorrent installation, by confirming the following:   Location of the hadoop executable  DFS location where all the DataTorrent files are stored  DataTorrent license  Summary and review of any remaining configuration items    When Kerberos Security is Enabled  When your hadoop cluster has security enabled with Kerberos, there will be four additional controls in the installation wizard:    Kerberos Principal : The Kerberos principal (e.g. primary/instance@REALM) to use on behalf of the management console.  Kerberos Keytab : The location (path) of the Kerberos keytab file to use on the gateway node's local file system.  YARN delegation token lifetime : If the value of the  yarn.resourcemanager.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.  Namenode delegation token lifetime : If the value of the  dfs.namenode.delegation.token.max-lifetime  property in your cluster configuration has been changed from the default, enter it here. Otherwise, leave this blank and the default will be assumed.    Note:  The token lifetime values you enter will not actually set these values in your hadoop configuration, it is only meant to inform the DataTorrent platform of these values.", 
            "title": "Introduction"
        }, 
        {
            "location": "/dtmanage/#configure-tab", 
            "text": "The configuration page can be found by clicking the \u201cConfigure\u201d link in the main navigation bar at the top. There are links to various tools to help you configure and troubleshoot your DataTorrent installation.  The configuration page links may differ depending on your cluster setup. The following is a screenshot with a cluster that has simple authentication/authorization enabled.   System Configuration  This page shows diagnostic information regarding the gateway and console, as well as any issues that the gateway may detect.   In addition, you can perform the following actions from this page:  Restart the Gateway  \nThis can be useful when Hadoop configuration has changed or some other factor of your cluster environment has changed.  Toggle Reporting   If enabled, your DataTorrent installation will send various pieces of information such as bug reporting and usage statistics back to our servers.  License Information  Use the License Information page to view how much of your DataTorrent license capacity your cluster is consuming as well as what capabilities your license permits. You can also upload new license files here.   User Profile  The User Profile page displays information about the current user, including their username, the authentication scheme being used, and the roles that the current user has. In addition, users can perform the following actions:   change password   change the default home page  change the theme of the console  restore the default options of the console    User Management  Use this page to manage users and roles of your DataTorrent cluster:   add users  change users\u2019 roles  change users\u2019 password  delete users  add roles  edit role permissions  delete roles     Note:  With most authentication schemes, the admin role cannot be deleted.   Installation Wizard  At any time, you can go back to the installation wizard from the Configuration Tab. It can help diagnose issues and reconfigure your cluster and gateway.", 
            "title": "Configure Tab"
        }, 
        {
            "location": "/dtmanage/#develop-tab", 
            "text": "The development area of dtManage is mainly geared towards the creation, upload, configuration, and launch of DataTorrent applications. The development home can be viewed by clicking the \u201cDevelop\u201d tab in the main navigation bar on the top of the screen. A prerequisite to using the development tools of the UI is an understanding of what Apex Application Packages are. For more information, see the  Application Packages Guide .   Application Packages  To access the application package listing, click on the \"Apps\" link from the Develop Tab index page. From here, you can perform several operators directly on application packages:   Download the app package  Delete the app package  Create a new application in an application package via dtAssemble (requires enterprise license)  Launch applications in the app package  Import default packages (see below)     Note:  If authentication is enabled, you may not be able to see others\u2019 app packages, depending on your permissions.   Importing Default Packages  When you install the DataTorrent platform, a folder located in the installation directory called  demos/app-packages  will contain various default app packages that can be imported into HDFS for use. Just above the list of Application Packages, there should be a button that says  Import default packages . Clicking this will take you to a page that shows the list of these demo app packages. Select one or more to import and click the  Import  button. This will upload the selected app package to HDFS.   Application Package Page  Once you have uploaded or imported an App Package, clicking on the package name in the list will take you to the Application Package Page, where you can view all the package details.   Aside from various pieces of meta information (owner, DataTorrent version, required properties, etc), you will see a list of apps found in this package.   Launching Apps  To launch an app in an App Package, click on the launch button to the far right of the list. A dialog box will appear with several options:    Specify a name for the running app \n  The console will pre-populate this field with an appropriate name, but you can specify your own name. Just be weary that it must be unique compared to the other applications running on your DataTorrent installation.  Specify the  scheduler queue \n  This input allows you to specify which queue you want the application to be launched under. The default behavior depends on your Hadoop installation, but typically will be  root.[USER_NAME] .  Use a config file when launching \n  App Package config files are xml files that contain  properties  that get interpreted and used for launching an application. To choose one, enable the check box and choose the config file you want to use for launch.  Specify custom properties \n  In addition to choosing a config file, you may also specify properties directly in the launch popup by selecting this option. Note that there are three helpful functions when specifying custom properties:  add required properties  - App Packages can have required properties (for example, twitter API access keys for the twitter demo). This function adds a new property with the required property name to the form, making it easy to fill in.  add default properties  - App Packages can also have default properties. This function will add the default properties to the list, making it easy for you to override the defaults  save this configuration as\u2026  - This function creates a new config file and saves it to the App Package, which can then be used later to launch the app with. That way, you can relaunch the app with the same properties without having to re-enter them.       Note:  For more information about config files and custom properties, see the  Application Packages Guide   Viewing an Application DAG  All DataTorrent applications are made up of operators that connect together via streams to form a Directed Acyclic Graph (DAG). To see a visualization of this DAG, click on the application name in the list of applications.   Creating apps with dtAssemble  If you have an Enterprise license, you will have access to the dtAssemble tool. Using this tool is outside the scope of this guide, but check out the  dtAssemble guide .", 
            "title": "Develop Tab"
        }, 
        {
            "location": "/dtmanage/#monitor-tab", 
            "text": "The main operations dashboard can be visited by clicking on the \u201cMonitor\u201d link in the main top navigation bar. This section of the Console can be used to monitor, debug, and kill running DataTorrent applications.  Operations Home  The operations home page shows overall cluster statistics as well as a list of running DataTorrent applications.   The cluster statistics include some performance statistics and memory usage information. As for the application list, there are two options to take note of:  retrieve ended apps  and  include system apps . The first option will include all ended applications that are still in the resource manager history. The second option will include system apps, which are apps like the App Data Tracker that are developed by DataTorrent and used to add functionality to your DataTorrent cluster.  Instance Page  To get to an application instance page, click on either the app name or the app id in the list of running applications.   All sections and subsections of the instance page currently use a dashboard/widget system. The controls for this system are located near the top of the screen, below the breadcrumbs:   There are tool tips to help you understand how to work with dashboards and widgets. For most users, the default dashboard configurations ( logical ,  physical ,  physical-dag-view ,  metric-view ) will suffice. The following is a list of widgets available on an app instance page:  Application Overview Widget  All the default dashboard tabs have this widget. It contains basic information regarding the app plus a few controls. To end a running application, use either the \u201cshutdown\u201d or \u201ckill\u201d buttons in this widget:   The \u201cshutdown\u201d function tries to gracefully stop the application, while \u201ckill\u201d forces the application to end. In either case, you will need to confirm your action.  You can also use the  set logging level  button on this widget to specify what logging level gets written to the dt.log files.    You will then be presented with a dialog where you can specify either fully-qualified class names or package identifiers with wildcards:   Stram Events Widget  Each application has a stream of notable events that can be viewed with the StrAM Events widget:   Some events have additional information attached to it, which can be viewed by clicking the \u201cdetails\u201d button in the list:   Logical DAG Widget  This widget visualizes the logical plan of the application being viewed:   Additionally, you can cycle through various metrics aggregated by logical operator. In the screenshot above, processed tuples per second and emitted tuples per second are shown.    Pro tip:  Hold the alt/option key while using your mouse scroll wheel to zoom in and out on the DAG.   Physical DAG Widget  This is similar to the Logical DAG Widget, except it shows the fully deployed \"physical\" operators. Depending on the partitioning of your application, this could be significantly more complex than the Logical DAG view.   Same-colored physical operators in this widget indicates that these operators are in the same container.  Logical Operators List Widget  This widget shows a list of logical operators in the application. This table, like others, has live updates, filtering, column ordering, stacked row sorting, and column resizing.   One nice feature specific to this widget is the ability to set the logging level for the Java class of a logical operator by selecting it in this list and using the provided dropdown, like so:   Physical Operators List Widget  Shows the physical operators in the application.  Containers List Widget  Shows the containers in the application. From this widget you can: select a container and go to one of its logs, fetch non-running containers and view information about them, and even kill selected containers.  Logical Streams List Widget  Shows a list of the streams in the application. There are also links to the logical operator pages for the sources and sinks of each stream.  Metrics Chart  Shows various metrics of your application on a real-time line chart. Single-click a metric to toggle its visibility. Double-click a metric to toggle all other keys' visibility.  Recording and Viewing Sample Tuples  There is a mechanism called tuple recording that can be used to easily look at the content of tuples flowing through your application. To use this feature, select a physical operator from the Physical Operators List widget and click on the \u201crecord a sample\u201d button. This will bring up a modal window which you can then use to traverse the sample and look at the actual content of the tuple (converted to a JSON structure):   Viewing Logs  Another useful feature of the Console is the ability to view container logs of a given application. To do this, select a container from the Containers List widget (default location of this widget is in the \u201cphysical\u201d dashboard). Then click the logs dropdown and select the log you want to look at:   Once you are viewing a log file in the console, there are few tricks to traversing it. You can scroll to the top to fetch earlier content, scroll to the bottom for later content, grep for strings in the selected range or over the entire log, and click the \u201ceye\u201d icon to the far left of every line to go to that location of the log:   There are numerous improvements in store for dtManage, and user feedback is highly valued in the planning, so please provide any that will help in your usage of the tool!  ~The DataTorrent UI Team", 
            "title": "Monitor Tab"
        }, 
        {
            "location": "/dtassemble/", 
            "text": "dtAssemble - Graphical Application Builder\n\n\nThe dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application. \n\n\n\n\nAccessing the Builder\n\n\nTo get to the App Builder, you must perform the following steps (illustrated by GIF):\n\n\n\n\n\n\nCreate a DataTorrent Application Package\n  To do this, read through the \nApplication Packages Guide\n, which provides step-by-step instructions on how to do this.\n\n\nUpload it to HDFS using the Console\n  Click on the \nDevelop\n link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.\n\n\nAdd a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.\n\n\nDrag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d\n\n\nConfigure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.\n\n\nConnect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.\n\n\n\n\nUsing Application Builder\n\n\nThe Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:\n\n\n\n\nOperator Library Navigator\n\n\n\n\nUse this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder\nsection). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.\n\n\nOnce you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.\n\n\n\n\nCanvas\n\n\n\n\nThe Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder\nsection.\n\n\n\n\nInspector\n\n\nThe Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.\n\n\nOperator Inspector\n\n\n\n\nWhen an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties. \n\n\nPort Inspector\n\n\nWhen a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port). \n\n\nStream Inspector\n\n\n\n\nThe stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.\n\n\nPrepping Operators for the Application Builder\n\n\nThe way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:\n\n\n\n\nOperators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.\n\n\n\n\nUse javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.\n\n\n/**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }\n\n\n\n\n\n\n\nEvery property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended\n\n\n/**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\n\n\nUtilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.  \n\n\n\n\nWhen a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.\n\n\nMark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization.\n/**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }\n\n\n\n\n\nMake the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.\n\n\n\n\n\n\n\n\nApp Builder Usage Examples\n\n\nPi Demo\n\n\nAs an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application. \n\n\n\n\n\n\nFirst you will need to clone the malhar repository:\n\n\ngit@github.com:DataTorrent/Malhar.git \n cd Malhar\n\n\n\n\n\n\n\nThen, run maven install, which will create the App Package jar need to upload to your gateway.\n\n\n\n\nFollow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.\n\n\nAdd the Console Output\noperators.\n\n\nConnect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.\n\n\nClick the \u201claunch\u201d button in the top left once it turns purple.\n\n\nIn the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.\n\n\nTo see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "dtAssemble"
        }, 
        {
            "location": "/dtassemble/#dtassemble-graphical-application-builder", 
            "text": "The dtAssemble Graphical Application  Builder is a UI tool that allows users to drag-and-drop operators onto a canvas and connect them together to build a DataTorrent application.", 
            "title": "dtAssemble - Graphical Application Builder"
        }, 
        {
            "location": "/dtassemble/#accessing-the-builder", 
            "text": "To get to the App Builder, you must perform the following steps (illustrated by GIF):    Create a DataTorrent Application Package\n  To do this, read through the  Application Packages Guide , which provides step-by-step instructions on how to do this.  Upload it to HDFS using the Console\n  Click on the  Develop  link at the top of the DataTorrent Console, then click the \u201cupload a package\u201d button.  Add a new application to your uploaded package\n  Once your package has been uploaded, click on its name in the list of packages. Then click the \u201cadd new application\u201d button.  Drag operators onto the canvas\n  Use the search field of the Operator Library panel on the left to find the operators that were found in your app package, then click and drag them out onto the \u201cApplication Canvas.\u201d  Configure the operators using the Operator Inspector\n  When the operator is selected, the right side of the screen will have the Operator Inspector, where you will see some meta information about the operator as well as the interface to set initial values for operator properties and attributes.  Connect the operators\u2019 ports to form streams between operators\n  Select a port by clicking on it. Ports that are compatible with the selected port will pulse green. Click and drag out a stream from one port and connect it with a compatible port of another operator.", 
            "title": "Accessing the Builder"
        }, 
        {
            "location": "/dtassemble/#using-application-builder", 
            "text": "The Application Builder contains three main parts: the Operator Library Navigator, the Canvas, and the Inspector:   Operator Library Navigator   Use this to quickly find and add operators that exist in the Application Package Jar. It groups operators by their Prepping Operators for the Application Builder section). You can search for operators using the input field at the top, which will look at the operators\u2019 titles, descriptions, and keywords. Clicking on an operator will expand a window with more information regarding the operator.  Once you have found an operator you would like to add to your application canvas, simply click and drag it onto the canvas.   Canvas   The Application Canvas is the main area that you use to assemble applications with operators and streams. Specifically, you will be connecting output ports (shown as magenta) of some operators to input ports (shown as blue) of other operators. When you click on a port, other ports that are compatible with it will pulse green, indicating that a stream can connect the two. See the note on tuple types of ports in the Prepping Operators for the Application Builder section.   Inspector  The Inspector is visible when an operator, a port, or a stream is selected on the canvas. It will look different depending on what is selected.  Operator Inspector   When an operator is selected on the canvas, you will see the Operator Inspector on the right side. You will see the operator class name, the java package it is a part of, and a field with the name you have given to it. You will also be able to edit the initial values of the operator\u2019s properties.   Port Inspector  When a port is selected, you will see the Port Inspector on the right side. Here you can see the name of the port, the tuple type that it emits (for an output port) or accepts (for an input port).   Stream Inspector   The stream inspector will appear when you have selected a stream in the canvas. You can use this to rename the stream or change the locality of the stream.", 
            "title": "Using Application Builder"
        }, 
        {
            "location": "/dtassemble/#prepping-operators-for-the-application-builder", 
            "text": "The way in which an operator shows up in the App Builder depends on how the operator is written in Java. In order to fully prep an operator so that it can be easily used in the App Builder, use the following guidelines:   Operators must be a concrete class and must have a no-arg constructor\n  This is actually a requirement that extends beyond app builder, but is noted here because operators that are abstract or that do not have a no-arg constructor will not appear in the Operator Library panel in the current version.   Use javadoc annotations in the comment block above the class declaration statement:\n  a.  @omitFromUI - Put this annotation if you do not want the operator to show up in the App Builder\n  b.  @category - The high-level category that this operator should reside in. The value you put here is arbitrary; it will be placed in (or create a new) dropdown in the Operator Library Navigator on the left of the App Builder. You can have multiple categories per operator.\n  c.  @tags - Space-separated list of \u201ctags\u201d; arbitrary strings of text that will be searchable via the Operator Library Navigator on the left of the App Builder. Tags work as filters. You can use them to enable search, project identification, etc.\n  d.  @required - This is a future annotation to denote whether a property is required.  /**\n * This is an example description of the operator It will be \n * displayed in the app builder under the operator in the Operator \n * Library Navigator.\n * @category Algo\n * @tags math sigma average avg\n */\npublic class MyOperator extends BaseOperator { /* \u2026 */ }    Every property's getter method should be preceded by a descriptive comment block that indicates what a property does, how to use it, common values, etc. This is not a must have, but very highly recommended  /**\n* This is an example description of a property on an operator class.\n* It will be displayed in the app builder under the property name.\n* It is ok to make this long because the UI will only show the first\n* sentence or so and allow the user to expand/collapse the rest.\n*/\npublic String getMyProperty() { /* \u2026 */ }    Utilize the @useSchema doclet annotation above properties\u2019 getter in order to mark a property\u2019s or subproperty\u2019s.     When a property's type is not a primitive, wrapper class for a primitive, or a String, try to be as specific as possible with the type signature.\n  For example, mark a property type as java.util.HashMap instead of java.util.Map, or, more generally, choose ConcreteSubClass over AbstractParentClassOrInterface. This will limit the assignable concrete types that the user of the app builder must choose from. For now, we only support property types that are public and either have a no-arg constructor themselves or their parent class does.  Mark properties that should be hidden in the app builder with the @omitFromUI javadoc annotation in the javadoc comment block above the getter of the property. This is a critical part of what an operator developer decides to expose for customization. /**\n * This is an example description of a property on an operator class\n * WS\n */\npublic String getMyProperty() { /* \u2026 */ }   Make the tuple type of output ports strict and that of input ports liberal.\n  The tuple type that an output port emits must be assignable to the tuple type of an input port in order for them to connect. In other words, the input port must either be the exact type or a parent type of the output port tuple type. Because of this, building operators with more specific output and less specific input will make them more reusable.", 
            "title": "Prepping Operators for the Application Builder"
        }, 
        {
            "location": "/dtassemble/#app-builder-usage-examples", 
            "text": "Pi Demo  As an example, we will rebuild the basic Pi demo. Please note that this example is relatively contrived and is only meant to illustrate how to connect compatible ports of operators to create a new application.     First you will need to clone the malhar repository:  git@github.com:DataTorrent/Malhar.git   cd Malhar    Then, run maven install, which will create the App Package jar need to upload to your gateway.   Follow steps 2,3, and 4 of the Accessing the App Builder section above, but with the app package jar located in Malhar/demos/pi/target/pi-demo-{VERSION}.apa.  Add the Console Output operators.  Connect the integer_data port of the Random Event Generator to the input port of the Pi Calculate operator, and connect the output port of the Pi Calculate operator to the input port of the Console Output operator.  Click the \u201claunch\u201d button in the top left once it turns purple.  In the resulting modal, click \u201cLaunch\u201d, then the \u201cView it on the Dashboard\u201d link in the subsequent notification box.  To see the output of the Console Output operator, navigate to the stdout log file viewer of the container where the operator is running.", 
            "title": "App Builder Usage Examples"
        }, 
        {
            "location": "/dtdashboard/", 
            "text": "dtDashboard - Application Data Visualization\n\n\nThe App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.\n\n\nExamples\n\n\nTwitter Example\n\n\nThe Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:\n\n\n\n\nAds Dimension Example\n\n\nThe Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.\n\n\n\n\nData Sources\n\n\nA Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.\n\n\n\n\nTo see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:\n\n\n\n\nThe operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.\n\n\nAnd below is the DAG for the Ads Dimension Demo:\n\n\n\n\nIn this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.\n\n\nAll these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.  \nApp Data Framework Programming Guide\n\n\nStats and Custom Metrics\n\n\nEach application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.\n\n\nData Visualization with Dashboards and Widgets\n\n\nOverview\n\n\nDataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.\n\n\n\n\nDashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.\n\n\nAccessing Dashboards\n\n\nDashboards are accessible from Visualize section in the DataTorrent Console menu.\n\n\n\n\nAfter selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.\n\n\n\n\nAn alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.\n\n\n\n\nBelow is an example of accessing the data visualization dashboard from a running application.\n\n\n\n\nCreating Dashboards\n\n\nThere are two ways to create a new visualization dashboard\n\n\n\n\ncreate new button on the Dashboards screen\n\n\ngenerate new dashboard option in the visualization menu of a compatible running DataTorrent application\n\n\n\n\nBelow is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)\n\n\n\n\n\n\nProvide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.\n\n\n\n\n\n\nInclude optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.\n\n\n\n\n\n\nSelect compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.\n\n\n\n\n\n\nChoose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nBelow is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.\n\n\n\n\n\n\n\n\nLocate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.\n\n\n\n\n\n\nChoose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.\n\n\n\n\n\n\nCustomize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.\n\n\n\n\n\n\nModifying Dashboards\n\n\nDashboards controls are presented as a row of buttons just below the dashboard title and description.\n\n\n\n\nNew widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.\n\n\nWidgets Overview\n\n\nDashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.\n\n\nAdding Widgets\n\n\nWidgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget \n\n\nAlternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.\n\n\n\n\nWhether using auto generate buttons, results are not persisted until save changes is applied.\n\n\nEach data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.\n\n\nEditing Widgets\n\n\nEach widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include\n\n\n\n\nlabel field selection\n\n\nquantity field selection\n\n\nsort order selection\n\n\n\n\nBelow is an example of changing label field and sort order for a bar chart widget.\n\n\n\n\nFor dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include\n\n\n\n\nTime ranges selection\n\n\nlive streaming\n\n\nhistorical range\n\n\nDimensions Selections\n\n\nkey combinations and key values selection\n\n\naggregate selection\n\n\n\n\n\n\nFor Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.\n\n\n\n\nAfter making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "dtDashboard"
        }, 
        {
            "location": "/dtdashboard/#dtdashboard-application-data-visualization", 
            "text": "The App Data Framework collection of UI tools and operator APIs that allows DataTorrent developers to visualize the data flowing through their applications.  This guide assumes the reader\u2019s basic knowledge on the DataTorrent RTS platform and the Console, and the concept of operators in streaming applications.", 
            "title": "dtDashboard - Application Data Visualization"
        }, 
        {
            "location": "/dtdashboard/#examples", 
            "text": "Twitter Example  The Twitter Hashtag Count Demo, shipped with DataTorrent RTS distribution, is a streaming application that utilizes the App Data Framework.  To demonstrate how the Application Data Framework works on a very high level, on the Application page in the Console, click on the visualize button next to the application name, and a dashboard for the Twitter Hashtag Count Demo will be created.  In it, you will see visualization of the top 10 hashtags computed in the application:   Ads Dimension Example  The Ads Dimension Demo included in the DataTorrent RTS distribution also utilizes the App Data Framework.  The widgets for this application demonstrates more features than the Twitter one because you can issue your own queries to choose what data you want to visualize.  For example, one might want to visualize the running revenue and cost for advertiser \u201cStarbucks\u201d and publisher \u201cGoogle\u201d.", 
            "title": "Examples"
        }, 
        {
            "location": "/dtdashboard/#data-sources", 
            "text": "A Data Source in the application consists of three operators.  The Query Operator, the Data Source Operator and the Result Operator.  The Query Operator takes in queries from a message queue and passes them to the Data Source Operator.  The Data Source Operator processes the queries and sends the results to the Result Operator.  The Result Operator delivers the results to the message queue.  The Data Source Operator generally takes in data from other parts of the DAG.   To see how this is fit in our previous examples, below is the DAG for the Twitter Hashtag Demo:   The operators \u201cQuery\u201d, \u201cTabular Server\u201d and \u201cQueryResult\u201d are the three operators that serve the data being visualized in the Console.  The \u201cTabular Server\u201d operator takes in data from the TopCounter operator, processes incoming queries, and generates results.  And below is the DAG for the Ads Dimension Demo:   In this DAG, the operators \u201cQuery\u201d, \u201cStore\u201d and \u201cQueryResult\u201d are the three operators that make up the Data Source.  The \u201cStore\u201d operator takes in incoming data, stores them in a persistent storage, and serve them.  In other words, the Data Source serves historical data as well as current data, as opposed to the Twitter Hashtag Demo, which only serves the current data.  All these operators are available in the Malhar (and Megh). When you are familiar with how the built-in operators work, you may want to create your own Data Sources in your own application.   App Data Framework Programming Guide", 
            "title": "Data Sources"
        }, 
        {
            "location": "/dtdashboard/#stats-and-custom-metrics", 
            "text": "Each application has statistics such as tuples processed per second, latency, and memory used.  Each operator in an application can contain custom metrics that are part of the application logic.  With the Application Data Framework, each application comes with Data Sources that give out historical and real-time application statistics data and custom metrics data.  You can visualize such data as you would for other Data Sources in the application.", 
            "title": "Stats and Custom Metrics"
        }, 
        {
            "location": "/dtdashboard/#data-visualization-with-dashboards-and-widgets", 
            "text": "", 
            "title": "Data Visualization with Dashboards and Widgets"
        }, 
        {
            "location": "/dtdashboard/#overview", 
            "text": "DataTorrent Dashboards and Widgets are UI tools that allow users to quickly and easily visualize historical and real-time application data.  Below is an example of a visualization dashboard with Stacked Area Chart, Pie Chart, Multi Bar Chart, and Table widgets.   Dashboards are quick and easy to create, and can include data from a single or multiple applications on the same screen.  Widgets can be added, removed, rearranged, and resized at any time.  Each widget provides a unique way to visualizes the application data, and provides a number of ways to configure the content and the corresponding visualizations.", 
            "title": "Overview"
        }, 
        {
            "location": "/dtdashboard/#accessing-dashboards", 
            "text": "Dashboards are accessible from Visualize section in the DataTorrent Console menu.   After selecting Visualize menu item, a list of available dashboards is displayed.  The list of available dashboards can be ordered or filtered by dashboard name, description, included applications, creating user, and modified timestamp.  Clicking one of the dashboard name links takes you to the selected dashboard.   An alternative way to access dashboards is from Monitor section.  Navigate to one of the running applications, and if the application supports data visualization, list of existing dashboards which include selected application will be displayed after clicking on visualize button below the application name.   Below is an example of accessing the data visualization dashboard from a running application.", 
            "title": "Accessing Dashboards"
        }, 
        {
            "location": "/dtdashboard/#creating-dashboards", 
            "text": "There are two ways to create a new visualization dashboard   create new button on the Dashboards screen  generate new dashboard option in the visualization menu of a compatible running DataTorrent application   Below is an illustrated example and a set of steps for creating a new dashboard from the Dashboards screen using the create new button reatingDashboard.gif](images/dtdashboard/image15.gif)    Provide a unique dashboard name. Names are required to be unique for a single user.  Two different users can have a dashboard with the same name.    Include optional dashboard description.  Descriptions help explain and provide context for visualizations presented in the dashboard to new users, and provide an additional way to search and filter dashboards in the list.    Select compatible applications to include in the data visualizations. Only applications with compatible data visualization sources will be listed.  Any number of applications can be included, and selection can be changed after a dashboard is created.    Choose to automatically generate a new dashboard or create one from scratch. Generating a new dashboard option automatically adds a widget to the new dashboard for every available data source in every selected application.  Creating dashboard from scratch involves manually choosing the widgets to display.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.    Below is an illustrated example of creating a new dashboard with generate new dashboard option in the visualization menu of a compatible running DataTorrent application.     Locate visualize menu in the Application Summary section of a running application.  Only applications with compatible data visualization sources include visualize menu option.    Choose to generate new dashboard from the visualize menu drop-down list.  New dashboard will be automatically named, generated, and saved.  The new dashboard name will reflect the selected application name, and widgets will be automatically added one for every available data source.    Customize and save the dashboard.  Add, remove, resize, and customize the widgets.  Save the changes to preserve the current dashboard state.", 
            "title": "Creating Dashboards"
        }, 
        {
            "location": "/dtdashboard/#modifying-dashboards", 
            "text": "Dashboards controls are presented as a row of buttons just below the dashboard title and description.   New widgets can be added with settings button allows you to change dashboard name, description, and list of associated applications.  Use the save changes button to persist the dashboard state, which includes any changes made to the dashboard settings or widgets.  And finally, display mode enables an alternative visualization mode, which removes widget controls and backgrounds to create a simplified and seamless viewing experience.", 
            "title": "Modifying Dashboards"
        }, 
        {
            "location": "/dtdashboard/#widgets-overview", 
            "text": "Dashboard widgets receive and display data in real time from DataTorrent application data sources.  Widgets can be added, removed, rearranged, and resized at any time.  Each widgets has a unique list of configurable properties, which include interactive elements displayed directly on the widget, as well as data query settings available from the widget settings.", 
            "title": "Widgets Overview"
        }, 
        {
            "location": "/dtdashboard/#adding-widgets", 
            "text": "Widgets can be added to the dashboard by clicking add widget button, selecting one of the available data sources, selecting one or more widgets, and confirming selection by clicking add widget   Alternatively, randomly selected widgets, one for every available data source, can be added to the dashboard by clicking auto generate button.  Widgets will be automatically placed on the dashboard without any further dialogs.  If data sources are responding slowly, auto generate may take longer to add new widgets.  During this time the button will remain disabled to avoid duplicate requests, and will show spinning arrows to indicate that previous action is already in progress.   Whether using auto generate buttons, results are not persisted until save changes is applied.  Each data source supports one or more data schema types, such as snapshot, dimensions  Each schema type has a specific list of compatible widgets which can be selected to visualize the data.", 
            "title": "Adding Widgets"
        }, 
        {
            "location": "/dtdashboard/#editing-widgets", 
            "text": "Each widget has an dimensions, snapshot) and widget type (table, chart, text For snapshot schema, which represents a single point in time, the primary widget controls include   label field selection  quantity field selection  sort order selection   Below is an example of changing label field and sort order for a bar chart widget.   For dimensions schema, which represents a series of points in time, with ability to configure dimensions based on key and value settings, the primary widget controls include   Time ranges selection  live streaming  historical range  Dimensions Selections  key combinations and key values selection  aggregate selection    For Notes widget, a text in Markdown format can be entered and should be translated to HTML look.  Below is an example of using Markdown syntax to produce headings, lists, and quoted text.   After making the widget settings changes, remember to use save changes button to persist the desired results.  If the resulting changes should not be saved, reloading the dashboard will revert it to the the original state.", 
            "title": "Editing Widgets"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "dtGateway"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/apex/", 
            "text": "Apache Apex\n\n\nApache Apex (incubating) is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.\n\n\n\n\nEvent processing guarantees\n\n\nIn-memory performance \n scalability\n\n\nFault tolerance and state management\n\n\nNative rolling and tumbling window support\n\n\nHadoop-native YARN \n HDFS implementation\n\n\n\n\nFor additional information visit \nApache Apex\n.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex/#apache-apex", 
            "text": "Apache Apex (incubating) is the industry\u2019s only open source, enterprise-grade unified stream and batch processing engine.  Apache Apex includes key features requested by open source developer community that are not available in current open source technologies.   Event processing guarantees  In-memory performance   scalability  Fault tolerance and state management  Native rolling and tumbling window support  Hadoop-native YARN   HDFS implementation   For additional information visit  Apache Apex .", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/apex_malhar/", 
            "text": "Apache Apex Malhar\n\n\nApache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.\n\n\n\n\nCapabilities common across Malhar operators\n\n\nFor most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities\n\n\n\n\nFault tolerance\n \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.\n\n\nProcessing guarantees\n \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once \n at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.\n\n\nDynamic updates\n \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.\n\n\nEase of extensibility\n \u2013 Malhar operators are based on templates that are easy to extend.\n\n\nPartitioning support\n \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels\n\n\n\n\nOperator Library Overview\n\n\nInput/output connectors\n\n\nBelow is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator\n\n\n\n\nFile Systems\n \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input \n output operators for HDFS, S3, NFS \n Local Files\n\n\nFlume\n \u2013 NOTE: Flume operator is not yet part of Malhar\n\n\n\n\nMany customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.\n\n\n\n\nRelational databases\n \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.\n\n\nNoSQL databases\n \u2013NoSQL key-value pair databases like Cassandra \n HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt. \n healthcare companies) MongoDB \n CouchDB.\n\n\nMessaging systems\n \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ \n RabbitMQ.\n\n\nNotification systems\n \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP \n SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)\n\n\nIn-memory Databases \n Caching platforms\n - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached \n Redis\n\n\nProtocols\n - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket \n FTP sources\n\n\n\n\nCompute\n\n\nOne of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.\n\n\nBelow is just a snapshot of the compute operators available in Malhar\n\n\n\n\nStatistics \n Math - Provide various mathematical and statistical computations over application defined time windows.\n\n\nFiltering \n pattern matching\n\n\nMachine learning \n Algorithms\n\n\nReal-time model scoring is a very common use case for stream processing platforms. \nMalhar allows users to invoke their R models from streaming applications\n\n\nSorting, Maps, Frequency, TopN, BottomN, Random Generator etc.\n\n\n\n\nQuery \n Script invocation\n\n\nMany streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.\n\n\nParsers\n\n\nThere are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM \n SAX), JSON (flat map converter), Apache log files, syslog, etc.\n\n\nStream manipulation\n\n\nStreaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.\n\n\nSocial Media\n\n\nMalhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Apache Apex-Malhar"
        }, 
        {
            "location": "/apex_malhar/#apache-apex-malhar", 
            "text": "Apache Apex Malhar is an open source operator and codec library that can be used with the Apache Apex platform to build real-time streaming applications.  As part of enabling enterprises extract value quickly, Malhar operators help get data in, analyze it in real-time and get data out of Hadoop in real-time with no paradigm limitations.  In addition to the operators, the library contains a number of demos applications, demonstrating operator features and capabilities.", 
            "title": "Apache Apex Malhar"
        }, 
        {
            "location": "/apex_malhar/#capabilities-common-across-malhar-operators", 
            "text": "For most streaming platforms, connectors are afterthoughts and often end up being simple \u2018bolt-ons\u2019 to the platform. As a result they often cause performance issues or data loss when put through failure scenarios and scalability requirements. Malhar operators do not face these issues as they were designed to be integral parts of apex*.md RTS. Hence, they have following core streaming runtime capabilities   Fault tolerance  \u2013 Apache Apex Malhar operators where applicable have fault tolerance built in. They use the checkpoint capability provided by the framework to ensure that there is no data loss under ANY failure scenario.  Processing guarantees  \u2013 Malhar operators where applicable provide out of the box support for ALL three processing guarantees \u2013 exactly once, at-least once   at-most once WITHOUT requiring the user to write any additional code.  Some operators like MQTT operator deal with source systems that cant track processed data and hence need the operators to keep track of the data. Malhar has support for a generic operator that uses alternate storage like HDFS to facilitate this. Finally for databases that support transactions or support any sort of atomic batch operations Malhar operators can do exactly once down to the tuple level.  Dynamic updates  \u2013 Based on changing business conditions you often have to tweak several parameters used by the operators in your streaming application without incurring any application downtime. You can also change properties of a Malhar operator at runtime without having to bring down the application.  Ease of extensibility  \u2013 Malhar operators are based on templates that are easy to extend.  Partitioning support  \u2013 In streaming applications the input data stream often needs to be partitioned based on the contents of the stream. Also for operators that ingest data from external systems partitioning needs to be done based on the capabilities of the external system. E.g. With the Kafka or Flume operator, the operator can automatically scale up or down based on the changes in the number of Kafka partitions or Flume channels", 
            "title": "Capabilities common across Malhar operators"
        }, 
        {
            "location": "/apex_malhar/#operator-library-overview", 
            "text": "", 
            "title": "Operator Library Overview"
        }, 
        {
            "location": "/apex_malhar/#inputoutput-connectors", 
            "text": "Below is a summary of the various sub categories of input and output operators. Input operators also have a corresponding output operator   File Systems  \u2013 Most streaming analytics use cases we have seen require the data to be stored in HDFS or perhaps S3 if the application is running in AWS. Also, customers often need to re-run their streaming analytical applications against historical data or consume data from upstream processes that are perhaps writing to some NFS share. Hence, it\u2019s not just enough to be able to save data to various file systems. You also have to be able to read data from them. RTS supports input   output operators for HDFS, S3, NFS   Local Files  Flume  \u2013 NOTE: Flume operator is not yet part of Malhar   Many customers have existing Flume deployments that are being used to aggregate log data from variety of sources. However Flume does not allow analytics on the log data on the fly. The Flume input/output operator enables RTS to consume data from flume and analyze it in real-time before being persisted.   Relational databases  \u2013 Most stream processing use cases require some reference data lookups to enrich, tag or filter streaming data. There is also a need to save results of the streaming analytical computation to a database so an operational dashboard can see them. RTS supports a JDBC operator so you can read/write data from any JDBC compliant RDBMS like Oracle, MySQL etc.  NoSQL databases  \u2013NoSQL key-value pair databases like Cassandra   HBase are becoming a common part of streaming analytics application architectures to lookup reference data or store results. Malhar has operators for HBase, Cassandra, Accumulo (common with govt.   healthcare companies) MongoDB   CouchDB.  Messaging systems  \u2013 JMS brokers have been the workhorses of messaging infrastructure in most enterprises. Also Kafka is fast coming up in almost every customer we talk to. Malhar has operators to read/write to Kafka, any JMS implementation, ZeroMQ   RabbitMQ.  Notification systems  \u2013 Almost every streaming analytics application has some notification requirements that are tied to a business condition being triggered. Malhar supports sending notifications via SMTP   SNMP. It also has an alert escalation mechanism built in so users don\u2019t get spammed by notifications (a common drawback in most streaming platforms)  In-memory Databases   Caching platforms  - Some streaming use cases need instantaneous access to shared state across the application. Caching platforms and in-memory databases serve this purpose really well. To support these use cases, Malhar has operators for memcached   Redis  Protocols  - Streaming use cases driven by machine-to-machine communication have one thing in common \u2013 there is no standard dominant protocol being used for communication. Malhar currently has support for MQTT. It is one of the more commonly, adopted protocols we see in the IoT space. Malhar also provides connectors that can directly talk to HTTP, RSS, Socket, WebSocket   FTP sources", 
            "title": "Input/output connectors"
        }, 
        {
            "location": "/apex_malhar/#compute", 
            "text": "One of the most important promises of a streaming analytics platform like Apache Apex is the ability to do analytics in real-time. However delivering on the promise becomes really difficult when the platform does not provide out of the box operators to support variety of common compute functions as the user then has to worry about making these scalable, fault tolerant etc. Malhar takes this responsibility away from the application developer by providing a huge variety of out of the box computational operators. The application developer can thus focus on the analysis.  Below is just a snapshot of the compute operators available in Malhar   Statistics   Math - Provide various mathematical and statistical computations over application defined time windows.  Filtering   pattern matching  Machine learning   Algorithms  Real-time model scoring is a very common use case for stream processing platforms.  Malhar allows users to invoke their R models from streaming applications  Sorting, Maps, Frequency, TopN, BottomN, Random Generator etc.", 
            "title": "Compute"
        }, 
        {
            "location": "/apex_malhar/#query-script-invocation", 
            "text": "Many streaming use cases are legacy implementations that need to be ported over. This often requires re-use some of the existing investments and code that perhaps would be really hard to re-write. With this in mind, Malhar supports invoking external scripts and queries as part of the streaming application using operators for invoking SQL query, Shell script, Ruby, Jython, and JavaScript etc.", 
            "title": "Query &amp; Script invocation"
        }, 
        {
            "location": "/apex_malhar/#parsers", 
            "text": "There are many industry vertical specific data formats that a streaming application developer might need to parse. Often there are existing parsers available for these that can be directly plugged into an Apache Apex application. For example in the Telco space, a Java based CDR parser can be directly plugged into Apache Apex operator. To further simplify development experience, Malhar also provides some operators for parsing common formats like XML (DOM   SAX), JSON (flat map converter), Apache log files, syslog, etc.", 
            "title": "Parsers"
        }, 
        {
            "location": "/apex_malhar/#stream-manipulation", 
            "text": "Streaming data aka \u2018stream\u2019 is raw data that inevitably needs processing to clean, filter, tag, summarize etc. The goal of Malhar is to enable the application developer to focus on \u2018WHAT\u2019 needs to be done to the stream to get it in the right format and not worry about the \u2018HOW\u2019. Hence, Malhar has several operators to perform the common stream manipulation actions like \u2013 DeDupe, GroupBy, Join, Distinct/Unique, Limit, OrderBy, Split, Sample, Inner join, Outer join, Select, Update etc.", 
            "title": "Stream manipulation"
        }, 
        {
            "location": "/apex_malhar/#social-media", 
            "text": "Malhar includes an operator to connect to the popular Twitter stream fire hose.", 
            "title": "Social Media"
        }, 
        {
            "location": "/installation/", 
            "text": "DataTorrent RTS Installation Guide\n\n\nThis guide covers installation of the DataTorrent RTS platform.\n\n\nRequirements\n\n\n\n\nLinux operating system (tested on CentOS 6.x and Ubuntu 12.04)\n\n\nJava 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)\n\n\nHadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH\n\n\nMinimum of 8G RAM available on the Hadoop cluster\n\n\nGoogle Chrome, Firefox, or Safari to access the DataTorrent Console UI\n\n\nPermissions to create HDFS directory for DataTorrent user\n\n\n\n\nInstallation\n\n\nComplete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]\n\n\nBinaries are available in /opt/datatorrent/current/bin and links in /usr/bin\n\n\nConfiguration files located in /opt/datatorrent/current/conf\n\n\nLog files located in /var/log/datatorrent\n\n\nDataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\na.  Installing from self-extracting archive (*.bin)\n\n\n    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin\n\n\n\nb.  Installing from RedHat Package Manager archive (*.rpm)\n\n\n  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm\n\n\n\nLimited Local Installation\n\n\nA limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.\n\n\n\n\nDataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]\n\n\nBinaries are available under $HOME/datatorrent/current/bin\n\n\nConfiguration files located under $HOME/datatorrent/conf\n\n\nLog files located under $HOME/.dt/logs\n\n\n\n\nDataTorrent Gateway running as current user, and managed with dtgateway command\n\n\n\n\n\n\nDownload and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.\n\n\ncurl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin\n\n\n\n\n\n\n\nAdd DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.\n\n\nDATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin\n\n\n\n\n\n\n\nUpgrades\n\n\nDataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.\n\n\nAutomatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.  \n\n\nFull uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.\n\n\nCustomizing Installation\n\n\nVarious options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.\n\n\n./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B \npath\n      Use \npath\n as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U \nuser\n      Use \nuser\n user account for installation.  Default: dtadmin\n-G \ngroup\n     Use \ngroup\n group for installation.  Default: dtadmin ( based on value of \nuser\n )\n-H \npath\n      Use \npath\n for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E \nexpr\n      Adds export \nexpr\n to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway  and dtcli\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s \nfile\n      Use \nfile\n DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e \nfile\n      Use \nfile\n DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.\n\n\n\nSome Hadoop distributions may rquire changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):\n\n\nsudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.\n\n\n\nIf JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:\n\n\nsudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path\n\n\n\nIn some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.\n\n\nsudo ./datatorrent-rts*.bin -U myuser\n\n\n\nInstallation Wizard\n\n\nAfter the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.\n\n\nhttp://\ninstallation_host\n:9090/\n\n\n\nThe Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.\n\n\nDataTorrent installation can be verified by running included demo applications.  See \nLaunching Demo Applications\n for details.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#datatorrent-rts-installation-guide", 
            "text": "This guide covers installation of the DataTorrent RTS platform.", 
            "title": "DataTorrent RTS Installation Guide"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "Linux operating system (tested on CentOS 6.x and Ubuntu 12.04)  Java 7 or 8 (tested with Oracle Java 7, OpenJDK Java 7)  Hadoop (2.2.0 or above) cluster with YARN, HDFS configured, and hadoop executable available in PATH  Minimum of 8G RAM available on the Hadoop cluster  Google Chrome, Firefox, or Safari to access the DataTorrent Console UI  Permissions to create HDFS directory for DataTorrent user", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Complete installation configures DataTorrent to run as a system service, installs globally available binaries, and requires root/sudo privileges to run.  After the installation is complete, you will have the following   DataTorrent platform release ( $DATATORRENT_HOME ) located in /opt/datatorrent/releases/[release_version]  Binaries are available in /opt/datatorrent/current/bin and links in /usr/bin  Configuration files located in /opt/datatorrent/current/conf  Log files located in /var/log/datatorrent  DataTorrent Gateway service dtgateway running as dtadmin, and managed with \"sudo service dtgateway\" command   Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  a.  Installing from self-extracting archive (*.bin)      curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\n    sudo sh ./datatorrent-rts.bin  b.  Installing from RedHat Package Manager archive (*.rpm)    curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.rpm\n  sudo rpm -ivh ./datatorrent-rts.rpm", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#limited-local-installation", 
            "text": "A limited local installation can be helpful for in environments with no root/sudo privileges, or for testing purposes.  All DataTorrent files will be installed under the user's home directory, and DataTorrent Gateway will be running as a local process.  No services or files outside user's home directory will be created.   DataTorrent platform release ( $DATATORRENT_HOME ) located under $HOME/datatorrent/releases/[release_version]  Binaries are available under $HOME/datatorrent/current/bin  Configuration files located under $HOME/datatorrent/conf  Log files located under $HOME/.dt/logs   DataTorrent Gateway running as current user, and managed with dtgateway command    Download and run the installer binary on one of the Hadoop cluster nodes.  This can be done on ResourceManager, NameNode, or any node which has correct Hadoop configuration and is able to communicate with the rest of the cluster.  curl -LSO https://www.datatorrent.com/downloads/datatorrent-rts.bin\nsh ./datatorrent-rts.bin    Add DataTorrent binaries to user PATH environment variable by pasting following into ~/.bashrc, ~/.profile or equivalent environment settings file.  DATATORRENT_HOME=~/datatorrent/current\nexport PATH=$PATH:$DATATORRENT_HOME/bin", 
            "title": "Limited Local Installation"
        }, 
        {
            "location": "/installation/#upgrades", 
            "text": "DataTorrent RTS installer automatically performs an upgrade when the same base installation directory is selected.  Configurations stored in local files such as dt-site.xml and custom-env.sh, as well as contents of DataTorrent DFS root directory will be be used to configure the newer version.  Automatic upgrade behavior can be avoided by providing installer with an options to perform full uninstall prior to running the installation.  See Customizing Installation section below.    Full uninstall prior to installation is strongly recommended when performing major version upgrades.  Not all settings and attributes will be backwards compatible between major releases.  This may result in applications failing to launch when an invalid attribute is loaded from dt-site.xml.  In such cases detailed error messages will be provided during an application launch, helping identify and fix outdated references before successfully launching applications.", 
            "title": "Upgrades"
        }, 
        {
            "location": "/installation/#customizing-installation", 
            "text": "Various options are available to customize the DataTorrent installation.  List of available options be displayed by running installer with -h flag.  ./datatorrent-rts*.bin -h\n\nOptions:\n\n-h             Help menu\n-B  path       Use  path  as base installation directory.  Must be an absolute path.  Default: /opt/datatorrent\n-U  user       Use  user  user account for installation.  Default: dtadmin\n-G  group      Use  group  group for installation.  Default: dtadmin ( based on value of  user  )\n-H  path       Use  path  for location for hadoop executable.  Overrides defaults of HADOOP_PREFIX and PATH.\n-E  expr       Adds export  expr  to custom-env.sh file.  Used to set an environment variable.  Examples include:\n                 -E JAVA_HOME=/my/java                 Java used by dtgateway  and dtcli\n                 -E DT_LOG_DIR=/var/log/datatorrent    Directory for dtgateway logs\n                 -E DT_RUN_DIR=/var/run/datatorrent    Directory for dtgateway pid files\n-s  file       Use  file  DataTorrent dt-site.xml file to configure new installation. Overrides default and previous dt-site.xml\n-e  file       Use  file  DataTorrent custom-env.sh file to configure new installation. Overrides default and previous custom-env.sh\n-g [ip:]port   DataTorrent Gateway address.  Defaults to 0.0.0.0:9090\n-u             Perform full uninstall prior to running installation. WARNING: All current settings will be removed!\n-x             Skip installation steps.  Useful for debugging installation settings with [-v] or perform uninstall with [-u]\n-r             Uninstall current release only.  Does not remove datatorrent, logs, var, etc directories.  Used with RPM uninstall.\n-v             Run in verbose mode, providing extra details for every step.\n-V             Print DataTorrent version and exit.  Some Hadoop distributions may rquire changes to default settings.  For example, when running Apache Hadoop, you may encounter that JAVA_HOME is not set for DTGateway user (defaults to dtadmin):  sudo -u dtadmin hadoop version\nError: JAVA_HOME is not set and could not be found.  If JAVA_HOME is not set, you can export it manually at the end of /opt/datatorrent/current/conf/custom-env.sh or run the installer with following option to set JAVA_HOME during installation time:  sudo ./datatorrent-rts*.bin -E JAVA_HOME=/valid/java/home/path  In some Hadoop installations, you may already have an existing user, which has administrative privileges to HFDS and YARN, and you prefer to use that user to run DTGateway service.  Assuming this user is named myuser, you can run the following command during installation to ensure DTGateway service runs as myuser.  sudo ./datatorrent-rts*.bin -U myuser", 
            "title": "Customizing Installation"
        }, 
        {
            "location": "/installation/#installation-wizard", 
            "text": "After the system installation is complete, remaining configuration steps are performed using the Installation Wizard in the DataTorrent UI Console.  DataTorrent UI Console address is typically hostname of the node where DataTorrent platform was installed, listening on port 9090.  The connection details are provided by the system installer, but may need to modified depending on the way Hadoop cluster is being accessed.  http:// installation_host :9090/  The Installation Wizard can be accessed and repeated at any time to update key system settings from the Configuration section of the DataTorrent UI Console.  DataTorrent installation can be verified by running included demo applications.  See  Launching Demo Applications  for details.", 
            "title": "Installation Wizard"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Configuration"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/dtgateway_security/", 
            "text": "DataTorrent Gateway Security\n\n\nDataTorrent Gateway supports different authentication mechanisms to\nsecure access to the Console. The supported types are local password\nauthentication, kerberos authentication and J\n\n\nConfiguring Authentication\n\n\nAfter DataTorrent RTS installation, you can turn on authentication and\nauthorization support to secure the DataTorrent Gateway. Gateway\nsupports three types of authentication.\n\n\n\n\nPassword\n\n\nKerberos\n\n\nJAAS\n\n\n\n\nEnabling Password Auth\n\n\n\u201cPassword\u201d is the only authentication mechanism presented here that does\nnot depend on any external systems.  When enabled, all users will be\npresented with the login prompt before being able to use the DT Console.\nPassword authentication can be enabled by performing following two\nsteps.\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n    \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\npassword\n/value\n\n    \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n( when running Gateway in local mode use  dtgateway restart command)\n\n\n\n\n\n\nOpen the Gateway URL in your browser, and you should be prompted for\nuser name and password.  Starting with DataTorrent RTS 2.0.0, the\ndefault username and password is \ndtadmin\n and \ndtadmin\n.\n\n\n\n\nOnce authenticated, active user name and an option to log out is\npresented in the top right corner of the DT Console screen.\n\n\n\n\nEnabling Kerberos Auth\n\n\nKerberos authentication can optionally be enabled for Hadoop web access.\nIf this is configured then all web browser access to the Hadoop\nmanagement consoles is Kerberos authenticated. Access to all the web\nservices is also Kerberos authenticated. This Kerberos authentication is\nperformed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.\n\n\nKerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by\nlogging in to kerberos system in a terminal emulator using kinit. Then\nthe user launches a browser and accesses the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.\n\n\nWhen this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the \nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nAdditional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.\n\n\n\n\n\n\nAdd the following properties to \ndt-site.xml\n configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install)\n\n\nconfiguration\n\n...\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.type\n/name\n\n    \nvalue\nkerberos\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.principal\n/name\n\n    \nvalue\n{kerberos-principal-of-web-service}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.kerberos.keytab\n/name\n\n    \nvalue\n{absolute-path-to-keytab-file}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.gateway.http.authentication.token.validity\n/name\n\n    \nvalue\n{authentication-token-validity-in-seconds}\n/value\n\n  \n/property\n\n  \nproperty\n\n  \nname\\\ndt.gateway.http.authentication.cookie.domain\n/name\n\n  \nvalue\\\n{http-cookie-domain-for-authentication-token}\n/value\n\n  \nproperty\n\n    \nname\\\ndt.gateway.http.authentication.cookie.path\n/name\n\n    \nvalue\n{http-cookie-path}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\\\ndt.gateway.http.authentication.signature.secret\n/name\n\n    \nvalue\n{absolute-path-of-secret-file-for-signing-authentication-tokens} \n/value\n\n  \n/property\n\n\n/configuration\n\n\n\n\nNote that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property\n\ndt.gateway.http.authentication.type\n should be replaced with the\nappropriate values for your setup.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nEnabling JAAS Auth\n\n\nJAAS is Java Authentication and Authorization Service. It is a pluggable\nand extensible mechanism for authentication. It is a generic framework\nand the actual authentication is performed by a JAAS plugin module which\ncan be configured using a configuration file. Among others LDAP and PAM\nauthentication can be performed via JAAS.\n\n\nSimilar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the\n\nExternal Role Mapping\n in the \nAuthorization using external roles\n section below for that.\n\n\nThis authentication can be enabled by performing the following general\nsteps. The specific steps for a couple of authentication mechanisms LDAP\nand PAM are shown in the next sections but other authentication\nmechanisms that have a JAAS plugin module can also be used.\n\n\n\n\n\n\nAdd a property to \ndt-site.xml\n configuration file, typically located\n    under \n/opt/datatorrent/current/conf\n ( or\n    \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n      \nname\ndt.gateway.http.authentication.type\n/name\n\n      \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\\\n\n      \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n      \nvalue\nname-of-jaas-module\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\nThe \ndt.gateway.http.authentication.jaas.name\n property specifies the\nplugin module to use with JAAS and the value should be the name of the\nplugin module.\n\n\n\n\n\n\nThe name of the plugin module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be \n/home/dtadmin/.java.login.config\n, if running as a\n    normal user it would be \n~/.java.login.config\n. The sample\n    configurations for LDAP and PAM are shown in the next sections.\n\n\n\n\n\n\nThe classes for the plugin module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.\n\n\nThe following step shows how to do this\n\n\na.  Edit the \ncustom-env.sh\n configuration file, typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like\n\n\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n( when running Gateway in local mode use  \ndtgateway restart\n command)\n\n\n\n\n\n\nLDAP\n\n\nLDAP authentication is a directory based authentication mechanism used\nin many enterprises. To enable LDAP authentication following are the\nspecifics for the configuration steps described above.\n\n\n\n\n\n\nFor step 1 above specify LDAP as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be \u201cldap\u201d\n    (without the quotes). This name should now be configured with the\n    appropriate settings as described in the next step.\n\n\n\n\n\n\nFor step 2, the JAAS name specified above should be configured with\n    the appropriate LDAP settings in the .java.login.config file. A\n    sample configuration is shown below\n\n\nldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case\n\u201cldap\u201d must match the jaas name specified in step 1. The first property\nwithin the braces \ncom.sun.security.auth.module.LdapLoginModule\n specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The next settings are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.\n\n\nPAM\n\n\nPAM is Pluggable Authentication Module. It is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules\nthey can be used in Gateway as well.\n\n\nPAM is implemented in C language and has C API. JPam is Java PAM bridge\nthat uses JNI to make PAM calls. It is available here\n\nhttp://jpam.sourceforge.net/\n and has detailed documentation on how to install and set it up. JPam also has a JAAS plugin module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS plugin module is available.\n\n\nTo enable JPAM following are the specifics for the configuration steps\nto enable JAAS authentication described above.\n\n\n\n\n\n\nJPAM has to be first installed on the system. Please following the\n    installation instructions from the JPAM website.\n\n\n\n\n\n\nFor step 1 above Specify JPAM as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be\n    \u201cnet-sf-jpam\u201d (without the quotes). This name should now be\n    configured with the appropriate settings as described in the next\n    step.\n\n\n\n\n\n\nFor step 2 the JAAS name specified above should be configured with\n    the appropriate JPAM settings in the .java.login.config file. A\n    sample configuration is shown below\n\n\nnet-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};\n\n\n\n\n\n\n\nNote that the first string before the open brace, in this case\n\u201cnet-sf-jpam\u201d must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM specific settings. The\nserviceName setting for example specifies the PAM service which would\nneed to be further configured in /etc/pam.d/net-sf-jpam to specify the\nPAM modules to use. Refer to PAM documentation on how to configure a PAM\nservice with PAM modules. If using Linux local accounts system-auth\ncould be specified as the PAM module in this file. The above settings\nare only provided as a reference example and a different serviceName for\nexample can be chosen.\n\n\n\n\nFor step 3 add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format \nJPam-\nversion\n.jar\n where\n   \nversion\n denotes the version, version 1.1 has been tested.\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/JPam-\\\nversion\\\n.jar\n\n\n\n\n\n\n\nGroups\n\n\nFor group support such as using LDAP groups for authorization refer to\nthe \nAuthorization using external roles\n section below.\n\n\nConfiguring Authorization\n\n\nWhen any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.\n\n\nPermissions\n\n\nThe list of all possible permissions in the DT Gateway is as follow:\n\n\n ACCESS_RM_PROXY\n\n\nAllow HTTP proxying requests to YARN\u2019s Resource Manager REST API\n\n\n EDIT_GLOBAL_CONFIG\n\n\nEdit global settings\n\n\n EDIT_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\n LAUNCH_APPS\n\n\nLaunch Apps\n\n\nMANAGE_LICENSES\n\n\nManage DataTorrent RTS licenses\n\n\n MANAGE_OTHER_USERS_APPS\n\n\nManage (e.g. edit, kill, etc) applications launched by other users\n\n\n MANAGE_OTHER_USERS_APP_PACKAGES\n\n\nManage App Packages uploaded by other users  \n\n\n MANAGE_ROLES\n\n\nManage roles (create/delete roles, or assign permissions to roles)\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nManage system alerts\n\n\n MANAGE_USERS\n\n\nManage users (create/delete users, change password)\n\n\n UPLOAD_APP_PACKAGES\n\n\nUpload App Packages and use the app builder\n\n\n VIEW_GLOBAL_CONFIG\n\n\nView global settings   \n\n\n VIEW_LICENSES\n\n\nView DataTorrent RTS licenses\n\n\n VIEW_OTHER_USERS_APPS\n\n\nView applications launched by others\n\n\n VIEW_OTHER_USERS_APP_PACKAGES\n\n\nView App Packages uploaded by other users\n\n\n VIEW_OTHER_USERS_CONFIG\n\n\nEdit other users\u2019 settings\n\n\n VIEW_SYSTEM_ALERTS\n\n\nView system alerts\n\n\nDefault Roles\n\n\nDataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.\n\n\nAdmin\n\n\nAn administrator of DataTorrent RTS is intended to be able to install,\nmanage \n modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.\n\n\nOperator\n\n\nOperators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.\n\n\nHere is the list of default permissions given to operators\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_OTHER_USERS_APPS\n\n\nVIEW_OTHER_USERS_APP_PACKAGES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nNote that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.  \n\n\nDeveloper\n\n\nDevelopers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.\n\n\nHere is the list of default permissions given to developers\n\n\nLAUNCH_APPS\n\n\nUPLOAD_APP_PACKAGES\n\n\nMANAGE_SYSTEM_ALERTS\n\n\nVIEW_GLOBAL_CONFIG\n\n\nVIEW_LICENSES\n\n\nVIEW_SYSTEM_ALERTS\n\n\nApp Permissions and App Package Permissions\n\n\nUsers can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.\n\n\nThe default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.\n\n\nAs of RTS 2.0.0, the Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the \nDT Gateway REST API document\n and \nhere\n for examples on how to use the REST API.\n\n\nViewing and Managing Auth in the Console\n\n\nViewing User Profile\n\n\nAfter you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has.\n\n\nAdministering Auth\n\n\n\n\nFrom the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:\n\n\n\n\nCreate new users\n\n\nDelete users\n\n\nChange existing users\u2019 passwords\n\n\nAssign roles to users\n\n\nCreate roles\n\n\nAssign permissions to roles\n\n\n\n\n\n\nDataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.\n\n\n Authorization using external roles\n\n\nWhen using an external authentication mechanism such as Kerberos or\nJAAS, roles defined in these external systems can be used to control\nauthorization in DataTorrent RTS. There are two steps involved. First\nsupport for external roles has to be configured in Gateway. This is\ndescribed below in the sections \nKerberos roles\n and\n\nJAAS roles\n. Then a mapping should be specified between\nthe external roles and DataTorrent roles to specify which role should be\nused for a user when the user logs in. How to setup this mapping is\ndescribed in the \nExternal Role Mapping\n section below.\nWhen this mapping is setup only users with roles that have a mapping are\nallowed to login the rest are not. The next sections describe how to\nconfigure the system for handling external roles.\n\n\nKerberos roles \n\n\nWhen Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form \nuser/group@DOMAIN\n\nthe group portion is used as the external role and no additional\nconfiguration is necessary.\n\n\nJAAS roles \n\n\nTo use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the implementing JAAS authentication module.\nSome of these principals can be for roles and these role principals need\nto be identified from the list. Additional configuration is needed to do\nthis and this configuration is specific to the JAAS authentication\nmodule being used. Specifically the JAVA class name identifying the role\nprincipal needs to be specified. This can be specified using the\nproperty \n\u201cdt.gateway.http.authentication.jaas.role.class.name\u201d\n in the\nconfiguration file as shown below\n\n\nconfiguration\n\n...\n  \nproperty\n\n       \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n          \nvalue\nfull-class-name-of-role\n/value\n\n \n/property\n\n...\n\n/configuration\n\n\n\n\n\nCallback Handlers\n\n\nIn JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. The callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS supports\nspecification of a custom callback handler to handle these callbacks.\nThe custom callback handler can be specified using the property\n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d. When this\nproperty is not specified a default callback handler is used but it may\nnot be sufficient for all login modules like in the LDAP case described\nbelow. The property can be specified as follows\n\n\nconfiguration\n\n...\n\nproperty\n                                                                            \nname\ndt.gateway.http.authentication.jaas.callback.class.name\n/name\n\n\nvalue\nfull-class-name-of-callback\n/value\n\n\n/property\n\n...\n\n/configuration\n\n\n\n\n\nCustom callback handlers can be implemented by extending the default\ncallback handler so they can build upon it or they can be built from\nscratch. The LDAP callback handler described below also extends the\ndefault callback handler. The source for the default callback handler\ncan be found here \nDefaultCallbackHandler\n can be used as a reference when\nimplementing new callback handlers.\n\n\nLDAP\n\n\nWhen using LDAP with JAAS, to use LDAP roles, a  LDAP login module\nsupporting roles should be used. Any LDAP module that supports roles can\nbe used. Jetty implements one such login module. The steps to configure\nthis module are as follows.\n\n\n\n\n\n\nThe Gateway service in DataTorrent RTS 2.0 is compatible with Jetty 8. The class name identifying the \n    role principal is \n\u201corg.eclipse.jetty.plus.jaas.JAASRole\u201d\n. When using the Jetty LDAP\n    login module a custom JAAS callback has to be handled by the caller.\n    This has been implemented by DataTorrent in a callback handler. The\n    class name for the callback handler implementation should be\n    specified as a property. The property name is\n    \n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d\n. The class\n    name of the callback handler is\n    \n\u201ccom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\u201d\n.\n    This property should be specified along with the class name\n    identifying the role principal for the Jetty login module as\n    described in the section above. The configuration file with all the\n    JAAS properties including these looks as follows\n\n\nconfiguration\n\n...\n  \nproperty\n\n          \nname\ndt.gateway.http.authentication.type\n/name\n\n          \nvalue\njaas\n/value\n\n  \n/property\n\n  \nproperty\n\n         \nname\ndt.gateway.http.authentication.jaas.name\n/name\n\n         \nvalue\nldap\n/value\n\n  \n/property\n\n  \nproperty\n  \n        \nname\ndt.gateway.http.authentication.jaas.role.class.name\n/name\n\n         \nvalue\norg.eclipse.jetty.plus.jaas.JAASRole\n/value\n\n  \n/property\n\n  \n/property\n\n\nname\n\n    dt.gateway.http.authentication.jaas.callback.class.name \n/name\n            \n\nvalue\n\n    com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\n\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\nNote that the JAAS callback handler property can be used to specify a\ncustom callback handler. The source for the Jetty custom callback\nhandler used above can be found here JettyJAASCallbackHandler\n\n\n\n\n\n\nAn issue was discovered with the Jetty login module supplied with\n    Jetty 8 that prevented LDAP authentication to be successful even\n    when the user credentials were correct. DataTorrent has a fix for\n    this and is providing the login module with the fix in a separate\n    package called dt-auth. The classname for the module is\n    \u201ccom.datatorrent.auth.jetty.JettyLdapLoginModule\u201d. The dt-auth\n    project along with the source can be found here \nAuth\n. DataTorrent\n    is working on submitting this fix back to Jetty project so that it\n    gets back into the main source.\n\n\nThe JAAS configuration file as described in\n\nLDAP\n section under \nEnabling JAAS Auth\n should be configured to specify the ldap settings\nfor roles. A sample configuration  roles based parameters to the\nconfiguration shown before\n\n\nldap {\n        com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" authenticationMethod=\"simple\"\n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\u201dcn\u201d\n    contextFactory=\u201dcom.sun.jndi.ldap.LdapCtxFactory\u201d;\n};\n\n\n\nFor more ldap settings refer to the java documentation of the login\n\n\n\n\n\n\nAfter the above configuration changes are made the gateway service\n    needs to be restarted. Before restarting however the different\n    classes specified in the configuration files above namely the\n    DataTorrent Jetty callback handler, the Jetty login module with the\n    DataTorrent fix and the Jetty dependencies containing the role class\n    should all be available for Gateway. This can be done by specifying\n    the jars containing these classes in a classpath variable that is\n    used by Gateway.\n\n\nThe jars can be obtained from the \nDataTorrent Auth\n project.\n\n\nPlease follow the instructions in the above url to obtain the project\njar files. After obtaining the jar files perform the following step to\nmake them available to Gateway\n\n\nEdit the custom-env.sh configuration file, typically located under\n\n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\nlocal install) and append the list of jars obtained above to the\n\nDT_CLASSPATH\n variable. This needs to be added at the end of the\nfile in the section for specifying local overrides to environment\nvariables. The line would look like\n\n\nexport DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\nwhen running Gateway in local mode use dtgateway restart command.\n\n\n\n\n\n\nExternal Role Mapping \n\n\nExternal role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps\n\n\n\n\n\n\nIn the configuration folder typically located under\n    \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is\n\n\nadmins:admin\nstaff: developer\n\n\n\nThis maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\nwhen running Gateway in local mode use  dtgateway restart command\n\n\n\n\n\n\nAdministering Using Command Line \n\n\nYou can also utilize the \ndtGateway REST API\n (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.\n\n\nLog in as admin:\n\n\n% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login\n\n\n\nThis curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar\n\n\nChanging the admin password:\n\n\n% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin\n\n\n\nThis uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.\n\n\nAdding a second admin user:\n\n\n% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.\n\n\nAdding a user in the developer role:\n\n\n% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.\n\n\nListing all users:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users\n\n\n\nGetting info for a specific user:\n\n\n% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john\n\n\n\nThis command returns the information about the user \u201cjohn\u201d.\n\n\nRemoving a user:\n\n\n% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane\n\n\n\nThis command removes the user \u201cjane\u201d.\n\n\nEnabling HTTPS in Gateway\n\n\nHTTPS in the Gateway can be enabled by performing following two steps.\n\n\n\n\n\n\nGenerate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here: \nhttp://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html\n\n\n\n\n\n\nAdd a property to dt-site.xml configuration file, typically located under \n/opt/datatorrent/current/conf\n ( or \n~/datatorrent/current/conf\n for local install).\n\n\nconfiguration\n\n...\n  \nproperty\n\n           \nname\ndt.gateway.sslKeystorePath\n/name\n\n           \nvalue\n{/path/to/keystore}\n/value\n\n  \n/property\n\n  \nproperty\n\n            \nname\ndt.gateway.sslKeystorePassword\n/name\n\n             \nvalue\n{keystore-password}\n/value\n\n  \n/property\n\n  \nproperty\n\n    \nname\ndt.attr.GATEWAY_USE_SSL\n/name\n\n          \nvalue\ntrue\n/value\n\n  \n/property\n\n...\n\n/configuration\n\n\n\n\n\n\n\n\nRestart the Gateway by running\n\n\nsudo service dtgateway restart\n\n\n\n( when running Gateway in local mode use \ndtgateway restart\n command)", 
            "title": "Security"
        }, 
        {
            "location": "/dtgateway_security/#datatorrent-gateway-security", 
            "text": "DataTorrent Gateway supports different authentication mechanisms to\nsecure access to the Console. The supported types are local password\nauthentication, kerberos authentication and J", 
            "title": "DataTorrent Gateway Security"
        }, 
        {
            "location": "/dtgateway_security/#configuring-authentication", 
            "text": "After DataTorrent RTS installation, you can turn on authentication and\nauthorization support to secure the DataTorrent Gateway. Gateway\nsupports three types of authentication.   Password  Kerberos  JAAS", 
            "title": "Configuring Authentication"
        }, 
        {
            "location": "/dtgateway_security/#enabling-password-auth", 
            "text": "\u201cPassword\u201d is the only authentication mechanism presented here that does\nnot depend on any external systems.  When enabled, all users will be\npresented with the login prompt before being able to use the DT Console.\nPassword authentication can be enabled by performing following two\nsteps.    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n     property \n     name dt.gateway.http.authentication.type /name \n     value password /value \n     /property \n... /configuration     Restart the Gateway by running  sudo service dtgateway restart  ( when running Gateway in local mode use  dtgateway restart command)    Open the Gateway URL in your browser, and you should be prompted for\nuser name and password.  Starting with DataTorrent RTS 2.0.0, the\ndefault username and password is  dtadmin  and  dtadmin .   Once authenticated, active user name and an option to log out is\npresented in the top right corner of the DT Console screen.", 
            "title": "Enabling Password Auth"
        }, 
        {
            "location": "/dtgateway_security/#enabling-kerberos-auth", 
            "text": "Kerberos authentication can optionally be enabled for Hadoop web access.\nIf this is configured then all web browser access to the Hadoop\nmanagement consoles is Kerberos authenticated. Access to all the web\nservices is also Kerberos authenticated. This Kerberos authentication is\nperformed using a protocol called SPNEGO which is Kerberos over HTTP.\nPlease refer to the administration guide of your Hadoop distribution on\nhow to enable this. The web browsers must also support SPNEGO, most\nmodern browsers do and should be configured to use SPNEGO for the\nspecific URLs being accessed. Please refer to the documentation of the\nindividual browsers on how to configure this. Gateway handles the SPNEGO\nauthentication needed when communicating with the Hadoop web-services if\nKerberos authentication is enabled for Hadoop web access.  Kerberos authentication can be enabled for UI Console as well. When it\nis enabled access to the Gateway web-services is Kerberos authenticated.\nThe browser should be configured for SPNEGO authentication when\naccessing the Console. A user typically obtains a master ticket first by\nlogging in to kerberos system in a terminal emulator using kinit. Then\nthe user launches a browser and accesses the Console URL. The browser\nwill use the master ticket obtained by kinit earlier to obtain the\nnecessary security tokens to authenticate with the Gateway.  When this authentication is enabled the first authenticated user that\naccesses the system is assigned the admin role as there are no other\nusers in the system at this point. Any subsequent authenticated user\nthat access the system starts with no roles. The admin user can then\nassign roles to these users. This behavior can be changed by configuring\nan external role mapping. Please refer to the  External Role Mapping  in the  Authorization using external roles  section below for that.  Additional configuration is needed to enable Kerberos authentication for\nthe Console. A separate set of kerberos credentials are needed. These\ncan be same as the Hadoop web-service Kerberos credentials which are\ntypically identified with the principal HTTP/HOST@DOMAIN. A few other\nconfiguration properties are also needed. These can be specified in the\nsame \u201cdt-site.xml\u201d configuration file as the DT Gateway authentication\nconfiguration described in the Operation and Installation Guide. This\nauthentication can be set up using the following steps.    Add the following properties to  dt-site.xml  configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install)  configuration \n...\n   property \n     name dt.gateway.http.authentication.type /name \n     value kerberos /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.principal /name \n     value {kerberos-principal-of-web-service} /value \n   /property \n   property \n     name dt.gateway.http.authentication.kerberos.keytab /name \n     value {absolute-path-to-keytab-file} /value \n   /property \n   property \n     name dt.gateway.http.authentication.token.validity /name \n     value {authentication-token-validity-in-seconds} /value \n   /property \n   property \n   name\\ dt.gateway.http.authentication.cookie.domain /name \n   value\\ {http-cookie-domain-for-authentication-token} /value \n   property \n     name\\ dt.gateway.http.authentication.cookie.path /name \n     value {http-cookie-path} /value \n   /property \n   property \n     name\\ dt.gateway.http.authentication.signature.secret /name \n     value {absolute-path-of-secret-file-for-signing-authentication-tokens}  /value \n   /property  /configuration   Note that the kerberos principal for web service should begin with\nHTTP/\u2026  All the values for the properties above except for the property dt.gateway.http.authentication.type  should be replaced with the\nappropriate values for your setup.    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)", 
            "title": "Enabling Kerberos Auth"
        }, 
        {
            "location": "/dtgateway_security/#enabling-jaas-auth", 
            "text": "JAAS is Java Authentication and Authorization Service. It is a pluggable\nand extensible mechanism for authentication. It is a generic framework\nand the actual authentication is performed by a JAAS plugin module which\ncan be configured using a configuration file. Among others LDAP and PAM\nauthentication can be performed via JAAS.  Similar to Kerberos when this authentication is enabled the first\nauthenticated user that accesses the system is assigned the admin role\nas there are no other users in the system at this point. Any subsequent\nauthenticated user that access the system starts with no roles. The\nadmin user can then assign roles to these users. This behavior can be\nchanged by configuring an external role mapping. Please refer to the External Role Mapping  in the  Authorization using external roles  section below for that.  This authentication can be enabled by performing the following general\nsteps. The specific steps for a couple of authentication mechanisms LDAP\nand PAM are shown in the next sections but other authentication\nmechanisms that have a JAAS plugin module can also be used.    Add a property to  dt-site.xml  configuration file, typically located\n    under  /opt/datatorrent/current/conf  ( or\n     ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n       name dt.gateway.http.authentication.type /name \n       value jaas /value \n   /property \n   property\\ \n       name dt.gateway.http.authentication.jaas.name /name \n       value name-of-jaas-module /value \n   /property \n... /configuration   The  dt.gateway.http.authentication.jaas.name  property specifies the\nplugin module to use with JAAS and the value should be the name of the\nplugin module.    The name of the plugin module specified above should be configured\n    with the appropriate settings for the plugin. This is module\n    specific configuration. This can be done in a file named\n    .java.login.config in the home directory of the user Gateway is\n    running under. If DataTorrent RTS was installed as a superuser,\n    Gateway would typically run as dtadmin and this file path would\n    typically be  /home/dtadmin/.java.login.config , if running as a\n    normal user it would be  ~/.java.login.config . The sample\n    configurations for LDAP and PAM are shown in the next sections.    The classes for the plugin module may need to be made available to\n    Gateway if they are not available in the default classpath. This can\n    be done by specifying the jars containing these classes in a\n    classpath variable that is used by Gateway.  The following step shows how to do this  a.  Edit the  custom-env.sh  configuration file, typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) and append the list of jars obtained above to the\n    DT_CLASSPATH variable. This needs to be added at the end of the\n    file in the section for specifying local overrides to environment\n    variables. The line would look like  export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart \n( when running Gateway in local mode use   dtgateway restart  command)    LDAP  LDAP authentication is a directory based authentication mechanism used\nin many enterprises. To enable LDAP authentication following are the\nspecifics for the configuration steps described above.    For step 1 above specify LDAP as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be \u201cldap\u201d\n    (without the quotes). This name should now be configured with the\n    appropriate settings as described in the next step.    For step 2, the JAAS name specified above should be configured with\n    the appropriate LDAP settings in the .java.login.config file. A\n    sample configuration is shown below  ldap {\n  com.sun.security.auth.module.LdapLoginModule required\n  userProvider=\"ldap://ldap-server-hostname\"\n  authIdentity=\"uid={USERNAME},ou=users,dc=domain,dc=com\";\n};    Note that the first string before the open brace, in this case\n\u201cldap\u201d must match the jaas name specified in step 1. The first property\nwithin the braces  com.sun.security.auth.module.LdapLoginModule  specifies\nthe actual JAAS plugin implementation class providing the LDAP\nauthentication. The next settings are LDAP settings specific to your\norganization and could be different from ones shown above. The above\nsettings are only provided as a reference example.  PAM  PAM is Pluggable Authentication Module. It is a Linux system equivalent\nof JAAS where applications call the generic PAM interface and the actual\nauthentication implementations called PAM modules are specified using\nconfiguration files. PAM is the login authentication mechanism in Linux\nso it can be used for example to authenticate and use local Linux user\naccounts in Gateway. If organizations have configured other PAM modules\nthey can be used in Gateway as well.  PAM is implemented in C language and has C API. JPam is Java PAM bridge\nthat uses JNI to make PAM calls. It is available here http://jpam.sourceforge.net/  and has detailed documentation on how to install and set it up. JPam also has a JAAS plugin module and hence can be used in Gateway via JAAS. Note that any other PAM implementation can be used as long as a JAAS plugin module is available.  To enable JPAM following are the specifics for the configuration steps\nto enable JAAS authentication described above.    JPAM has to be first installed on the system. Please following the\n    installation instructions from the JPAM website.    For step 1 above Specify JPAM as the authentication module to use\n    with JAAS by first specifying the value of the\n    dt.gateway.http.authentication.jaas.name property above to be\n    \u201cnet-sf-jpam\u201d (without the quotes). This name should now be\n    configured with the appropriate settings as described in the next\n    step.    For step 2 the JAAS name specified above should be configured with\n    the appropriate JPAM settings in the .java.login.config file. A\n    sample configuration is shown below  net-sf-jpam {\n   net.sf.jpam.jaas.JpamLoginModule required serviceName=\"net-sf-jpam\";\n};    Note that the first string before the open brace, in this case\n\u201cnet-sf-jpam\u201d must match the jaas name specified in step 1. The first\nproperty within the braces net.sf.jpam.jaas.JpamLoginModule specifies\nthe actual JAAS plugin implementation class providing the JPAM\nauthentication. The next settings are JPAM specific settings. The\nserviceName setting for example specifies the PAM service which would\nneed to be further configured in /etc/pam.d/net-sf-jpam to specify the\nPAM modules to use. Refer to PAM documentation on how to configure a PAM\nservice with PAM modules. If using Linux local accounts system-auth\ncould be specified as the PAM module in this file. The above settings\nare only provided as a reference example and a different serviceName for\nexample can be chosen.   For step 3 add the JPam jar to the DT_CLASSPATH variable. The JPam\n    jar should be available in the JPam installation package and\n    typically has the filename format  JPam- version .jar  where\n    version  denotes the version, version 1.1 has been tested. export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/JPam-\\ version\\ .jar    Groups  For group support such as using LDAP groups for authorization refer to\nthe  Authorization using external roles  section below.", 
            "title": "Enabling JAAS Auth"
        }, 
        {
            "location": "/dtgateway_security/#configuring-authorization", 
            "text": "When any authentication method is enabled, authorization will also be\nenabled.  DT Gateway uses roles and permissions for authorization.\n Permissions are possession of the authority to perform certain actions,\ne.g. to launch apps, or to add users.  Roles have a collection of\npermissions and individual users are assigned to one or more roles.\n What roles the user is in dictates what permissions the user has.", 
            "title": "Configuring Authorization"
        }, 
        {
            "location": "/dtgateway_security/#permissions", 
            "text": "The list of all possible permissions in the DT Gateway is as follow:   ACCESS_RM_PROXY  Allow HTTP proxying requests to YARN\u2019s Resource Manager REST API   EDIT_GLOBAL_CONFIG  Edit global settings   EDIT_OTHER_USERS_CONFIG  Edit other users\u2019 settings   LAUNCH_APPS  Launch Apps  MANAGE_LICENSES  Manage DataTorrent RTS licenses   MANAGE_OTHER_USERS_APPS  Manage (e.g. edit, kill, etc) applications launched by other users   MANAGE_OTHER_USERS_APP_PACKAGES  Manage App Packages uploaded by other users     MANAGE_ROLES  Manage roles (create/delete roles, or assign permissions to roles)  MANAGE_SYSTEM_ALERTS  Manage system alerts   MANAGE_USERS  Manage users (create/delete users, change password)   UPLOAD_APP_PACKAGES  Upload App Packages and use the app builder   VIEW_GLOBAL_CONFIG  View global settings      VIEW_LICENSES  View DataTorrent RTS licenses   VIEW_OTHER_USERS_APPS  View applications launched by others   VIEW_OTHER_USERS_APP_PACKAGES  View App Packages uploaded by other users   VIEW_OTHER_USERS_CONFIG  Edit other users\u2019 settings   VIEW_SYSTEM_ALERTS  View system alerts", 
            "title": "Permissions"
        }, 
        {
            "location": "/dtgateway_security/#default-roles", 
            "text": "DataTorrent RTS ships with three roles by default. The permissions for\nthese roles have been set accordingly but can be customized if needed.  Admin  An administrator of DataTorrent RTS is intended to be able to install,\nmanage   modify DataTorrent RTS as well as all the applications running\non it. They have ALL the permissions.  Operator  Operators are intended to ensure DataTorrent RTS and the applications\nrunning on top of it are always up and running optimally. The default\npermissions assigned to Operators, enable them to effectively\ntroubleshoot the entire RTS system and the applications running on it.\nOperators are not allowed however to develop or launch new applications.\nOperators are also not allowed to make system configuration changes or\nmanage users.  Here is the list of default permissions given to operators  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_OTHER_USERS_APPS  VIEW_OTHER_USERS_APP_PACKAGES  VIEW_SYSTEM_ALERTS  Note that VIEW_OTHER_USERS_APPS and VIEW_OTHER_USERS_APP_PACKAGES\nare in the list.  This means all users in the \u201coperator\u201d role will have\nread access to all apps and all app packages in the system.  You can\nremove those permissions from the \u201coperator\u201d role using the Console if\nthis is not desirable.    Developer  Developers need to have to ability to develop, manage and run\napplications on DataTorrent RTS. They are not allowed to change any\nsystem settings or manage licenses.  Here is the list of default permissions given to developers  LAUNCH_APPS  UPLOAD_APP_PACKAGES  MANAGE_SYSTEM_ALERTS  VIEW_GLOBAL_CONFIG  VIEW_LICENSES  VIEW_SYSTEM_ALERTS", 
            "title": "Default Roles"
        }, 
        {
            "location": "/dtgateway_security/#app-permissions-and-app-package-permissions", 
            "text": "Users can share their running application instances and their\napplication packages with certain roles or certain users on a\nper-instance or per-package basis.  Users can specify which users or\nwhile roles have read-only or read-write access.  In addition, users can\nset their own defaults so they does not have to make permission change\nevery time they launch an app or uploads an app package.  The default for app and app package sharing is that the \u201coperator\u201d role\nhas read-only permission.  As of RTS 2.0.0, the Console does not yet support managing App\nPermissions or App Package Permissions.  But one can manage App\nPermissions and App Package Permissions using the Gateway REST API with\nURI\u2019s /ws/v2/apps/{appid}/permissions and\n/ws/v2/appPackages/{user}/{name}/permissions respectively.  Please refer\nto the  DT Gateway REST API document  and  here  for examples on how to use the REST API.", 
            "title": "App Permissions and App Package Permissions"
        }, 
        {
            "location": "/dtgateway_security/#viewing-and-managing-auth-in-the-console", 
            "text": "", 
            "title": "Viewing and Managing Auth in the Console"
        }, 
        {
            "location": "/dtgateway_security/#viewing-user-profile", 
            "text": "After you are logged in on the Console, click on the Configuration tab\non the left, and select \u201cUser Profile\u201d.  This gives you the information\nof the logged in user, including what roles the user is in, and what\npermissions the user has. \nAdministering Auth   From the Configuration tab, click on \u201cAuth Management\u201d.  On this page,\nyou can perform the following tasks:   Create new users  Delete users  Change existing users\u2019 passwords  Assign roles to users  Create roles  Assign permissions to roles    DataTorrent RTS installation comes with three preset roles (admin,\noperator, and developer).  You can edit the permissions for those roles\nexcept for admin.", 
            "title": "Viewing User Profile"
        }, 
        {
            "location": "/dtgateway_security/#kerberos-roles", 
            "text": "When Kerberos authentication is used the role for the user is derived\nfrom the principal. If the principal is of the form  user/group@DOMAIN \nthe group portion is used as the external role and no additional\nconfiguration is necessary.", 
            "title": "Kerberos roles "
        }, 
        {
            "location": "/dtgateway_security/#jaas-roles", 
            "text": "To use JAAS roles the system should be configured first to recognize\nthese roles. When a user is authenticated with JAAS a list of principals\nis returned for the user by the implementing JAAS authentication module.\nSome of these principals can be for roles and these role principals need\nto be identified from the list. Additional configuration is needed to do\nthis and this configuration is specific to the JAAS authentication\nmodule being used. Specifically the JAVA class name identifying the role\nprincipal needs to be specified. This can be specified using the\nproperty  \u201cdt.gateway.http.authentication.jaas.role.class.name\u201d  in the\nconfiguration file as shown below  configuration \n...\n   property \n        name dt.gateway.http.authentication.jaas.role.class.name /name \n           value full-class-name-of-role /value \n  /property \n... /configuration   Callback Handlers  In JAAS authentication, a login module may need a custom callback to be\nhandled by the caller. The callbacks are typically used to provide\nauthentication credentials to the login module. DataTorrent RTS supports\nspecification of a custom callback handler to handle these callbacks.\nThe custom callback handler can be specified using the property\n\u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d. When this\nproperty is not specified a default callback handler is used but it may\nnot be sufficient for all login modules like in the LDAP case described\nbelow. The property can be specified as follows  configuration \n... property                                                                              name dt.gateway.http.authentication.jaas.callback.class.name /name  value full-class-name-of-callback /value  /property \n... /configuration   Custom callback handlers can be implemented by extending the default\ncallback handler so they can build upon it or they can be built from\nscratch. The LDAP callback handler described below also extends the\ndefault callback handler. The source for the default callback handler\ncan be found here  DefaultCallbackHandler  can be used as a reference when\nimplementing new callback handlers.  LDAP  When using LDAP with JAAS, to use LDAP roles, a  LDAP login module\nsupporting roles should be used. Any LDAP module that supports roles can\nbe used. Jetty implements one such login module. The steps to configure\nthis module are as follows.    The Gateway service in DataTorrent RTS 2.0 is compatible with Jetty 8. The class name identifying the \n    role principal is  \u201corg.eclipse.jetty.plus.jaas.JAASRole\u201d . When using the Jetty LDAP\n    login module a custom JAAS callback has to be handled by the caller.\n    This has been implemented by DataTorrent in a callback handler. The\n    class name for the callback handler implementation should be\n    specified as a property. The property name is\n     \u201cdt.gateway.http.authentication.jaas.callback.class.name\u201d . The class\n    name of the callback handler is\n     \u201ccom.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler\u201d .\n    This property should be specified along with the class name\n    identifying the role principal for the Jetty login module as\n    described in the section above. The configuration file with all the\n    JAAS properties including these looks as follows  configuration \n...\n   property \n           name dt.gateway.http.authentication.type /name \n           value jaas /value \n   /property \n   property \n          name dt.gateway.http.authentication.jaas.name /name \n          value ldap /value \n   /property \n   property   \n         name dt.gateway.http.authentication.jaas.role.class.name /name \n          value org.eclipse.jetty.plus.jaas.JAASRole /value \n   /property \n   /property  name \n    dt.gateway.http.authentication.jaas.callback.class.name  /name              value \n    com.datatorrent.contrib.security.jetty.JettyJAASCallbackHandler /value \n   /property \n... /configuration   Note that the JAAS callback handler property can be used to specify a\ncustom callback handler. The source for the Jetty custom callback\nhandler used above can be found here JettyJAASCallbackHandler    An issue was discovered with the Jetty login module supplied with\n    Jetty 8 that prevented LDAP authentication to be successful even\n    when the user credentials were correct. DataTorrent has a fix for\n    this and is providing the login module with the fix in a separate\n    package called dt-auth. The classname for the module is\n    \u201ccom.datatorrent.auth.jetty.JettyLdapLoginModule\u201d. The dt-auth\n    project along with the source can be found here  Auth . DataTorrent\n    is working on submitting this fix back to Jetty project so that it\n    gets back into the main source.  The JAAS configuration file as described in LDAP  section under  Enabling JAAS Auth  should be configured to specify the ldap settings\nfor roles. A sample configuration  roles based parameters to the\nconfiguration shown before  ldap {\n        com.datatorrent.auth.jetty.JettyLdapLoginModule required\n    hostname=\"ldap-server-hostname\" authenticationMethod=\"simple\"\n    userBaseDn=\"ou=users,dc=domain,dc=com\" userIdAttribute=\"uid\"\n    userRdnAttribute=\"uid\" roleBaseDn=\"ou=groups,dc=domain,dc=com\"\n    roleNameAttribute=\u201dcn\u201d\n    contextFactory=\u201dcom.sun.jndi.ldap.LdapCtxFactory\u201d;\n};  For more ldap settings refer to the java documentation of the login    After the above configuration changes are made the gateway service\n    needs to be restarted. Before restarting however the different\n    classes specified in the configuration files above namely the\n    DataTorrent Jetty callback handler, the Jetty login module with the\n    DataTorrent fix and the Jetty dependencies containing the role class\n    should all be available for Gateway. This can be done by specifying\n    the jars containing these classes in a classpath variable that is\n    used by Gateway.  The jars can be obtained from the  DataTorrent Auth  project.  Please follow the instructions in the above url to obtain the project\njar files. After obtaining the jar files perform the following step to\nmake them available to Gateway  Edit the custom-env.sh configuration file, typically located under /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\nlocal install) and append the list of jars obtained above to the DT_CLASSPATH  variable. This needs to be added at the end of the\nfile in the section for specifying local overrides to environment\nvariables. The line would look like  export DT\\_CLASSPATH=\\${DT\\_CLASSPATH}:path/to/jar1:path/to/jar2:..    Restart the Gateway by running  sudo service dtgateway restart  when running Gateway in local mode use dtgateway restart command.", 
            "title": "JAAS roles "
        }, 
        {
            "location": "/dtgateway_security/#external-role-mapping", 
            "text": "External role mapping is specified to map the external roles to the\nDataTorrent roles. For example users from an LDAP group called admins\nshould have the admin role when using the Console. This can be specified\nby doing the following steps    In the configuration folder typically located under\n     /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for\n    local install) edit the file called external-roles or create the\n    file if it not already present. In this file each line contains a\n    mapping from an external role to a datatorrent role separated by a\n    delimiter \u2018:\u2019 An example listing is  admins:admin\nstaff: developer  This maps the external role admins to the DataTorrent role admin and\nexternal role staff to the DataTorrent role developer.    Restart the Gateway by running  sudo service dtgateway restart  when running Gateway in local mode use  dtgateway restart command", 
            "title": "External Role Mapping "
        }, 
        {
            "location": "/dtgateway_security/#administering-using-command-line", 
            "text": "You can also utilize the  dtGateway REST API  (under /ws/v2/auth) to add or remove users and to change roles and passwords.\n Here are the examples with password authentication.", 
            "title": "Administering Using Command Line "
        }, 
        {
            "location": "/dtgateway_security/#log-in-as-admin", 
            "text": "% curl -c ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"userName\":\"admin\",\"password\":\"admin\"}' http://localhost:9090/ws/v2/login  This curl command logs in as user \u201cadmin\u201d (with the default password\n\u201cadmin\u201d) and stores the cookie in ~/cookie-jar", 
            "title": "Log in as admin:"
        }, 
        {
            "location": "/dtgateway_security/#changing-the-admin-password", 
            "text": "% curl -b ~/cookie-jar -XPOST -H \"Content-Type: application/json\" -d '{\"newPassword\":\"xyz\"}' http://localhost:9090/ws/v2/auth/users/admin  This uses the \u201cadmin\u201d credential from the cookie jar to change the password to \u201cxyz\u201d for user \u201cadmin\u201d.", 
            "title": "Changing the admin password:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-second-admin-user", 
            "text": "% curl -b ~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [ \"admin\" ] }' http://localhost:9090/ws/v2/auth/users/john  This command adds a user \u201cjohn\u201d with password \u201cabc\u201d with admin access.", 
            "title": "Adding a second admin user:"
        }, 
        {
            "location": "/dtgateway_security/#adding-a-user-in-the-developer-role", 
            "text": "% curl -b \\~/cookie-jar -XPUT -H \"Content-Type: application/json\" -d '{\"password\":\"abc\",\"roles\": [\"developer\"] (http://localhost:9090/ws/v1/login) }' http://localhost:9090/ws/v2/auth/users/jane  This command adds a user \u201cjane\u201d with password \u201cabc\u201d with the developer role.", 
            "title": "Adding a user in the developer role:"
        }, 
        {
            "location": "/dtgateway_security/#listing-all-users", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users", 
            "title": "Listing all users:"
        }, 
        {
            "location": "/dtgateway_security/#getting-info-for-a-specific-user", 
            "text": "% curl -b ~/cookie-jar http://localhost:9090/ws/v2/auth/users/john  This command returns the information about the user \u201cjohn\u201d.", 
            "title": "Getting info for a specific user:"
        }, 
        {
            "location": "/dtgateway_security/#removing-a-user", 
            "text": "% curl -b ~/cookie-jar -XDELETE http://localhost:9090/ws/v2/auth/users/jane  This command removes the user \u201cjane\u201d.", 
            "title": "Removing a user:"
        }, 
        {
            "location": "/dtgateway_security/#enabling-https-in-gateway", 
            "text": "HTTPS in the Gateway can be enabled by performing following two steps.    Generate an SSL keystore if you don\u2019t have one.  Instruction on how to generate an SSL keystore is here:  http://docs.oracle.com/cd/E19509-01/820-3503/ggfen/index.html    Add a property to dt-site.xml configuration file, typically located under  /opt/datatorrent/current/conf  ( or  ~/datatorrent/current/conf  for local install).  configuration \n...\n   property \n            name dt.gateway.sslKeystorePath /name \n            value {/path/to/keystore} /value \n   /property \n   property \n             name dt.gateway.sslKeystorePassword /name \n              value {keystore-password} /value \n   /property \n   property \n     name dt.attr.GATEWAY_USE_SSL /name \n           value true /value \n   /property \n... /configuration     Restart the Gateway by running  sudo service dtgateway restart  ( when running Gateway in local mode use  dtgateway restart  command)", 
            "title": "Enabling HTTPS in Gateway"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "dtCli"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/coming_soon/", 
            "text": "Coming Soon", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/coming_soon/#coming-soon", 
            "text": "", 
            "title": "Coming Soon"
        }, 
        {
            "location": "/glossary/", 
            "text": "Glossary of Terms\n\n\nApache Apex\n\n\n\n\nApache Hadoop\n\u00a0- \u00a0\nApache Hadoop\n is a programming framework that supports the processing of large data sets in a distributed computing environment.\n\n\nYARN\n\u00a0- \nApache Hadoop YARN\n (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.\n\n\nResource Manager\n\u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.\n\n\nContainer\n\u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.\n\n\nApplication\n\u00a0- unified batch and real-time stream processing application running on Apache Apex platform.\n\n\nOperator\n\u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.\n\n\nPhysical Operator\n - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.\n\n\nPort\n\u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.\n\n\nStream\n\u00a0- A stream consists of data tuples that flow from one port of an operator to another.\n\n\nSTRAM\n - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.\n\n\nLogical Plan\n\u00a0- Logical representation of an Apache Apex application, where the computational nodes are called \nOperators\n and the data-flow edges are called\u00a0\nStreams\n.\n\n\nPhysical Plan\n\u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.\n\n\nDAG\n\u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their \nLogical\n / \nPhysical\n plans, composed of operators connected by streams.\n\n\nData Tuples Processed\n\u00a0- Number of data objects processed by the operators in an Apache Apex application.\n\n\nData Tuples Emitted\n\u00a0- Number of data objects emitted by the operators with an output port.\n\n\nCurrent Window Id\n\u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.\n\n\nRecovery Window Id\n\u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.\n\n\n\n\nDataTorrent RTS\n\n\n\n\ndtGateway\n\u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS\n\n\ndtManage\n\u00a0- the web based interface to install, configure, manage \n monitor Apache Apex applications running in a Hadoop Cluster\n\n\ndtAssemble\n\u00a0- graphical application assembly tool used to develop applications.\n\n\ndtDashboard\n\u00a0- graphical visualization tool to view and query system and application data.", 
            "title": "Glossary"
        }, 
        {
            "location": "/glossary/#glossary-of-terms", 
            "text": "", 
            "title": "Glossary of Terms"
        }, 
        {
            "location": "/glossary/#apache-apex", 
            "text": "Apache Hadoop \u00a0- \u00a0 Apache Hadoop  is a programming framework that supports the processing of large data sets in a distributed computing environment.  YARN \u00a0-  Apache Hadoop YARN  (Yet Another Resource Negotiator) is a cluster resource management technology, introduced with Hadoop 2.0.  Resource Manager \u00a0- YARN component that allocates and arbitrates the resources such as CPU, Memory and Network.  Container \u00a0- A physical resource with CPU and memory constraints allocated by YARN\u2019s Resource Manager.  Application \u00a0- unified batch and real-time stream processing application running on Apache Apex platform.  Operator \u00a0- An entity that holds a computational logic to process the data tuples. It is part of a real-time stream processing application. The Operator computational logic gets executed inside a YARN Container.  Physical Operator  - Physical representation of the operator, which contains information such as the name of container and the Hadoop node where operator instance is running.  Port \u00a0- Each operator can have ports on which it can receive input data tuples and also output processed data tuples.  Stream \u00a0- A stream consists of data tuples that flow from one port of an operator to another.  STRAM  - Streaming Application Manager is the first process that is activated upon application launch and orchestrates the deployment, management, and monitoring of the Apache Apex applications throughout their lifecycle.  Logical Plan \u00a0- Logical representation of an Apache Apex application, where the computational nodes are called  Operators  and the data-flow edges are called\u00a0 Streams .  Physical Plan \u00a0- Physical representation of the Logical Plan of the application and is a blueprint of how the application will run inside YARN containers deployed across nodes in a Hadoop cluster.  DAG \u00a0- Directed Acyclic Graph, formed by a collection of vertices and directed edges without cycles.  Sometimes interchangeably used to describe Apache Apex applications, specifically referring to their  Logical  /  Physical  plans, composed of operators connected by streams.  Data Tuples Processed \u00a0- Number of data objects processed by the operators in an Apache Apex application.  Data Tuples Emitted \u00a0- Number of data objects emitted by the operators with an output port.  Current Window Id \u00a0- Sequentially increasing identifier for a specific computation time period within Apache Apex platform.  Recovery Window Id \u00a0- Identifier for the last computational window at which the operator state was check-pointed into HFDS.", 
            "title": "Apache Apex"
        }, 
        {
            "location": "/glossary/#datatorrent-rts", 
            "text": "dtGateway \u00a0- HTTP server used by dtManage to interact with STRAM, YARN Resource Manager, and HDFS  dtManage \u00a0- the web based interface to install, configure, manage   monitor Apache Apex applications running in a Hadoop Cluster  dtAssemble \u00a0- graphical application assembly tool used to develop applications.  dtDashboard \u00a0- graphical visualization tool to view and query system and application data.", 
            "title": "DataTorrent RTS"
        }, 
        {
            "location": "/additional_docs/", 
            "text": "Additional Resources\n\n\nApache Apex\n\n\nTo find out more about Apache Apex visit\n\n\n\n\nApache Apex (incubating): \nhttp://apex.incubator.apache.org/\n\n\nApex Mailing List: \ndev@apex.incubator.apache.org\n\n\nApex Overview and Comparison\n\n\n\n\nDataTorrent RTS\n\n\nFor more DataTorrent documentation visit\n\n\n\n\nBlogs\n\n\nFeatured Resources\n\n\nProduct Features\n\n\nArchitecture Overview\n\n\nDocumentation and Guides\n\n\n\n\nFor webinars and videos check out\n\n\n\n\nWebinars\n\n\nSolution Demos", 
            "title": "Resources"
        }, 
        {
            "location": "/additional_docs/#additional-resources", 
            "text": "Apache Apex  To find out more about Apache Apex visit   Apache Apex (incubating):  http://apex.incubator.apache.org/  Apex Mailing List:  dev@apex.incubator.apache.org  Apex Overview and Comparison   DataTorrent RTS  For more DataTorrent documentation visit   Blogs  Featured Resources  Product Features  Architecture Overview  Documentation and Guides   For webinars and videos check out   Webinars  Solution Demos", 
            "title": "Additional Resources"
        }
    ]
}